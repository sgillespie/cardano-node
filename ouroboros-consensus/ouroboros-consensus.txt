-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Consensus layer for the Ouroboros blockchain protocol
--   
--   Consensus layer for the Ouroboros blockchain protocol.
@package ouroboros-consensus
@version 0.15.0.0

module Ouroboros.Consensus.BlockchainTime.WallClock.Types
newtype () => SystemStart
SystemStart :: UTCTime -> SystemStart
[getSystemStart] :: SystemStart -> UTCTime
newtype () => RelativeTime
RelativeTime :: NominalDiffTime -> RelativeTime
[getRelativeTime] :: RelativeTime -> NominalDiffTime
addRelTime :: NominalDiffTime -> RelativeTime -> RelativeTime
diffRelTime :: RelativeTime -> RelativeTime -> NominalDiffTime
fromRelativeTime :: SystemStart -> RelativeTime -> UTCTime
toRelativeTime :: SystemStart -> UTCTime -> RelativeTime

-- | System time
--   
--   Slots are counted from the system start.
data SystemTime m
SystemTime :: m RelativeTime -> m () -> SystemTime m

-- | Get current time (as a <a>RelativeTime</a>)
--   
--   For real deployment, this will take the current <tt>UTCTime</tt> and
--   then subtract the <a>SystemStart</a> (see <tt>defaultSystemTime</tt>).
--   Tests don't bother with a <tt>UTCTime</tt> and just work entirely in
--   <a>RelativeTime</a>.
[systemTimeCurrent] :: SystemTime m -> m RelativeTime

-- | Wait for <a>SystemStart</a>
--   
--   For the real deployment, this waits for the current <tt>UTCTime</tt>
--   to reach <a>SystemStart</a>. In tests this does nothing.
[systemTimeWait] :: SystemTime m -> m ()
getSlotLength :: SlotLength -> NominalDiffTime
mkSlotLength :: NominalDiffTime -> SlotLength
slotLengthFromMillisec :: Integer -> SlotLength
slotLengthFromSec :: Integer -> SlotLength
slotLengthToMillisec :: SlotLength -> Integer
slotLengthToSec :: SlotLength -> Integer
data () => SlotLength
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.BlockchainTime.WallClock.Types.SystemTime m)

module Ouroboros.Consensus.Config.SecurityParam

-- | Protocol security parameter
--   
--   We interpret this as the number of rollbacks we support.
--   
--   i.e., k == 0: we can't roll back at all k == 1: we can roll back at
--   most one block, etc
--   
--   NOTE: This talks about the number of <i>blocks</i> we can roll back,
--   not the number of <i>slots</i>.
newtype SecurityParam
SecurityParam :: Word64 -> SecurityParam
[maxRollbacks] :: SecurityParam -> Word64
instance GHC.Show.Show Ouroboros.Consensus.Config.SecurityParam.SecurityParam
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Config.SecurityParam.SecurityParam
instance GHC.Generics.Generic Ouroboros.Consensus.Config.SecurityParam.SecurityParam
instance GHC.Classes.Eq Ouroboros.Consensus.Config.SecurityParam.SecurityParam

module Ouroboros.Consensus.HardFork.Combinator.Info

-- | Additional newtype wrapper around <a>SingleEraInfo</a>
--   
--   This is primarily useful for use in error messages: it marks which era
--   info came from the ledger, and which came from a
--   tx<i>block</i>header/etc.
newtype LedgerEraInfo blk
LedgerEraInfo :: SingleEraInfo blk -> LedgerEraInfo blk
[getLedgerEraInfo] :: LedgerEraInfo blk -> SingleEraInfo blk

-- | Information about an era (mostly for type errors)
data SingleEraInfo blk
SingleEraInfo :: !Text -> SingleEraInfo blk
[singleEraName] :: SingleEraInfo blk -> !Text
instance Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.Combinator.Info.SingleEraInfo blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Info.SingleEraInfo blk)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Info.SingleEraInfo blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Info.SingleEraInfo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.Info.SingleEraInfo blk)
instance Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.Combinator.Info.LedgerEraInfo blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Info.LedgerEraInfo blk)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Info.LedgerEraInfo blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Info.LedgerEraInfo blk)

module Ouroboros.Consensus.HardFork.Simple

-- | The trigger condition that will cause the hard fork transition.
--   
--   This type is only intended for use as part of a <a>LedgerCfg</a>,
--   which means it is "static": it cannot change during an execution of
--   the node process.
data TriggerHardFork

-- | Trigger the transition when the on-chain protocol major version (from
--   the ledger state) reaches this number.
--   
--   Note: The HFC logic does not require the trigger version for one era
--   to be the successor of the trigger version for the previous era.
TriggerHardForkAtVersion :: !Word16 -> TriggerHardFork

-- | For testing only, trigger the transition at a specific hard-coded
--   epoch, irrespective of the ledger state.
TriggerHardForkAtEpoch :: !EpochNo -> TriggerHardFork

-- | Ledger states in this era cannot determine when the hard fork
--   transition will happen.
--   
--   It's crucial to note that this option does <i>not</i> imply that "the
--   era will never end". Instead, the era cannot end within this node
--   process before it restarts with different software and/or
--   configuration for this era.
TriggerHardForkNotDuringThisExecution :: TriggerHardFork
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.Simple.TriggerHardFork
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.Simple.TriggerHardFork
instance GHC.Show.Show Ouroboros.Consensus.HardFork.Simple.TriggerHardFork

module Ouroboros.Consensus.Ledger.Query.Version

-- | Version of the `Query blk` type.
--   
--   Multiple top level queries are now supported. The encoding now has
--   constructor tags for the different top level queries for QueryVersion1
--   onwards.
data QueryVersion
QueryVersion1 :: QueryVersion
QueryVersion2 :: QueryVersion

-- | Get the <tt>QueryVersion</tt> supported by this
--   <tt>NodeToClientVersion</tt>.
nodeToClientVersionToQueryVersion :: NodeToClientVersion -> QueryVersion
instance GHC.Show.Show Ouroboros.Consensus.Ledger.Query.Version.QueryVersion
instance GHC.Enum.Bounded Ouroboros.Consensus.Ledger.Query.Version.QueryVersion
instance GHC.Enum.Enum Ouroboros.Consensus.Ledger.Query.Version.QueryVersion
instance GHC.Classes.Ord Ouroboros.Consensus.Ledger.Query.Version.QueryVersion
instance GHC.Classes.Eq Ouroboros.Consensus.Ledger.Query.Version.QueryVersion

module Ouroboros.Consensus.Node.NetworkProtocolVersion

-- | Protocol versioning
--   
--   IMPORTANT Note that this is entirely independent of the
--   <a>shelleyProtVer</a> field et al.
--   
--   Its primary purpose is to control the details of on-the-wire codecs.
--   And additionally which queries are allowed, in the case of
--   <tt>BlockNodeToClienVersion</tt> (this use is already handled by
--   <a>shelleyProtVer</a> in the NTN case).
class (Show (BlockNodeToNodeVersion blk), Show (BlockNodeToClientVersion blk), Eq (BlockNodeToNodeVersion blk), Eq (BlockNodeToClientVersion blk)) => HasNetworkProtocolVersion blk where {
    type BlockNodeToNodeVersion blk :: Type;
    type BlockNodeToClientVersion blk :: Type;
    type BlockNodeToNodeVersion blk = ();
    type BlockNodeToClientVersion blk = ();
}
class HasNetworkProtocolVersion blk => SupportedNetworkProtocolVersion blk

-- | Enumerate all supported node-to-node versions
supportedNodeToNodeVersions :: SupportedNetworkProtocolVersion blk => Proxy blk -> Map NodeToNodeVersion (BlockNodeToNodeVersion blk)

-- | Enumerate all supported node-to-client versions
supportedNodeToClientVersions :: SupportedNetworkProtocolVersion blk => Proxy blk -> Map NodeToClientVersion (BlockNodeToClientVersion blk)

-- | The latest released version
--   
--   This is the latest version intended for deployment.
--   
--   IMPORTANT Note that this is entirely independent of the
--   <a>shelleyProtVer</a> field et al.
latestReleasedNodeVersion :: SupportedNetworkProtocolVersion blk => Proxy blk -> (Maybe NodeToNodeVersion, Maybe NodeToClientVersion)

-- | A default for <a>latestReleasedNodeVersion</a>
--   
--   Chooses the greatest in <a>supportedNodeToNodeVersions</a> and
--   <a>supportedNodeToClientVersions</a>.
latestReleasedNodeVersionDefault :: SupportedNetworkProtocolVersion blk => Proxy blk -> (Maybe NodeToNodeVersion, Maybe NodeToClientVersion)
data () => NodeToClientVersion
NodeToClientV_9 :: NodeToClientVersion
NodeToClientV_10 :: NodeToClientVersion
NodeToClientV_11 :: NodeToClientVersion
NodeToClientV_12 :: NodeToClientVersion
NodeToClientV_13 :: NodeToClientVersion
NodeToClientV_14 :: NodeToClientVersion
NodeToClientV_15 :: NodeToClientVersion
NodeToClientV_16 :: NodeToClientVersion
data () => NodeToNodeVersion
NodeToNodeV_7 :: NodeToNodeVersion
NodeToNodeV_8 :: NodeToNodeVersion
NodeToNodeV_9 :: NodeToNodeVersion
NodeToNodeV_10 :: NodeToNodeVersion
NodeToNodeV_11 :: NodeToNodeVersion
NodeToNodeV_12 :: NodeToNodeVersion
NodeToNodeV_13 :: NodeToNodeVersion


-- | Support for protocols that include a signature
module Ouroboros.Consensus.Protocol.Signed

-- | The part of the header that is signed
type family Signed hdr :: Type

-- | Header that contain a signed part
--   
--   This class enforces that signatures are computed over the header only
--   (without the block body). This is important: we must be able to verify
--   the signature in a header without having access to the block (which we
--   download separately). Typically of course the header will contain a
--   hash of the body, so the signature can include the body implicitly.
class SignedHeader hdr

-- | Extract the part of the header that the signature should be computed
--   over
headerSigned :: SignedHeader hdr => hdr -> Signed hdr

module Ouroboros.Consensus.Storage.LedgerDB.DiskPolicy

-- | On-disk policy
--   
--   We only write ledger states that are older than <tt>k</tt> blocks to
--   disk (that is, snapshots that are guaranteed valid). The on-disk
--   policy determines how often we write to disk and how many checkpoints
--   we keep.
data DiskPolicy
DiskPolicy :: Word -> (TimeSinceLast DiffTime -> Word64 -> Bool) -> DiskPolicy

-- | How many snapshots do we want to keep on disk?
--   
--   A higher number of on-disk snapshots is primarily a safe-guard against
--   disk corruption: it trades disk space for reliability.
--   
--   Examples:
--   
--   <ul>
--   <li><tt>0</tt>: Delete the snapshot immediately after writing.
--   Probably not a useful value :-D</li>
--   <li><tt>1</tt>: Delete the previous snapshot immediately after writing
--   the next Dangerous policy: if for some reason the deletion happens
--   before the new snapshot is written entirely to disk (we don't
--   <tt>fsync</tt>), we have no choice but to start at the genesis
--   snapshot on the next startup.</li>
--   <li><tt>2</tt>: Always keep 2 snapshots around. This means that when
--   we write the next snapshot, we delete the oldest one, leaving the
--   middle one available in case of truncation of the write. This is
--   probably a sane value in most circumstances.</li>
--   </ul>
[onDiskNumSnapshots] :: DiskPolicy -> Word

-- | Should we write a snapshot of the ledger state to disk?
--   
--   This function is passed two bits of information:
--   
--   <ul>
--   <li>The time since the last snapshot, or <a>NoSnapshotTakenYet</a> if
--   none was taken yet. Note that <a>NoSnapshotTakenYet</a> merely means
--   no snapshot had been taking yet since the node was started; it does
--   not necessarily mean that none exist on disk.</li>
--   <li>The distance in terms of blocks applied to the <i>oldest</i>
--   ledger snapshot in memory. During normal operation, this is the number
--   of blocks written to the ImmutableDB since the last snapshot. On
--   startup, it is computed by counting how many immutable blocks we had
--   to reapply to get to the chain tip. This is useful, as it allows the
--   policy to decide to take a snapshot <i>on node startup</i> if a lot of
--   blocks had to be replayed.</li>
--   </ul>
--   
--   See also <a>defaultDiskPolicy</a>
[onDiskShouldTakeSnapshot] :: DiskPolicy -> TimeSinceLast DiffTime -> Word64 -> Bool

-- | Length of time, requested by the user, that has to pass after which a
--   snapshot is taken. It can be:
--   
--   <ol>
--   <li>either explicitly provided by user in seconds</li>
--   <li>or default value can be requested - the specific DiskPolicy
--   determines what that is exactly, see <a>defaultDiskPolicy</a> as an
--   example</li>
--   </ol>
data SnapshotInterval
DefaultSnapshotInterval :: SnapshotInterval
RequestedSnapshotInterval :: DiffTime -> SnapshotInterval
data TimeSinceLast time
NoSnapshotTakenYet :: TimeSinceLast time
TimeSinceLast :: time -> TimeSinceLast time

-- | Default on-disk policy suitable to use with cardano-node
defaultDiskPolicy :: SecurityParam -> SnapshotInterval -> DiskPolicy
instance GHC.Show.Show Ouroboros.Consensus.Storage.LedgerDB.DiskPolicy.SnapshotInterval
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.LedgerDB.DiskPolicy.SnapshotInterval
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.LedgerDB.DiskPolicy.SnapshotInterval
instance GHC.Show.Show time => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.DiskPolicy.TimeSinceLast time)
instance GHC.Base.Functor Ouroboros.Consensus.Storage.LedgerDB.DiskPolicy.TimeSinceLast
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.LedgerDB.DiskPolicy.DiskPolicy

module Ouroboros.Consensus.Ticked

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type
instance GHC.Show.Show (Ouroboros.Consensus.Ticked.Ticked ())
instance GHC.Show.Show (Ouroboros.Consensus.Ticked.Ticked (f a)) => GHC.Show.Show ((Data.SOP.BasicFunctors.:.:) Ouroboros.Consensus.Ticked.Ticked f a)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ticked.Ticked (f a)) => NoThunks.Class.NoThunks ((Data.SOP.BasicFunctors.:.:) Ouroboros.Consensus.Ticked.Ticked f a)


-- | Miscellaneous utilities
module Ouroboros.Consensus.Util
class Empty a
class () => ShowProxy (p :: k)
showProxy :: ShowProxy p => Proxy p -> String
data Some (f :: k -> Type)
[Some] :: f a -> Some f

-- | Pair of functors instantiated to the <i>same</i> existential
data SomePair (f :: k -> Type) (g :: k -> Type)
[SomePair] :: f a -> g a -> SomePair f g

-- | Hide the second type argument of some functor
--   
--   <tt>SomeSecond f a</tt> is isomorphic to <tt>Some (f a)</tt>, but is
--   more convenient in partial applications.
data SomeSecond (f :: Type -> Type -> Type) a
[SomeSecond] :: !f a b -> SomeSecond f a
mustBeRight :: Either Void a -> a
foldlM' :: forall m a b. Monad m => (b -> a -> m b) -> b -> [a] -> m b

-- | Apply a function n times. The value of each application is forced.
nTimes :: forall a. (a -> a) -> Word64 -> a -> a

-- | Apply a function n times through a monadic bind. The value of each
--   application is forced.
nTimesM :: forall m a. Monad m => (a -> m a) -> Word64 -> a -> m a
repeatedly :: (a -> b -> b) -> [a] -> b -> b
repeatedlyM :: Monad m => (a -> b -> m b) -> [a] -> b -> m b
allEqual :: Eq a => [a] -> Bool
chunks :: Int -> [a] -> [[a]]

-- | Drop the last <tt>n</tt> elements
dropLast :: Word64 -> [a] -> [a]
firstJust :: forall a b f. Foldable f => (a -> Maybe b) -> f a -> Maybe b

-- | Mark the last element of the list as <a>Right</a>
markLast :: [a] -> [Either a a]

-- | All possible ways to pick on element from a list, preserving order
--   
--   <pre>
--   pickOne [1,2,3] = [ ([], 1, [2, 3])
--                     , ([1], 2, [3])
--                     , ([1,2], 3, [])
--                     ]
--   </pre>
pickOne :: [a] -> [([a], a, [a])]

-- | Split a list given a delimiter predicate.
--   
--   <pre>
--   &gt;&gt;&gt; split (`elem` "xy") "axbyxc"
--   "a" :| ["b","","c"]
--   </pre>
--   
--   We have the laws
--   
--   <pre>
--   concat (split p as) === filter (not . p) as
--   length (split p as) === length (filter p as) + 1
--   </pre>
split :: (a -> Bool) -> [a] -> NonEmpty [a]

-- | Focus on one element in the list
--   
--   E.g.
--   
--   <pre>
--      splits [1..3]
--   == [ ([]    , 1 , [2,3])
--      , ([1]   , 2 , [3]  )
--      , ([1,2] , 3 , []   )
--      ]
--   </pre>
splits :: [a] -> [([a], a, [a])]

-- | Take the last <tt>n</tt> elements
takeLast :: Word64 -> [a] -> [a]

-- | Take items until the condition is true. If the condition is true for
--   an item, include that item as the last item in the returned list. If
--   the condition was never true, the original list is returned.
--   
--   <pre>
--   takeUntil (== 3) [1,2,3,4]
--   </pre>
--   
--   <ul>
--   <li><i>1,2,3</i> &gt; takeUntil (== 2) [0,1,0]</li>
--   <li><i>0,1,0</i> &gt; takeUntil (== 2) [2,2,3]</li>
--   <li><i>2</i></li>
--   </ul>
takeUntil :: (a -> Bool) -> [a] -> [a]
lastMaybe :: [a] -> Maybe a
safeMaximum :: Ord a => [a] -> Maybe a
safeMaximumBy :: (a -> a -> Ordering) -> [a] -> Maybe a
safeMaximumOn :: Ord b => (a -> b) -> [a] -> Maybe a

-- | Calls <a>hashFromBytes</a> and throws an error if the input is of the
--   wrong length.
hashFromBytesE :: forall h a. (HashAlgorithm h, HasCallStack) => ByteString -> Hash h a

-- | Calls <a>hashFromBytesShort</a> and throws an error if the input is of
--   the wrong length.
hashFromBytesShortE :: forall h a. (HashAlgorithm h, HasCallStack) => ShortByteString -> Hash h a
byteStringChunks :: Int -> ByteString -> [ByteString]
lazyByteStringChunks :: Int -> ByteString -> [ByteString]
whenJust :: Applicative f => Maybe a -> (a -> f ()) -> f ()

-- | Assertion
--   
--   Variation on <tt>assert</tt> for use in testing code.
checkThat :: (Show a, Monad m) => String -> (a -> Bool) -> a -> m ()

-- | Check that a bunch of sets are all mutually disjoint
allDisjoint :: forall a. Ord a => [Set a] -> Bool
(......:) :: (y -> z) -> (x0 -> x1 -> x2 -> x3 -> x4 -> x5 -> x6 -> y) -> x0 -> x1 -> x2 -> x3 -> x4 -> x5 -> x6 -> z
(.....:) :: (y -> z) -> (x0 -> x1 -> x2 -> x3 -> x4 -> x5 -> y) -> x0 -> x1 -> x2 -> x3 -> x4 -> x5 -> z
(....:) :: (y -> z) -> (x0 -> x1 -> x2 -> x3 -> x4 -> y) -> x0 -> x1 -> x2 -> x3 -> x4 -> z
(...:) :: (y -> z) -> (x0 -> x1 -> x2 -> x3 -> y) -> x0 -> x1 -> x2 -> x3 -> z
(..:) :: (y -> z) -> (x0 -> x1 -> x2 -> y) -> x0 -> x1 -> x2 -> z
(.:) :: (y -> z) -> (x0 -> x1 -> y) -> x0 -> x1 -> z
pairFst :: Product f g a -> f a
pairSnd :: Product f g a -> g a
eitherToMaybe :: Either a b -> Maybe b

-- | Fast Fibonacci computation, using Binet's formula
fib :: Word64 -> Word64
instance forall k (a :: k). Ouroboros.Consensus.Util.Empty a


-- | Utilities for arguments record with defaults
--   
--   Useful for when you want to define a default value of an arguments
--   record consisting of a mix of arguments with/without defaults.
--   
--   The following code example explains it best:
--   
--   <pre>
--   data Args f = Args {
--         hasNoDefault :: HKD f Int
--       , hasDefault   :: Bool
--       }
--   
--   defaultArgs :: Args Defaults
--   defaultArgs = Args {
--         hasNoDefault = NoDefault
--       , hasDefault   = False
--       }
--   
--   theArgs :: Args Identity
--   theArgs = defaultArgs {
--         hasNoDefault = 0
--       }
--   
--   useArgs :: Args Identity -&gt; (Int, Bool)
--   useArgs (Args a b) = (a, b)
--   </pre>
--   
--   Leaving out the <tt>hasNoDefault</tt> field from <tt>theArgs</tt> will
--   result in a type error.
module Ouroboros.Consensus.Util.Args
data Defaults t
NoDefault :: Defaults t
type family HKD f a
class MapHKD f
mapHKD :: MapHKD f => proxy (f b) -> (a -> b) -> HKD f a -> HKD f b

-- | Identity functor and monad. (a non-strict monad)
newtype () => Identity a
Identity :: a -> Identity a
[runIdentity] :: Identity a -> a
instance GHC.Base.Functor Ouroboros.Consensus.Util.Args.Defaults
instance Ouroboros.Consensus.Util.Args.MapHKD Data.Functor.Identity.Identity
instance Ouroboros.Consensus.Util.Args.MapHKD Ouroboros.Consensus.Util.Args.Defaults


-- | CallStack with a nicer <a>Show</a> instance
--   
--   Use of this module is intended to <i>replace</i> import of
--   <tt>GHC.Stack</tt>
module Ouroboros.Consensus.Util.CallStack
prettyCallStack :: HasCallStack => PrettyCallStack

-- | CallStack with <a>Show</a> instance using <a>prettyCallStack</a>
data PrettyCallStack

-- | Request a CallStack.
--   
--   NOTE: The implicit parameter <tt>?callStack :: CallStack</tt> is an
--   implementation detail and <b>should not</b> be considered part of the
--   <a>CallStack</a> API, we may decide to change the implementation in
--   the future.
type HasCallStack = ?callStack :: CallStack
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Util.CallStack.PrettyCallStack
instance GHC.Show.Show Ouroboros.Consensus.Util.CallStack.PrettyCallStack

module Ouroboros.Consensus.Util.DepPair

-- | Dependent pair
--   
--   A dependent pair is a pair of values where the type of the value
--   depends on the first value.
type DepPair = GenDepPair I

-- | Generalization of <a>DepPair</a>
--   
--   This adds an additional functor <tt>g</tt> around the second value in
--   the pair.
data GenDepPair g f
[GenDepPair] :: !f a -> !g a -> GenDepPair g f
pattern DepPair :: f a -> a -> DepPair f
depPairFirst :: (forall a. f a -> f' a) -> GenDepPair g f -> GenDepPair g f'
class SameDepIndex f
sameDepIndex :: SameDepIndex f => f a -> f b -> Maybe (a :~: b)
sameDepIndex :: (SameDepIndex f, TrivialDependency f) => f a -> f b -> Maybe (a :~: b)

-- | A dependency is trivial if it always maps to the same type <tt>b</tt>
class TrivialDependency f where {
    type TrivialIndex f :: Type;
}
hasSingleIndex :: TrivialDependency f => f a -> f b -> a :~: b
indexIsTrivial :: TrivialDependency f => f (TrivialIndex f)
fromTrivialDependency :: TrivialDependency f => f a -> a -> TrivialIndex f
toTrivialDependency :: TrivialDependency f => f a -> TrivialIndex f -> a

-- | <a>Proxy</a> is a type that holds no data, but has a phantom parameter
--   of arbitrary type (or even kind). Its use is to provide type
--   information, even though there is no value available of that type (or
--   it may be too costly to create one).
--   
--   Historically, <tt><a>Proxy</a> :: <a>Proxy</a> a</tt> is a safer
--   alternative to the <tt><a>undefined</a> :: a</tt> idiom.
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy (Void, Int -&gt; Int)
--   Proxy
--   </pre>
--   
--   Proxy can even hold types of higher kinds,
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Either
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Functor
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy complicatedStructure
--   Proxy
--   </pre>
data () => Proxy (t :: k)
Proxy :: Proxy (t :: k)

-- | Propositional equality. If <tt>a :~: b</tt> is inhabited by some
--   terminating value, then the type <tt>a</tt> is the same as the type
--   <tt>b</tt>. To use this equality in practice, pattern-match on the
--   <tt>a :~: b</tt> to get out the <tt>Refl</tt> constructor; in the body
--   of the pattern-match, the compiler knows that <tt>a ~ b</tt>.
data () => (a :: k) :~: (b :: k)
[Refl] :: forall {k} (a :: k). a :~: a
infix 4 :~:

module Ouroboros.Consensus.Block.NestedContent

-- | Nested content inside a block
--   
--   Consider a simplified version of the hard fork combinator, defining
--   
--   <pre>
--   type HardFork a b = Either a b
--   </pre>
--   
--   Then encoding <tt>Hardfork ByronBlock ShelleyBlock</tt> is easy, in
--   the same way that we encode <i>any</i> <tt>Either</tt>. The
--   <i>header</i> of such a block will have type
--   
--   <pre>
--   HardFork (Header ByronBlock) (Header ShelleyBlock)
--   </pre>
--   
--   and encoding those (for example, to send them across the network) is
--   similarly trivial. But now suppose we want to read a header from disk.
--   We do not store headers directly, but instead store the blocks. The DB
--   will know the offset and length (both in bytes) of the header inside
--   the block, but how do we decode such a header? If it's a Byron block,
--   we should use the decoder for <tt>Header ByronBlock</tt>, and
--   similarly for Shelley, but how should we express this more generally?
--   
--   Here is where <a>HasNestedContent</a> comes in. Continuing the
--   example, we can <tt>unnest</tt> a <tt>Header (HardFork ByronBlock
--   ShelleyBlock)</tt> into a pair of values, where the first value (a
--   <tt>NestedCtxt</tt>) tells us what type of block we have, and the
--   second value gives us the actual header. So, if the first value says
--   "this is a Byron block", the second value is a <tt>Header
--   ByronBlock</tt>, and vice versa. In other words, this is a dependent
--   pair.
--   
--   This then solves the serialisation problem: we expect a
--   <i>dependent</i> decoder which, <i>given</i> a <tt>NestedCtxt</tt>
--   identifying the block type, decodes the raw bytes from the block into
--   the type indicated by that <tt>NestedCtxt</tt>.
--   
--   TODO: We could perhaps define this independent of blocks in
--   <a>GenDepPair</a>.
class (forall a. Show (NestedCtxt_ blk f a), SameDepIndex (NestedCtxt_ blk f)) => HasNestedContent f blk
unnest :: HasNestedContent f blk => f blk -> DepPair (NestedCtxt f blk)
nest :: HasNestedContent f blk => DepPair (NestedCtxt f blk) -> f blk
unnest :: (HasNestedContent f blk, TrivialDependency (NestedCtxt f blk), TrivialIndex (NestedCtxt f blk) ~ f blk) => f blk -> DepPair (NestedCtxt f blk)
nest :: (HasNestedContent f blk, TrivialDependency (NestedCtxt f blk), TrivialIndex (NestedCtxt f blk) ~ f blk) => DepPair (NestedCtxt f blk) -> f blk

-- | Context identifying what kind of block we have
--   
--   In almost all places we will use <a>NestedCtxt</a> rather than
--   <a>NestedCtxt_</a>.
data family NestedCtxt_ blk :: (Type -> Type) -> (Type -> Type)
curriedNest :: HasNestedContent f blk => NestedCtxt f blk a -> a -> f blk

-- | Version of <a>NestedCtxt_</a> with the type arguments swapped
--   
--   <a>NestedCtxt</a> must be indexed on <tt>blk</tt>: it is the block
--   that determines this type. However, we often want to partially apply
--   the second argument (the functor), leaving the block type not yet
--   defined.
newtype NestedCtxt f blk a
NestedCtxt :: NestedCtxt_ blk f a -> NestedCtxt f blk a
[flipNestedCtxt] :: NestedCtxt f blk a -> NestedCtxt_ blk f a
castNestedCtxt :: (NestedCtxt_ blk f a -> NestedCtxt_ blk' f a) -> NestedCtxt f blk a -> NestedCtxt f blk' a
mapNestedCtxt :: (NestedCtxt_ blk f a -> NestedCtxt_ blk' f' a') -> NestedCtxt f blk a -> NestedCtxt f' blk' a'
castSomeNestedCtxt :: (forall a. NestedCtxt_ blk f a -> NestedCtxt_ blk' f a) -> SomeSecond (NestedCtxt f) blk -> SomeSecond (NestedCtxt f) blk'
mapSomeNestedCtxt :: (forall a. NestedCtxt_ blk f a -> NestedCtxt_ blk' f' a) -> SomeSecond (NestedCtxt f) blk -> SomeSecond (NestedCtxt f') blk'

-- | Hide the second type argument of some functor
--   
--   <tt>SomeSecond f a</tt> is isomorphic to <tt>Some (f a)</tt>, but is
--   more convenient in partial applications.
data SomeSecond (f :: Type -> Type -> Type) a
[SomeSecond] :: !f a b -> SomeSecond f a
instance GHC.Show.Show (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ blk f a) => GHC.Show.Show (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f blk a)
instance (Ouroboros.Consensus.Block.NestedContent.HasNestedContent f blk, forall a. GHC.Show.Show (g a)) => GHC.Show.Show (Ouroboros.Consensus.Util.DepPair.GenDepPair g (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f blk))
instance Ouroboros.Consensus.Block.NestedContent.HasNestedContent f blk => GHC.Show.Show (Ouroboros.Consensus.Util.SomeSecond (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f) blk)
instance (Data.Typeable.Internal.Typeable f, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.SomeSecond (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f) blk)
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ blk f) => Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f blk)
instance Ouroboros.Consensus.Util.DepPair.TrivialDependency (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ blk f) => Ouroboros.Consensus.Util.DepPair.TrivialDependency (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f blk)
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ blk f) => GHC.Classes.Eq (Ouroboros.Consensus.Util.SomeSecond (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f) blk)


-- | Utility functions for enclosing a code segment with tracing events.
module Ouroboros.Consensus.Util.Enclose
type Enclosing = Enclosing' ()
data Enclosing' a

-- | Preceding a specific code segment.
RisingEdge :: Enclosing' a

-- | Succeeding a specific code segment, with extra information.
FallingEdgeWith :: !a -> Enclosing' a

-- | Enclose an action using the given <a>Tracer</a>.
encloseWith :: Applicative m => Tracer m Enclosing -> m a -> m a
pattern FallingEdge :: Enclosing' ()
instance GHC.Classes.Ord a => GHC.Classes.Ord (Ouroboros.Consensus.Util.Enclose.Enclosing' a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Ouroboros.Consensus.Util.Enclose.Enclosing' a)
instance GHC.Show.Show a => GHC.Show.Show (Ouroboros.Consensus.Util.Enclose.Enclosing' a)

module Ouroboros.Consensus.Util.FileLock

-- | Abstraction for file locks
data FileLock m
FileLock :: (FilePath -> m (m ())) -> FileLock m

-- | Obtain an exclusive lock on the given file.
--   
--   Returns the function to unlock the file.
--   
--   Blocks until the lock is available.
--   
--   We don't guarantee the ability to read/write to a locked file, not
--   even when holding the lock.
[lockFile] :: FileLock m -> FilePath -> m (m ())

-- | Implementation of <a>FileLock</a> for <a>IO</a>, using on
--   <a>System.FileLock</a>.
--   
--   Locking the file can block in FFI, so not interruptible.
--   
--   Unlocking the file is not guaranteed to be synchronous. Near
--   instantaneous on Linux, but not synchronous. On Windows, unlocking is
--   even more lazy.
ioFileLock :: FileLock IO


-- | Heterogeneous lists
--   
--   Intended for qualified import
module Ouroboros.Consensus.Util.HList
type family All c as :: Constraint
data HList :: [Type] -> Type
[Nil] :: HList '[]
[:*] :: a -> HList as -> HList (a ': as)
infixr 9 :*
collapse :: forall c as b proxy. All c as => proxy c -> (forall a. c a => a -> b) -> HList as -> [b]
foldMap :: forall c as b proxy. (All c as, Monoid b) => proxy c -> (forall a. c a => a -> b) -> HList as -> b
foldl :: forall c as b proxy. All c as => proxy c -> (forall a. c a => b -> a -> b) -> b -> HList as -> b
foldlM :: forall c as m b proxy. (All c as, Monad m) => proxy c -> (forall a. c a => b -> a -> m b) -> b -> HList as -> m b
foldr :: forall c as b proxy. All c as => proxy c -> (forall a. c a => a -> b -> b) -> b -> HList as -> b

-- | Apply function repeatedly for all elements of the list
--   
--   <pre>
--   repeatedly p = flip . foldl p . flip
--   </pre>
repeatedly :: forall c as b proxy. All c as => proxy c -> (forall a. c a => a -> b -> b) -> HList as -> b -> b
repeatedlyM :: forall c as b proxy m. (Monad m, All c as) => proxy c -> (forall a. c a => a -> b -> m b) -> HList as -> b -> m b
class IsList (xs :: [Type])
isList :: IsList xs => SList xs
data SList :: [Type] -> Type
type family Fn as b
afterFn :: SList as -> (b -> c) -> Fn as b -> Fn as c
applyFn :: Fn as b -> HList as -> b
instance (Ouroboros.Consensus.Util.HList.IsList as, Ouroboros.Consensus.Util.HList.All GHC.Classes.Eq as) => GHC.Classes.Eq (Ouroboros.Consensus.Util.HList.HList as)
instance (Ouroboros.Consensus.Util.HList.IsList as, Ouroboros.Consensus.Util.HList.All GHC.Classes.Eq as, Ouroboros.Consensus.Util.HList.All GHC.Classes.Ord as) => GHC.Classes.Ord (Ouroboros.Consensus.Util.HList.HList as)
instance Ouroboros.Consensus.Util.HList.IsList '[]
instance Ouroboros.Consensus.Util.HList.IsList as => Ouroboros.Consensus.Util.HList.IsList (a : as)
instance Ouroboros.Consensus.Util.HList.All GHC.Show.Show as => GHC.Show.Show (Ouroboros.Consensus.Util.HList.HList as)

module Ouroboros.Consensus.Util.Condense

-- | Condensed but human-readable output
class Condense a
condense :: Condense a => a -> String
class Condense1 f
liftCondense :: Condense1 f => (a -> String) -> f a -> String

-- | Lift the standard <a>condense</a> function through the type
--   constructor
condense1 :: (Condense1 f, Condense a) => f a -> String
instance Ouroboros.Consensus.Util.Condense.Condense1 []
instance Ouroboros.Consensus.Util.Condense.Condense1 Data.Set.Internal.Set
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Base.Void
instance Ouroboros.Consensus.Util.Condense.Condense Data.Text.Internal.Text
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Types.Bool
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Types.Int
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Int.Int64
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Types.Word
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Word.Word32
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Word.Word64
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Num.Natural.Natural
instance Ouroboros.Consensus.Util.Condense.Condense GHC.Real.Rational
instance Ouroboros.Consensus.Util.Condense.Condense a => Ouroboros.Consensus.Util.Condense.Condense [a]
instance Ouroboros.Consensus.Util.Condense.Condense a => Ouroboros.Consensus.Util.Condense.Condense (GHC.Maybe.Maybe a)
instance Ouroboros.Consensus.Util.Condense.Condense a => Ouroboros.Consensus.Util.Condense.Condense (Data.Set.Internal.Set a)
instance (Ouroboros.Consensus.Util.Condense.Condense a, Ouroboros.Consensus.Util.Condense.Condense b) => Ouroboros.Consensus.Util.Condense.Condense (a, b)
instance (Ouroboros.Consensus.Util.Condense.Condense a, Ouroboros.Consensus.Util.Condense.Condense b, Ouroboros.Consensus.Util.Condense.Condense c) => Ouroboros.Consensus.Util.Condense.Condense (a, b, c)
instance (Ouroboros.Consensus.Util.Condense.Condense a, Ouroboros.Consensus.Util.Condense.Condense b, Ouroboros.Consensus.Util.Condense.Condense c, Ouroboros.Consensus.Util.Condense.Condense d) => Ouroboros.Consensus.Util.Condense.Condense (a, b, c, d)
instance (Ouroboros.Consensus.Util.Condense.Condense a, Ouroboros.Consensus.Util.Condense.Condense b, Ouroboros.Consensus.Util.Condense.Condense c, Ouroboros.Consensus.Util.Condense.Condense d, Ouroboros.Consensus.Util.Condense.Condense e) => Ouroboros.Consensus.Util.Condense.Condense (a, b, c, d, e)
instance (Ouroboros.Consensus.Util.Condense.Condense k, Ouroboros.Consensus.Util.Condense.Condense a) => Ouroboros.Consensus.Util.Condense.Condense (Data.Map.Internal.Map k a)
instance Ouroboros.Consensus.Util.Condense.Condense Data.ByteString.Internal.Type.ByteString
instance Ouroboros.Consensus.Util.Condense.Condense Data.ByteString.Lazy.Internal.ByteString
instance Ouroboros.Consensus.Util.HList.All Ouroboros.Consensus.Util.Condense.Condense as => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Util.HList.HList as)
instance Ouroboros.Consensus.Util.Condense.Condense Cardano.Slotting.Block.BlockNo
instance Ouroboros.Consensus.Util.Condense.Condense Cardano.Slotting.Slot.SlotNo
instance Ouroboros.Consensus.Util.Condense.Condense Cardano.Slotting.Slot.EpochNo
instance Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.HeaderHash b) => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.ChainHash b)
instance Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.HeaderHash b) => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.Tip b)
instance Ouroboros.Consensus.Util.Condense.Condense a => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Slotting.Slot.WithOrigin a)
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.SigDSIGN v) => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.SignedDSIGN v a)
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.SigDSIGN Cardano.Crypto.DSIGN.Ed25519.Ed25519DSIGN)
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.SigDSIGN Cardano.Crypto.DSIGN.Ed448.Ed448DSIGN)
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.SigDSIGN Cardano.Crypto.DSIGN.Mock.MockDSIGN)
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SigKES v) => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SignedKES v a)
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SigKES (Cardano.Crypto.KES.Mock.MockKES t))
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SigKES Cardano.Crypto.KES.NeverUsed.NeverKES)
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.SigDSIGN d) => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SigKES (Cardano.Crypto.KES.Simple.SimpleKES d t))
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.SigDSIGN d) => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SigKES (Cardano.Crypto.KES.Single.SingleKES d))
instance GHC.Show.Show (Cardano.Crypto.DSIGN.Class.VerKeyDSIGN d) => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.VerKeyDSIGN d)
instance (Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SigKES d), Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.VerKeyKES d)) => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.SigKES (Cardano.Crypto.KES.Sum.SumKES h d))
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.DSIGN.Class.VerKeyDSIGN d) => Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.VerKeyKES (Cardano.Crypto.KES.Single.SingleKES d))
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.KES.Class.VerKeyKES (Cardano.Crypto.KES.Sum.SumKES h d))
instance Ouroboros.Consensus.Util.Condense.Condense (Cardano.Crypto.Hash.Class.Hash h a)
instance Ouroboros.Consensus.Util.Condense.Condense Control.Monad.Class.MonadTime.SI.Time

module Ouroboros.Consensus.Protocol.PBFT.Crypto

-- | Crypto primitives required by BFT
--   
--   Cardano stores a map of stakeholder IDs rather than the verification
--   key directly. We make this family injective for convenience - whilst
--   it's _possible_ that there could be non-injective instances, the
--   chances of there being more than the two instances here are basically
--   non-existent.
class (Typeable c, DSIGNAlgorithm (PBftDSIGN c), Condense (SigDSIGN (PBftDSIGN c)), Show (PBftVerKeyHash c), Ord (PBftVerKeyHash c), Eq (PBftVerKeyHash c), Show (PBftVerKeyHash c), NoThunks (PBftVerKeyHash c), NoThunks (PBftDelegationCert c)) => PBftCrypto c where {
    type PBftDSIGN c :: Type;
    type PBftDelegationCert c = (d :: Type) | d -> c;
    type PBftVerKeyHash c = (d :: Type) | d -> c;
}
dlgCertGenVerKey :: PBftCrypto c => PBftDelegationCert c -> VerKeyDSIGN (PBftDSIGN c)
dlgCertDlgVerKey :: PBftCrypto c => PBftDelegationCert c -> VerKeyDSIGN (PBftDSIGN c)
hashVerKey :: PBftCrypto c => VerKeyDSIGN (PBftDSIGN c) -> PBftVerKeyHash c
data PBftMockCrypto

-- | We don't hash and just use the underlying <a>Word64</a>.
newtype PBftMockVerKeyHash
PBftMockVerKeyHash :: VerKeyDSIGN MockDSIGN -> PBftMockVerKeyHash
[getPBftMockVerKeyHash] :: PBftMockVerKeyHash -> VerKeyDSIGN MockDSIGN
instance GHC.Classes.Ord Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftMockVerKeyHash
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftMockVerKeyHash
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftMockVerKeyHash
instance GHC.Generics.Generic Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftMockVerKeyHash
instance GHC.Show.Show Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftMockVerKeyHash
instance GHC.Classes.Eq Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftMockVerKeyHash
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftMockCrypto

module Ouroboros.Consensus.NodeId

-- | Core node ID
newtype CoreNodeId
CoreNodeId :: Word64 -> CoreNodeId
[unCoreNodeId] :: CoreNodeId -> Word64
data NodeId
CoreId :: !CoreNodeId -> NodeId
RelayId :: !Word64 -> NodeId
decodeNodeId :: Decoder s NodeId
encodeNodeId :: NodeId -> Encoding
fromCoreNodeId :: CoreNodeId -> NodeId
instance GHC.Show.Show Ouroboros.Consensus.NodeId.CoreNodeId
instance NoThunks.Class.NoThunks Ouroboros.Consensus.NodeId.CoreNodeId
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.NodeId.CoreNodeId
instance Ouroboros.Consensus.Util.Condense.Condense Ouroboros.Consensus.NodeId.CoreNodeId
instance GHC.Generics.Generic Ouroboros.Consensus.NodeId.CoreNodeId
instance GHC.Classes.Ord Ouroboros.Consensus.NodeId.CoreNodeId
instance GHC.Classes.Eq Ouroboros.Consensus.NodeId.CoreNodeId
instance NoThunks.Class.NoThunks Ouroboros.Consensus.NodeId.NodeId
instance GHC.Generics.Generic Ouroboros.Consensus.NodeId.NodeId
instance GHC.Show.Show Ouroboros.Consensus.NodeId.NodeId
instance GHC.Classes.Ord Ouroboros.Consensus.NodeId.NodeId
instance GHC.Classes.Eq Ouroboros.Consensus.NodeId.NodeId
instance Ouroboros.Consensus.Util.Condense.Condense Ouroboros.Consensus.NodeId.NodeId
instance Data.Hashable.Class.Hashable Ouroboros.Consensus.NodeId.NodeId
instance Ouroboros.Network.Util.ShowProxy.ShowProxy Ouroboros.Consensus.NodeId.NodeId
instance Data.Hashable.Class.Hashable Ouroboros.Consensus.NodeId.CoreNodeId


-- | Generic infrastructure for working with EBBs
module Ouroboros.Consensus.Block.EBB

-- | Whether a block is an Epoch Boundary Block (EBB)
--   
--   See <a>Ouroboros.Storage.ImmutableDB.API</a> for a discussion of EBBs.
--   Key idiosyncracies:
--   
--   <ul>
--   <li>An EBB carries no unique information.</li>
--   <li>An EBB has the same <tt>BlockNo</tt> as its predecessor.</li>
--   <li>EBBs are vestigial. As of Shelley, nodes no longer forge EBBs:
--   they are only a legacy/backwards-compatibility concern.</li>
--   </ul>
data IsEBB
IsEBB :: IsEBB
IsNotEBB :: IsEBB
fromIsEBB :: IsEBB -> Bool
toIsEBB :: Bool -> IsEBB
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Block.EBB.IsEBB
instance GHC.Generics.Generic Ouroboros.Consensus.Block.EBB.IsEBB
instance GHC.Show.Show Ouroboros.Consensus.Block.EBB.IsEBB
instance GHC.Classes.Eq Ouroboros.Consensus.Block.EBB.IsEBB
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.Block.EBB.IsEBB
instance Ouroboros.Consensus.Util.Condense.Condense Ouroboros.Consensus.Block.EBB.IsEBB

module Ouroboros.Consensus.Block.Abstract

-- | Map block to consensus protocol
type family BlockProtocol blk :: Type

-- | Static configuration required to work with this type of blocks
data family BlockConfig blk :: Type

-- | Static configuration required for serialisation and deserialisation of
--   types pertaining to this type of block.
--   
--   Data family instead of type family to get better type inference.
data family CodecConfig blk :: Type

-- | Config needed for the <a>NodeInitStorage</a> class. Defined here to
--   avoid circular dependencies.
data family StorageConfig blk :: Type
class (HasHeader blk, GetHeader blk) => GetPrevHash blk

-- | Get the hash of the predecessor of this block
headerPrevHash :: GetPrevHash blk => Header blk -> ChainHash blk
blockPrevHash :: GetPrevHash blk => blk -> ChainHash blk
class HasHeader (Header blk) => GetHeader blk
getHeader :: GetHeader blk => blk -> Header blk

-- | Check whether the header is the header of the block.
--   
--   For example, by checking whether the hash of the body stored in the
--   header matches that of the block.
blockMatchesHeader :: GetHeader blk => Header blk -> blk -> Bool

-- | When the given header is the header of an Epoch Boundary Block,
--   returns its epoch number.
headerIsEBB :: GetHeader blk => Header blk -> Maybe EpochNo
data family Header blk :: Type
blockIsEBB :: GetHeader blk => blk -> Maybe EpochNo
blockToIsEBB :: GetHeader blk => blk -> IsEBB

-- | Get the <a>HeaderFields</a> of a block, without requiring 'HasHeader
--   blk'
--   
--   This is primarily useful as a a simple definition of <a>HasHeader</a>
--   for block types:
--   
--   <pre>
--   instance HasHeader SomeBlock where
--     getHeaderFields = getBlockHeaderFields
--   </pre>
--   
--   provided that there is a <a>HasHeader</a> instance for the header.
--   
--   Unfortunately we cannot give a <a>HasHeader</a> instance once and for
--   all; if we mapped from a header to a block instead we could do
--   
--   <pre>
--   instance HasHeader hdr =&gt; HasHeader (Block hdr) where
--    ..
--   </pre>
--   
--   but we can't do that when we do things this way around.
getBlockHeaderFields :: GetHeader blk => blk -> HeaderFields blk
headerHash :: HasHeader (Header blk) => Header blk -> HeaderHash blk
headerPoint :: HasHeader (Header blk) => Header blk -> Point blk
headerToIsEBB :: GetHeader blk => Header blk -> IsEBB

-- | Convert a hash from/to raw bytes
--   
--   Variants of <a>toRawHash</a> and <a>fromRawHash</a> for
--   <a>ShortByteString</a> are included. Override the default
--   implementations to avoid an extra step in case the <a>HeaderHash</a>
--   is a <a>ShortByteString</a> under the hood.
class ConvertRawHash blk

-- | Get the raw bytes from a hash
toRawHash :: ConvertRawHash blk => proxy blk -> HeaderHash blk -> ByteString

-- | Construct the hash from a raw hash
--   
--   PRECONDITION: the bytestring's size must match <a>hashSize</a>
fromRawHash :: ConvertRawHash blk => proxy blk -> ByteString -> HeaderHash blk

-- | Variant of <a>toRawHash</a> for <a>ShortByteString</a>
toShortRawHash :: ConvertRawHash blk => proxy blk -> HeaderHash blk -> ShortByteString

-- | Variant of <a>fromRawHash</a> for <a>ShortByteString</a>
fromShortRawHash :: ConvertRawHash blk => proxy blk -> ShortByteString -> HeaderHash blk

-- | The size of the hash in number of bytes
hashSize :: ConvertRawHash blk => proxy blk -> Word32
decodeRawHash :: ConvertRawHash blk => proxy blk -> forall s. Decoder s (HeaderHash blk)
encodeRawHash :: ConvertRawHash blk => proxy blk -> HeaderHash blk -> Encoding

-- | Return the successor of a <a>WithOrigin</a> value. Useful in
--   combination with <a>SlotNo</a> and <a>BlockNo</a>.
succWithOrigin :: (Bounded t, Enum t) => WithOrigin t -> t
data () => ChainHash (b :: k)
GenesisHash :: ChainHash (b :: k)
BlockHash :: !HeaderHash b -> ChainHash (b :: k)
class (StandardHash b, Typeable b) => HasHeader b
getHeaderFields :: HasHeader b => b -> HeaderFields b
data () => HeaderFields (b :: k)
HeaderFields :: SlotNo -> BlockNo -> HeaderHash b -> HeaderFields (b :: k)
[headerFieldSlot] :: HeaderFields (b :: k) -> SlotNo
[headerFieldBlockNo] :: HeaderFields (b :: k) -> BlockNo
[headerFieldHash] :: HeaderFields (b :: k) -> HeaderHash b
type family HeaderHash (b :: k)
data () => Point (block :: k)
pattern GenesisPoint :: Point block
pattern BlockPoint :: SlotNo -> HeaderHash block -> Point block
class (Eq HeaderHash b, Ord HeaderHash b, Show HeaderHash b, Typeable HeaderHash b, NoThunks HeaderHash b) => StandardHash (b :: k)
blockHash :: HasHeader b => b -> HeaderHash b
blockNo :: HasHeader b => b -> BlockNo
blockPoint :: HasHeader block => block -> Point block
blockSlot :: HasHeader b => b -> SlotNo
castHash :: forall {k1} {k2} (b :: k1) (b' :: k2). Coercible (HeaderHash b) (HeaderHash b') => ChainHash b -> ChainHash b'
castHeaderFields :: forall {k1} {k2} (b :: k1) (b' :: k2). HeaderHash b ~ HeaderHash b' => HeaderFields b -> HeaderFields b'
castPoint :: forall {k1} {k2} (b :: k1) (b' :: k2). Coercible (HeaderHash b) (HeaderHash b') => Point b -> Point b'
pointHash :: forall {k} (block :: k). Point block -> ChainHash block
pointSlot :: forall {k} (block :: k). Point block -> WithOrigin SlotNo
newtype () => BlockNo
BlockNo :: Word64 -> BlockNo
[unBlockNo] :: BlockNo -> Word64
newtype () => EpochNo
EpochNo :: Word64 -> EpochNo
[unEpochNo] :: EpochNo -> Word64
newtype () => EpochSize
EpochSize :: Word64 -> EpochSize
[unEpochSize] :: EpochSize -> Word64
newtype () => SlotNo
SlotNo :: Word64 -> SlotNo
[unSlotNo] :: SlotNo -> Word64
data () => WithOrigin t
Origin :: WithOrigin t

-- | Custom pattern for <a>WithOrigin</a>
--   
--   This avoids clashing with our (extensive) use of <tt>At</tt> for
--   testing.
pattern NotOrigin :: t -> WithOrigin t
fromWithOrigin :: t -> WithOrigin t -> t
withOrigin :: b -> (t -> b) -> WithOrigin t -> b
withOriginFromMaybe :: Maybe t -> WithOrigin t
withOriginToMaybe :: WithOrigin t -> Maybe t
instance Ouroboros.Network.Block.HasHeader blk => Ouroboros.Network.Block.StandardHash (Ouroboros.Consensus.Block.Abstract.Header blk)

module Ouroboros.Consensus.Protocol.Abstract

-- | Static configuration required to run the consensus protocol
--   
--   Every method in the <a>ConsensusProtocol</a> class takes the consensus
--   configuration as a parameter, so having this as a data family rather
--   than a type family resolves most ambiguity.
--   
--   Defined out of the class so that protocols can define this type
--   without having to define the entire protocol at the same time (or
--   indeed in the same module).
data family ConsensusConfig p :: Type

-- | The (open) universe of Ouroboros protocols
--   
--   This class encodes the part that is independent from any particular
--   block representation.
class (Show (ChainDepState p), Show (ValidationErr p), Show (SelectView p), Show (LedgerView p), Eq (ChainDepState p), Eq (ValidationErr p), Ord (SelectView p), NoThunks (ConsensusConfig p), NoThunks (ChainDepState p), NoThunks (ValidationErr p), NoThunks (SelectView p), Typeable p) => ConsensusProtocol p where {
    
    -- | Protocol-specific state
    --   
    --   NOTE: This chain is blockchain dependent, i.e., updated when new
    --   blocks come in (more precisely, new <i>headers</i>), and subject to
    --   rollback.
    type ChainDepState p :: Type;
    
    -- | Evidence that a node <i>is</i> the leader
    type IsLeader p :: Type;
    
    -- | Evidence that we <i>can</i> be a leader
    type CanBeLeader p :: Type;
    
    -- | View on a header required for chain selection
    --   
    --   Chain selection is implemented by the chain database, which takes care
    --   of two things independent of a choice of consensus protocol: we never
    --   switch to chains that fork off more than <tt>k</tt> blocks ago, and we
    --   never adopt an invalid chain. The actual comparison of chains however
    --   depends on the chain selection protocol. We define chain selection
    --   (which is itself a partial order) in terms of a totally ordered
    --   <i>select view</i> on the headers at the tips of those chains: chain A
    --   is strictly preferred over chain B whenever A's select view is greater
    --   than B's select view. When the select view on A and B is the same, the
    --   chains are considered to be incomparable (neither chain is preferred
    --   over the other).
    type SelectView p :: Type;
    
    -- | Projection of the ledger state the Ouroboros protocol needs access to
    --   
    --   The <a>LedgerView</a> is a summary of the state of the ledger that the
    --   consensus algorithm requires to do its job. Under certain
    --   circumstances the consensus algorithm may require the
    --   <a>LedgerView</a> for slots in the past (before the current tip of the
    --   chain) or in the (near) future (beyond the tip of the current chain,
    --   without having seen those future blocks yet).
    --   
    --   This puts limitations on what the <a>LedgerView</a> can be. For
    --   example, it cannot be the "current stake distribution", since it is of
    --   course impossible to compute the current stake distibution for a slot
    --   in the future. This means that for a consensus algorithm that requires
    --   the stake distribution such as Praos, the <a>LedgerView</a> for a
    --   particular slot must be the "stake distribution for the purpose of
    --   leader selection". This "relevant" stake distribution <i>can</i> be
    --   computed for slots in the (near) future because it is based on
    --   historical stake, not current.
    --   
    --   A somewhat unfortunate consequence of this is that some decisions that
    --   ought to live in the consensus layer (such as the decision precisely
    --   which historical stake to sample to determine the relevant stake
    --   distribution) instead live in the ledger layer. It is difficult to
    --   disentangle this, because the ledger may indeed <i>depend</i> on those
    --   sampling decisions (for example, reward calculations <i>must</i> be
    --   based on that same stake distribution).
    --   
    --   There are also some <i>advantages</i> to moving these sorts of
    --   decisions to the ledger layer. It means that the consensus algorithm
    --   can continue to function without modifications if we decide that the
    --   stake distribution for leader selection should be based on something
    --   else instead (for example, for some bespoke version of the blockchain
    --   we may wish to use a committee instead of a decentralized blockchain).
    --   Having sampling decisions in the ledger layer rather than the
    --   consensus layer means that these decisions can be made without
    --   modifying the consensus algorithm.
    --   
    --   Note that for the specific case of Praos, whilst the ledger layer
    --   provides the relevant stake distribution, the precise leader election
    --   must still live in the consensus layer since that depends on the
    --   computation (and sampling) of entropy, which is done consensus side,
    --   not ledger side (the reward calculation does not depend on this).
    type LedgerView p :: Type;
    
    -- | Validation errors
    type ValidationErr p :: Type;
    
    -- | View on a header required to validate it
    type ValidateView p :: Type;
    type SelectView p = BlockNo;
}

-- | Check if a node is the leader
checkIsLeader :: (ConsensusProtocol p, HasCallStack) => ConsensusConfig p -> CanBeLeader p -> SlotNo -> Ticked (ChainDepState p) -> Maybe (IsLeader p)

-- | Tick the <a>ChainDepState</a>
--   
--   We pass the <a>LedgerView</a> to <a>tickChainDepState</a>. Functions
--   that <i>take</i> a ticked <a>ChainDepState</a> are not separately
--   passed a ledger view; protocols that require it, can include it in
--   their ticked <a>ChainDepState</a> type.
tickChainDepState :: ConsensusProtocol p => ConsensusConfig p -> LedgerView p -> SlotNo -> ChainDepState p -> Ticked (ChainDepState p)

-- | Apply a header
updateChainDepState :: (ConsensusProtocol p, HasCallStack) => ConsensusConfig p -> ValidateView p -> SlotNo -> Ticked (ChainDepState p) -> Except (ValidationErr p) (ChainDepState p)

-- | Re-apply a header to the same <a>ChainDepState</a> we have been able
--   to successfully apply to before.
--   
--   Since a header can only be applied to a single, specific,
--   <a>ChainDepState</a>, if we apply a previously applied header again it
--   will be applied in the very same <a>ChainDepState</a>, and therefore
--   can't possibly fail.
--   
--   It is worth noting that since we already know that the header is valid
--   w.r.t. the provided <a>ChainDepState</a>, no validation checks should
--   be performed.
reupdateChainDepState :: (ConsensusProtocol p, HasCallStack) => ConsensusConfig p -> ValidateView p -> SlotNo -> Ticked (ChainDepState p) -> ChainDepState p

-- | We require that protocols support a <tt>k</tt> security parameter
protocolSecurityParam :: ConsensusProtocol p => ConsensusConfig p -> SecurityParam

-- | Compare a candidate chain to our own
--   
--   If both chains are equally preferable, the Ouroboros class of
--   consensus protocols <i>always</i> sticks with the current chain.
preferCandidate :: ConsensusProtocol p => proxy p -> SelectView p -> SelectView p -> Bool

-- | Protocol security parameter
--   
--   We interpret this as the number of rollbacks we support.
--   
--   i.e., k == 0: we can't roll back at all k == 1: we can roll back at
--   most one block, etc
--   
--   NOTE: This talks about the number of <i>blocks</i> we can roll back,
--   not the number of <i>slots</i>.
newtype SecurityParam
SecurityParam :: Word64 -> SecurityParam
[maxRollbacks] :: SecurityParam -> Word64

module Ouroboros.Consensus.Protocol.ModChainSel
data ModChainSel p s

-- | Static configuration required to run the consensus protocol
--   
--   Every method in the <a>ConsensusProtocol</a> class takes the consensus
--   configuration as a parameter, so having this as a data family rather
--   than a type family resolves most ambiguity.
--   
--   Defined out of the class so that protocols can define this type
--   without having to define the entire protocol at the same time (or
--   indeed in the same module).
data family ConsensusConfig p :: Type
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.Protocol.ModChainSel.ModChainSel p s))
instance (Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol p, GHC.Classes.Ord s, GHC.Show.Show s, Data.Typeable.Internal.Typeable s, NoThunks.Class.NoThunks s) => Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol (Ouroboros.Consensus.Protocol.ModChainSel.ModChainSel p s)
instance Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol p => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.Protocol.ModChainSel.ModChainSel p s))

module Ouroboros.Consensus.Protocol.MockChainSel

-- | Chain selection between our chain and list of candidates
--   
--   This is only a <i>model</i> of chain selection: in reality of course
--   we will not work with entire chains in memory. This function is
--   intended as an explanation of how chain selection should work
--   conceptually.
--   
--   The <tt>l</tt> parameter here models the ledger state for each chain,
--   and serves as evidence that the chains we are selecting between have
--   been validated. (It would <i>not</i> be correct to run chain selection
--   on unvalidated chains and then somehow fail if the selected chain
--   turns out to be invalid.)
--   
--   Returns <a>Nothing</a> if we stick with our current chain.
selectChain :: forall proxy p hdr l. ConsensusProtocol p => proxy p -> (hdr -> SelectView p) -> Chain hdr -> [(Chain hdr, l)] -> Maybe (Chain hdr, l)

-- | Chain selection on unvalidated chains
selectUnvalidatedChain :: ConsensusProtocol p => proxy p -> (hdr -> SelectView p) -> Chain hdr -> [Chain hdr] -> Maybe (Chain hdr)


-- | Definition is <a>IsLedger</a>
--   
--   Normally this is imported from
--   <a>Ouroboros.Consensus.Ledger.Abstract</a>. We pull this out to avoid
--   circular module dependencies.
module Ouroboros.Consensus.Ledger.Basics
class GetTip l

-- | Point of the most recently applied block
--   
--   Should be <tt>genesisPoint</tt> when no blocks have been applied yet
getTip :: GetTip l => l -> Point l
getTipHash :: GetTip l => l -> ChainHash l
getTipSlot :: GetTip l => l -> WithOrigin SlotNo

-- | The result of invoke a ledger function that does validation
--   
--   Note: we do not instantiate <a>Applicative</a> or <a>Monad</a> for
--   this type because those interfaces would typically incur space leaks.
--   We encourage you to process the events each time you invoke a ledger
--   function.
data LedgerResult l a
LedgerResult :: [AuxLedgerEvent l] -> !a -> LedgerResult l a
[lrEvents] :: LedgerResult l a -> [AuxLedgerEvent l]
[lrResult] :: LedgerResult l a -> !a

-- | A <a>Void</a> isomorph for explicitly declaring that some ledger has
--   no events
data VoidLedgerEvent l
castLedgerResult :: AuxLedgerEvent l ~ AuxLedgerEvent l' => LedgerResult l a -> LedgerResult l' a
embedLedgerResult :: (AuxLedgerEvent l -> AuxLedgerEvent l') -> LedgerResult l a -> LedgerResult l' a
pureLedgerResult :: a -> LedgerResult l a
class (Show l, Eq l, NoThunks l, NoThunks (LedgerCfg l), Show (LedgerErr l), Eq (LedgerErr l), NoThunks (LedgerErr l), GetTip l, GetTip (Ticked l)) => IsLedger l where {
    
    -- | Errors that can arise when updating the ledger
    --   
    --   This is defined here rather than in <tt>ApplyBlock</tt>, since the
    --   <i>type</i> of these errors does not depend on the type of the block.
    type LedgerErr l :: Type;
    
    -- | Event emitted by the ledger
    --   
    --   TODO we call this <a>AuxLedgerEvent</a> to differentiate from
    --   <tt>LedgerEvent</tt> in <tt>InspectLedger</tt>. When that module is
    --   rewritten to make use of ledger derived events, we may rename this
    --   type.
    type AuxLedgerEvent l :: Type;
}

-- | Apply "slot based" state transformations
--   
--   When a block is applied to the ledger state, a number of things happen
--   purely based on the slot number of that block. For example:
--   
--   <ul>
--   <li>In Byron, scheduled updates are applied, and the update system
--   state is updated.</li>
--   <li>In Shelley, delegation state is updated (on epoch
--   boundaries).</li>
--   </ul>
--   
--   The consensus layer must be able to apply such a "chain tick"
--   function, primarily when validating transactions in the mempool
--   (which, conceptually, live in "some block in the future") or when
--   extracting valid transactions from the mempool to insert into a new
--   block to be produced.
--   
--   This is not allowed to throw any errors. After all, if this could
--   fail, it would mean a <i>previous</i> block set up the ledger state in
--   such a way that as soon as a certain slot was reached, <i>any</i>
--   block would be invalid.
--   
--   PRECONDITION: The slot number must be strictly greater than the slot
--   at the tip of the ledger (except for EBBs, obviously..).
--   
--   NOTE: <a>applyChainTickLedgerResult</a> should <i>not</i> change the
--   tip of the underlying ledger state, which should still refer to the
--   most recent applied <i>block</i>. In other words, we should have
--   
--   <pre>
--      ledgerTipPoint (applyChainTick cfg slot st)
--   == ledgerTipPoint st
--   </pre>
applyChainTickLedgerResult :: IsLedger l => LedgerCfg l -> SlotNo -> l -> LedgerResult l (Ticked l)

-- | Static environment required for the ledger
type family LedgerCfg l :: Type

-- | <a>lrResult</a> after <a>applyChainTickLedgerResult</a>
applyChainTick :: IsLedger l => LedgerCfg l -> SlotNo -> l -> Ticked l
type LedgerConfig blk = LedgerCfg (LedgerState blk)
type LedgerError blk = LedgerErr (LedgerState blk)

-- | Ledger state associated with a block
data family LedgerState blk :: Type
type TickedLedgerState blk = Ticked (LedgerState blk)
instance Data.Traversable.Traversable (Ouroboros.Consensus.Ledger.Basics.LedgerResult l)
instance GHC.Base.Functor (Ouroboros.Consensus.Ledger.Basics.LedgerResult l)
instance Data.Foldable.Foldable (Ouroboros.Consensus.Ledger.Basics.LedgerResult l)


-- | Interface to the ledger layer
module Ouroboros.Consensus.Ledger.Abstract

-- | " Validated " transaction or block
--   
--   The ledger defines how to validate transactions and blocks. It's
--   possible the type before and after validation may be distinct (eg
--   Alonzo transactions), which originally motivated this family.
--   
--   We also gain the related benefit that certain interface functions,
--   such as those that <i>reapply</i> blocks, can have a more precise type
--   now. TODO
--   
--   Similarly, the Node-to-Client mini protocols can explicitly indicate
--   that the client trusts the blocks from the local server, by having the
--   server send <a>Validated</a> blocks to the client. TODO
--   
--   Note that validation has different implications for a transaction than
--   for a block. In particular, a validated transaction can be " reapplied
--   " to different ledger states, whereas a validated block must only be "
--   reapplied " to the exact same ledger state (eg as part of rebuilding
--   from an on-disk ledger snapshot).
--   
--   Since the ledger defines validation, see the ledger details for
--   concrete examples of what determines the validity (wrt to a
--   <a>LedgerState</a>) of a transaction and/or block. Example properties
--   include: a transaction's claimed inputs exist and are still unspent, a
--   block carries a sufficient cryptographic signature, etc.
data family Validated x :: Type
class (IsLedger l, HeaderHash l ~ HeaderHash blk, HasHeader blk, HasHeader (Header blk)) => ApplyBlock l blk

-- | Apply a block to the ledger state.
--   
--   This is passed the ledger state ticked with the slot of the given
--   block, so <a>applyChainTickLedgerResult</a> has already been called.
applyBlockLedgerResult :: (ApplyBlock l blk, HasCallStack) => LedgerCfg l -> blk -> Ticked l -> Except (LedgerErr l) (LedgerResult l l)

-- | Re-apply a block to the very same ledger state it was applied in
--   before.
--   
--   Since a block can only be applied to a single, specific, ledger state,
--   if we apply a previously applied block again it will be applied in the
--   very same ledger state, and therefore can't possibly fail.
--   
--   It is worth noting that since we already know that the block is valid
--   in the provided ledger state, the ledger layer should not perform
--   <i>any</i> validation checks.
reapplyBlockLedgerResult :: (ApplyBlock l blk, HasCallStack) => LedgerCfg l -> blk -> Ticked l -> LedgerResult l l

-- | Interaction with the ledger layer
class ApplyBlock (LedgerState blk) blk => UpdateLedger blk

-- | <a>lrResult</a> after <a>applyBlockLedgerResult</a>
applyLedgerBlock :: (ApplyBlock l blk, HasCallStack) => LedgerCfg l -> blk -> Ticked l -> Except (LedgerErr l) l
foldLedger :: ApplyBlock l blk => LedgerCfg l -> [blk] -> l -> Except (LedgerErr l) l

-- | <a>lrResult</a> after <a>reapplyBlockLedgerResult</a>
reapplyLedgerBlock :: (ApplyBlock l blk, HasCallStack) => LedgerCfg l -> blk -> Ticked l -> l
refoldLedger :: ApplyBlock l blk => LedgerCfg l -> [blk] -> l -> l
tickThenApply :: ApplyBlock l blk => LedgerCfg l -> blk -> l -> Except (LedgerErr l) l
tickThenApplyLedgerResult :: ApplyBlock l blk => LedgerCfg l -> blk -> l -> Except (LedgerErr l) (LedgerResult l l)
tickThenReapply :: ApplyBlock l blk => LedgerCfg l -> blk -> l -> l
tickThenReapplyLedgerResult :: ApplyBlock l blk => LedgerCfg l -> blk -> l -> LedgerResult l l
ledgerTipHash :: UpdateLedger blk => LedgerState blk -> ChainHash blk
ledgerTipPoint :: UpdateLedger blk => LedgerState blk -> Point blk
ledgerTipSlot :: UpdateLedger blk => LedgerState blk -> WithOrigin SlotNo

module Ouroboros.Consensus.Ledger.SupportsPeerSelection
class LedgerSupportsPeerSelection blk

-- | Return peers registered in the ledger ordered by descending
--   <a>PoolStake</a>.
--   
--   For example, for Shelley, the relays that have been registered in the
--   ledger for the respective stake pools will be returned.
--   
--   Ledgers/blocks that don't support staking can return an empty list.
--   
--   Note: if the ledger state is old, the registered relays can also be
--   old and may no longer be online.
getPeers :: LedgerSupportsPeerSelection blk => LedgerState blk -> [(PoolStake, NonEmpty StakePoolRelay)]
newtype () => PoolStake
PoolStake :: Rational -> PoolStake
[unPoolStake] :: PoolStake -> Rational

-- | A relay registered for a stake pool
data StakePoolRelay

-- | One of the current relays
CurrentRelay :: RelayAccessPoint -> StakePoolRelay

-- | One of the future relays
FutureRelay :: RelayAccessPoint -> StakePoolRelay
stakePoolRelayAccessPoint :: StakePoolRelay -> RelayAccessPoint
data () => DomainAccessPoint
DomainAccessPoint :: !Domain -> !PortNumber -> DomainAccessPoint
[dapDomain] :: DomainAccessPoint -> !Domain
[dapPortNumber] :: DomainAccessPoint -> !PortNumber
data () => IP
IPv4 :: IPv4 -> IP
[ipv4] :: IP -> IPv4
IPv6 :: IPv6 -> IP
[ipv6] :: IP -> IPv6
data () => PortNumber
data () => RelayAccessPoint
RelayAccessDomain :: !Domain -> !PortNumber -> RelayAccessPoint
RelayAccessAddress :: !IP -> !PortNumber -> RelayAccessPoint
pattern RelayDomainAccessPoint :: DomainAccessPoint -> RelayAccessPoint
instance GHC.Classes.Eq Ouroboros.Consensus.Ledger.SupportsPeerSelection.StakePoolRelay
instance GHC.Show.Show Ouroboros.Consensus.Ledger.SupportsPeerSelection.StakePoolRelay
instance Control.DeepSeq.NFData Ouroboros.Consensus.Ledger.SupportsPeerSelection.StakePoolRelay

module Ouroboros.Consensus.Ledger.CommonProtocolParams

-- | Ask the ledger for common protocol parameters.
class UpdateLedger blk => CommonProtocolParams blk

-- | The maximum header size in bytes according to the currently adopted
--   protocol parameters of the ledger state.
maxHeaderSize :: CommonProtocolParams blk => LedgerState blk -> Word32

-- | The maximum transaction size in bytes according to the currently
--   adopted protocol parameters of the ledger state.
maxTxSize :: CommonProtocolParams blk => LedgerState blk -> Word32

module Ouroboros.Consensus.Config

-- | The top-level node configuration
data TopLevelConfig blk
TopLevelConfig :: !ConsensusConfig (BlockProtocol blk) -> !LedgerConfig blk -> !BlockConfig blk -> !CodecConfig blk -> !StorageConfig blk -> TopLevelConfig blk
[topLevelConfigProtocol] :: TopLevelConfig blk -> !ConsensusConfig (BlockProtocol blk)
[topLevelConfigLedger] :: TopLevelConfig blk -> !LedgerConfig blk
[topLevelConfigBlock] :: TopLevelConfig blk -> !BlockConfig blk
[topLevelConfigCodec] :: TopLevelConfig blk -> !CodecConfig blk
[topLevelConfigStorage] :: TopLevelConfig blk -> !StorageConfig blk
castTopLevelConfig :: (Coercible (ConsensusConfig (BlockProtocol blk)) (ConsensusConfig (BlockProtocol blk')), LedgerConfig blk ~ LedgerConfig blk', Coercible (BlockConfig blk) (BlockConfig blk'), Coercible (CodecConfig blk) (CodecConfig blk'), Coercible (StorageConfig blk) (StorageConfig blk')) => TopLevelConfig blk -> TopLevelConfig blk'
mkTopLevelConfig :: ConsensusConfig (BlockProtocol blk) -> LedgerConfig blk -> BlockConfig blk -> CodecConfig blk -> StorageConfig blk -> TopLevelConfig blk
configBlock :: TopLevelConfig blk -> BlockConfig blk
configCodec :: TopLevelConfig blk -> CodecConfig blk
configConsensus :: TopLevelConfig blk -> ConsensusConfig (BlockProtocol blk)
configLedger :: TopLevelConfig blk -> LedgerConfig blk
configStorage :: TopLevelConfig blk -> StorageConfig blk
configSecurityParam :: ConsensusProtocol (BlockProtocol blk) => TopLevelConfig blk -> SecurityParam
instance GHC.Generics.Generic (Ouroboros.Consensus.Config.TopLevelConfig blk)
instance (Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Basics.LedgerConfig blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.BlockConfig blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.CodecConfig blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.StorageConfig blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Config.TopLevelConfig blk)

module Ouroboros.Consensus.Block.SupportsProtocol

-- | Evidence that a block supports its protocol
class (GetHeader blk, GetPrevHash blk, ConsensusProtocol (BlockProtocol blk), NoThunks (Header blk), NoThunks (BlockConfig blk), NoThunks (CodecConfig blk), NoThunks (StorageConfig blk)) => BlockSupportsProtocol blk
validateView :: BlockSupportsProtocol blk => BlockConfig blk -> Header blk -> ValidateView (BlockProtocol blk)
selectView :: BlockSupportsProtocol blk => BlockConfig blk -> Header blk -> SelectView (BlockProtocol blk)
selectView :: (BlockSupportsProtocol blk, SelectView (BlockProtocol blk) ~ BlockNo) => BlockConfig blk -> Header blk -> SelectView (BlockProtocol blk)


-- | See <a>BlockSupportsMetrics</a>.
module Ouroboros.Consensus.Block.SupportsMetrics

-- | Evidence that a block supports the metrics needed for business
--   requirements
--   
--   For example, we use <a>isSelfIssued</a> to help Stake Pool Operators
--   monitor how many of their forged blocks have not ended up on their
--   chain.
class BlockSupportsMetrics blk

-- | See <a>WhetherSelfIssued</a>.
isSelfIssued :: BlockSupportsMetrics blk => BlockConfig blk -> Header blk -> WhetherSelfIssued

-- | Whether a block was issued by a stakeholder currently forging on this
--   node
data WhetherSelfIssued
IsSelfIssued :: WhetherSelfIssued
IsNotSelfIssued :: WhetherSelfIssued

-- | We are unable to determine
UnknownSelfIssued :: WhetherSelfIssued

-- | Use this default for block types that cannot or do not yet support the
--   predicate
isSelfIssuedConstUnknown :: BlockConfig blk -> Header blk -> WhetherSelfIssued
instance GHC.Classes.Eq Ouroboros.Consensus.Block.SupportsMetrics.WhetherSelfIssued
instance GHC.Show.Show Ouroboros.Consensus.Block.SupportsMetrics.WhetherSelfIssued

module Ouroboros.Consensus.Block.RealPoint

-- | Point of an actual block (i.e., not genesis)
data RealPoint blk
RealPoint :: !SlotNo -> !HeaderHash blk -> RealPoint blk
decodeRealPoint :: (forall s. Decoder s (HeaderHash blk)) -> forall s. Decoder s (RealPoint blk)
encodeRealPoint :: (HeaderHash blk -> Encoding) -> RealPoint blk -> Encoding
blockRealPoint :: HasHeader blk => blk -> RealPoint blk
castRealPoint :: forall blk blk'. Coercible (HeaderHash blk) (HeaderHash blk') => RealPoint blk -> RealPoint blk'
headerRealPoint :: (HasHeader blk, HasHeader (Header blk)) => Header blk -> RealPoint blk
pointToWithOriginRealPoint :: Point blk -> WithOrigin (RealPoint blk)
realPointHash :: RealPoint blk -> HeaderHash blk
realPointSlot :: RealPoint blk -> SlotNo
realPointToPoint :: RealPoint blk -> Point blk
withOriginRealPointToPoint :: WithOrigin (RealPoint blk) -> Point blk
instance GHC.Generics.Generic (Ouroboros.Consensus.Block.RealPoint.RealPoint blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Block.RealPoint.RealPoint blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Ord (Ouroboros.Consensus.Block.RealPoint.RealPoint blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Block.RealPoint.RealPoint blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.RealPoint.RealPoint blk)
instance Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.HeaderHash blk) => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Block.RealPoint.RealPoint blk)

module Ouroboros.Consensus.Util.MonadSTM.StrictSVar
castStrictSVar :: (TMVar m ~ TMVar n, TVar m ~ TVar n) => StrictSVar m a -> StrictSVar n a
isEmptySVar :: MonadSTM m => StrictSVar m a -> m Bool
modifySVar :: (MonadSTM m, MonadCatch m, HasCallStack) => StrictSVar m a -> (a -> m (a, b)) -> m b
modifySVar_ :: (MonadSTM m, MonadCatch m, HasCallStack) => StrictSVar m a -> (a -> m a) -> m ()
newEmptySVar :: MonadSTM m => a -> m (StrictSVar m a)

-- | Create an initially empty <a>StrictSVar</a>
--   
--   NOTE: Since <a>readSVarSTM</a> allows to read the <a>StrictSVar</a>
--   even when it is empty, we need an initial value of <tt>a</tt> even
--   though the <a>StrictSVar</a> starts out empty. However, we are
--   <i>NOT</i> strict in this value, to allow it to be <tt>error</tt>.
newEmptySVarWithInvariant :: MonadSTM m => (a -> Maybe String) -> a -> m (StrictSVar m a)
newSVar :: MonadSTM m => a -> m (StrictSVar m a)
newSVarWithInvariant :: (MonadSTM m, HasCallStack) => (a -> Maybe String) -> a -> m (StrictSVar m a)
putSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> a -> m ()
readSVar :: MonadSTM m => StrictSVar m a -> m a

-- | Read the possibly-stale value of the <tt>SVar</tt>
--   
--   Will return the current value of the <tt>SVar</tt> if it non-empty, or
--   the last known value otherwise.
readSVarSTM :: MonadSTM m => StrictSVar m a -> STM m a

-- | Swap value of a <a>StrictSVar</a>
--   
--   NOTE: Since swapping the value can't leave the <a>StrictSVar</a>
--   empty, we <i>could</i> check the invariant first and only then swap.
--   We nonetheless swap first and check the invariant after to keep the
--   semantics the same with <a>putSVar</a>, otherwise it will be difficult
--   to understand when a <a>StrictSVar</a> is updated and when it is not.
swapSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> a -> m a
takeSVar :: MonadSTM m => StrictSVar m a -> m a
tryPutSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> a -> m Bool
tryReadSVar :: MonadSTM m => StrictSVar m a -> m (Maybe a)
tryTakeSVar :: MonadSTM m => StrictSVar m a -> m (Maybe a)
updateSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> (a -> (a, b)) -> m b
updateSVar_ :: (MonadSTM m, HasCallStack) => StrictSVar m a -> (a -> a) -> m ()

-- | Strict SVar (modelled using a lazy <a>TMVar</a> under the hood)
--   
--   The <a>StrictSVar</a> API is slightly stronger than the usual
--   <tt>SVar</tt> one, as we offer a primitive to read the value of the
--   SVar even if it is empty (in which case we will return the oldest
--   known stale one). See <a>readSVarSTM</a>.
--   
--   There is a weaker invariant for a <a>StrictSVar</a> than for a
--   <tt>StrictTVar</tt>: although all functions that modify the
--   <a>StrictSVar</a> check the invariant, we do <i>not</i> guarantee that
--   the value inside the <a>StrictSVar</a> always satisfies the invariant.
--   Instead, we <i>do</i> guarantee that if the <a>StrictSVar</a> is
--   updated with a value that does not satisfy the invariant, an exception
--   is thrown. The reason for this weaker guarantee is that leaving an
--   <tt>SVar</tt> empty can lead to very hard to debug "blocked
--   indefinitely" problems.
--   
--   This is also the reason we do not offer support for an invariant in
--   <tt>StrictTMVar</tt>: if we throw an exception from an STM
--   transaction, the STM transaction is not executed, and so we would not
--   even be able to provide the weaker guarantee that we provide for
--   <a>StrictSVar</a>.
data StrictSVar m a
StrictSVar :: !a -> Maybe String -> !TMVar m a -> !TVar m a -> StrictSVar m a

-- | Invariant checked whenever updating the <a>StrictSVar</a>.
[invariant] :: StrictSVar m a -> !a -> Maybe String

-- | The main TMVar supporting this <a>StrictSVar</a>
[tmvar] :: StrictSVar m a -> !TMVar m a

-- | TVar for supporting <a>readSVarSTM</a>
--   
--   This TVar is always kept up to date with the <a>TMVar</a>, but holds
--   on the old value of the <a>TMVar</a> when it is empty. This is very
--   useful to support single writer/many reader scenarios.
--   
--   NOTE: We should always update the <a>tmvar</a> before the <a>tvar</a>
--   so that if the update to the <a>tmvar</a> fails, the 'tvar is left
--   unchanged.
[tvar] :: StrictSVar m a -> !TVar m a
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.MonadSTM.StrictSVar.StrictSVar GHC.Types.IO a)

module Ouroboros.Consensus.Util.MonadSTM.NormalForm

-- | Strict SVar (modelled using a lazy <a>TMVar</a> under the hood)
--   
--   The <a>StrictSVar</a> API is slightly stronger than the usual
--   <tt>SVar</tt> one, as we offer a primitive to read the value of the
--   SVar even if it is empty (in which case we will return the oldest
--   known stale one). See <a>readSVarSTM</a>.
--   
--   There is a weaker invariant for a <a>StrictSVar</a> than for a
--   <tt>StrictTVar</tt>: although all functions that modify the
--   <a>StrictSVar</a> check the invariant, we do <i>not</i> guarantee that
--   the value inside the <a>StrictSVar</a> always satisfies the invariant.
--   Instead, we <i>do</i> guarantee that if the <a>StrictSVar</a> is
--   updated with a value that does not satisfy the invariant, an exception
--   is thrown. The reason for this weaker guarantee is that leaving an
--   <tt>SVar</tt> empty can lead to very hard to debug "blocked
--   indefinitely" problems.
--   
--   This is also the reason we do not offer support for an invariant in
--   <tt>StrictTMVar</tt>: if we throw an exception from an STM
--   transaction, the STM transaction is not executed, and so we would not
--   even be able to provide the weaker guarantee that we provide for
--   <a>StrictSVar</a>.
data StrictSVar m a
StrictSVar :: !a -> Maybe String -> !TMVar m a -> !TVar m a -> StrictSVar m a

-- | Invariant checked whenever updating the <a>StrictSVar</a>.
[invariant] :: StrictSVar m a -> !a -> Maybe String

-- | The main TMVar supporting this <a>StrictSVar</a>
[tmvar] :: StrictSVar m a -> !TMVar m a

-- | TVar for supporting <a>readSVarSTM</a>
--   
--   This TVar is always kept up to date with the <a>TMVar</a>, but holds
--   on the old value of the <a>TMVar</a> when it is empty. This is very
--   useful to support single writer/many reader scenarios.
--   
--   NOTE: We should always update the <a>tmvar</a> before the <a>tvar</a>
--   so that if the update to the <a>tmvar</a> fails, the 'tvar is left
--   unchanged.
[tvar] :: StrictSVar m a -> !TVar m a
castStrictSVar :: (TMVar m ~ TMVar n, TVar m ~ TVar n) => StrictSVar m a -> StrictSVar n a
isEmptySVar :: MonadSTM m => StrictSVar m a -> m Bool
modifySVar :: (MonadSTM m, MonadCatch m, HasCallStack) => StrictSVar m a -> (a -> m (a, b)) -> m b
modifySVar_ :: (MonadSTM m, MonadCatch m, HasCallStack) => StrictSVar m a -> (a -> m a) -> m ()
putSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> a -> m ()
readSVar :: MonadSTM m => StrictSVar m a -> m a

-- | Read the possibly-stale value of the <tt>SVar</tt>
--   
--   Will return the current value of the <tt>SVar</tt> if it non-empty, or
--   the last known value otherwise.
readSVarSTM :: MonadSTM m => StrictSVar m a -> STM m a

-- | Swap value of a <a>StrictSVar</a>
--   
--   NOTE: Since swapping the value can't leave the <a>StrictSVar</a>
--   empty, we <i>could</i> check the invariant first and only then swap.
--   We nonetheless swap first and check the invariant after to keep the
--   semantics the same with <a>putSVar</a>, otherwise it will be difficult
--   to understand when a <a>StrictSVar</a> is updated and when it is not.
swapSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> a -> m a
takeSVar :: MonadSTM m => StrictSVar m a -> m a
tryPutSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> a -> m Bool
tryReadSVar :: MonadSTM m => StrictSVar m a -> m (Maybe a)
tryTakeSVar :: MonadSTM m => StrictSVar m a -> m (Maybe a)
updateSVar :: (MonadSTM m, HasCallStack) => StrictSVar m a -> (a -> (a, b)) -> m b
updateSVar_ :: (MonadSTM m, HasCallStack) => StrictSVar m a -> (a -> a) -> m ()
class (Monad m, Monad STM m) => MonadSTM (m :: Type -> Type) where {
    type family STM (m :: Type -> Type) = (stm :: Type -> Type) | stm -> m;
}
atomically :: (MonadSTM m, HasCallStack) => STM m a -> m a
retry :: MonadSTM m => STM m a
orElse :: MonadSTM m => STM m a -> STM m a -> STM m a
check :: MonadSTM m => Bool -> STM m ()
type family STM (m :: Type -> Type) = (stm :: Type -> Type) | stm -> m
class (MonadSTM m, Monad InspectMonad m) => MonadInspectSTM (m :: Type -> Type) where {
    type family InspectMonad (m :: Type -> Type) :: Type -> Type;
}
inspectTVar :: MonadInspectSTM m => proxy m -> TVar m a -> InspectMonad m a
inspectTMVar :: MonadInspectSTM m => proxy m -> TMVar m a -> InspectMonad m (Maybe a)
type family InspectMonad (m :: Type -> Type) :: Type -> Type
class MonadSTM m => MonadLabelledSTM (m :: Type -> Type)
class MonadInspectSTM m => MonadTraceSTM (m :: Type -> Type)
traceTMVar :: MonadTraceSTM m => proxy m -> TMVar m a -> (Maybe (Maybe a) -> Maybe a -> InspectMonad m TraceValue) -> STM m ()
traceTQueue :: MonadTraceSTM m => proxy m -> TQueue m a -> (Maybe [a] -> [a] -> InspectMonad m TraceValue) -> STM m ()
traceTBQueue :: MonadTraceSTM m => proxy m -> TBQueue m a -> (Maybe [a] -> [a] -> InspectMonad m TraceValue) -> STM m ()
traceTSem :: MonadTraceSTM m => proxy m -> TSem m -> (Maybe Integer -> Integer -> InspectMonad m TraceValue) -> STM m ()
traceTMVarIO :: MonadTraceSTM m => TMVar m a -> (Maybe (Maybe a) -> Maybe a -> InspectMonad m TraceValue) -> m ()
traceTQueueIO :: MonadTraceSTM m => TQueue m a -> (Maybe [a] -> [a] -> InspectMonad m TraceValue) -> m ()
traceTBQueueIO :: MonadTraceSTM m => TBQueue m a -> (Maybe [a] -> [a] -> InspectMonad m TraceValue) -> m ()
traceTSemIO :: MonadTraceSTM m => TSem m -> (Maybe Integer -> Integer -> InspectMonad m TraceValue) -> m ()
data () => TraceValue
[TraceValue] :: forall tr. Typeable tr => Maybe tr -> Maybe String -> TraceValue
pattern DontTrace :: TraceValue
pattern TraceDynamic :: () => Typeable tr => tr -> TraceValue
pattern TraceString :: String -> TraceValue
data () => StrictTMVar (m :: Type -> Type) a
type LazyTMVar (m :: Type -> Type) = TMVar m
throwSTM :: forall (m :: Type -> Type) e a. (MonadSTM m, MonadThrow (STM m), Exception e) => e -> STM m a
newEmptyTMVar :: forall (m :: Type -> Type) a. MonadSTM m => STM m (StrictTMVar m a)
newEmptyTMVarIO :: MonadSTM m => m (StrictTMVar m a)
takeTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> STM m a
tryTakeTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> STM m (Maybe a)
putTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> a -> STM m ()
tryPutTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> a -> STM m Bool
readTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> STM m a
tryReadTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> STM m (Maybe a)
swapTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> a -> STM m a
isEmptyTMVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTMVar m a -> STM m Bool
labelTMVar :: forall (m :: Type -> Type) a. MonadLabelledSTM m => StrictTMVar m a -> String -> STM m ()
labelTMVarIO :: MonadLabelledSTM m => StrictTMVar m a -> String -> m ()
castStrictTMVar :: forall (m :: Type -> Type) (n :: Type -> Type) a. LazyTMVar m ~ LazyTMVar n => StrictTMVar m a -> StrictTMVar n a
fromLazyTMVar :: forall (m :: Type -> Type) a. LazyTMVar m a -> StrictTMVar m a
newEmptySVar :: (MonadSTM m, NoThunks a) => a -> m (StrictSVar m a)
newSVar :: (MonadSTM m, HasCallStack, NoThunks a) => a -> m (StrictSVar m a)
uncheckedNewEmptySVar :: MonadSTM m => a -> m (StrictSVar m a)
uncheckedNewSVar :: MonadSTM m => a -> m (StrictSVar m a)


-- | <a>StrictMVar</a>s with <a>NoThunks</a> invariants.
--   
--   Custom invariants can still be specified in addition to the default
--   <a>NoThunks</a> invariant. See <a>newMVarWithInvariant</a> and
--   <a>newEmptyMVarWithInvariant</a>.
--   
--   Use the <tt>checkmvarinvariants</tt> cabal flag from the
--   <tt>strict-checked-vars</tt> package to enable or disable invariant
--   checks at compile time.
--   
--   The exports of this module (should) mirror the exports of the
--   <a>Control.Concurrent.Class.MonadMVar.Strict.Checked</a> module from
--   the <tt>strict-checked-vars</tt> package.
module Ouroboros.Consensus.Util.NormalForm.StrictMVar

-- | Create an empty <a>StrictMVar</a> with a <a>NoThunks</a> invariant.
newEmptyMVar :: (MonadMVar m, NoThunks a) => m (StrictMVar m a)

-- | Create an empty <a>StrictMVar</a> with a custom invariant <i>and</i> a
--   <a>NoThunks</a> invariant.
--   
--   When both the custom and <a>NoThunks</a> invariants are broken, only
--   the error related to the custom invariant is reported.
newEmptyMVarWithInvariant :: (MonadMVar m, NoThunks a) => (a -> Maybe String) -> m (StrictMVar m a)

-- | Create a <a>StrictMVar</a> with a <a>NoThunks</a> invariant.
newMVar :: (HasCallStack, MonadMVar m, NoThunks a) => a -> m (StrictMVar m a)

-- | Create a <a>StrictMVar</a> with a custom invariant <i>and</i> a
--   <a>NoThunks</a> invariant.
--   
--   When both the custom and <a>NoThunks</a> invariants are broken, only
--   the error related to the custom invariant is reported.
newMVarWithInvariant :: (HasCallStack, MonadMVar m, NoThunks a) => (a -> Maybe String) -> a -> m (StrictMVar m a)
noThunksInvariant :: NoThunks a => a -> Maybe String

-- | Like <a>newEmptyMVar</a>, but without a <a>NoThunks</a> invariant.
uncheckedNewEmptyMVar :: MonadMVar m => m (StrictMVar m a)

-- | Like <a>newMVar</a>, but without a <a>NoThunks</a> invariant.
uncheckedNewMVar :: MonadMVar m => a -> m (StrictMVar m a)
class Monad m => MonadMVar (m :: Type -> Type)
data () => StrictMVar (m :: Type -> Type) a
type LazyMVar (m :: Type -> Type) = MVar m
takeMVar :: MonadMVar m => StrictMVar m a -> m a
readMVar :: MonadMVar m => StrictMVar m a -> m a
putMVar :: (HasCallStack, MonadMVar m) => StrictMVar m a -> a -> m ()
tryTakeMVar :: MonadMVar m => StrictMVar m a -> m (Maybe a)
tryPutMVar :: (HasCallStack, MonadMVar m) => StrictMVar m a -> a -> m Bool
tryReadMVar :: MonadMVar m => StrictMVar m a -> m (Maybe a)
isEmptyMVar :: MonadMVar m => StrictMVar m a -> m Bool
withMVar :: MonadMVar m => StrictMVar m a -> (a -> m b) -> m b
modifyMVar_ :: (HasCallStack, MonadMVar m) => StrictMVar m a -> (a -> m a) -> m ()
swapMVar :: (HasCallStack, MonadMVar m) => StrictMVar m a -> a -> m a
withMVarMasked :: MonadMVar m => StrictMVar m a -> (a -> m b) -> m b
modifyMVar :: (HasCallStack, MonadMVar m) => StrictMVar m a -> (a -> m (a, b)) -> m b
modifyMVarMasked_ :: (HasCallStack, MonadMVar m) => StrictMVar m a -> (a -> m a) -> m ()
modifyMVarMasked :: (HasCallStack, MonadMVar m) => StrictMVar m a -> (a -> m (a, b)) -> m b
checkInvariant :: HasCallStack => Maybe String -> a -> a
toLazyMVar :: forall (m :: Type -> Type) a. StrictMVar m a -> LazyMVar m a
castStrictMVar :: forall (m :: Type -> Type) (n :: Type -> Type) a. LazyMVar m ~ LazyMVar n => StrictMVar m a -> StrictMVar n a
fromLazyMVar :: forall (m :: Type -> Type) a. LazyMVar m a -> StrictMVar m a
unsafeToUncheckedStrictMVar :: forall (m :: Type -> Type) a. StrictMVar m a -> StrictMVar m a
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Control.Concurrent.Class.MonadMVar.Strict.Checked.StrictMVar GHC.Types.IO a)


-- | <a>StrictTVar</a>s with <a>NoThunks</a> invariants.
--   
--   Custom invariants can still be specified in addition to the default
--   <a>NoThunks</a> invariant. See <a>newTVarWithInvariant</a> and
--   <a>newTVarWithInvariantIO</a>.
--   
--   Use the <tt>checktvarinvariants</tt> cabal flag from the
--   <tt>strict-checked-vars</tt> package to enable or disable invariant
--   checks at compile time.
--   
--   The exports of this module (should) mirror the exports of the
--   <a>Control.Concurrent.Class.MonadSTM.Strict.TVar.Checked</a> module
--   from the <tt>strict-checked-vars</tt> package.
module Ouroboros.Consensus.Util.NormalForm.StrictTVar

-- | Create a <a>StrictTVar</a> with a <a>NoThunks</a> invariant.
newTVar :: (HasCallStack, MonadSTM m, NoThunks a) => a -> STM m (StrictTVar m a)

-- | Create an <a>StrictTVar</a> with a <a>NoThunks</a> invariant.
newTVarIO :: (HasCallStack, MonadSTM m, NoThunks a) => a -> m (StrictTVar m a)

-- | Create a <a>StrictTVar</a> with a custom invariant <i>and</i> a
--   <a>NoThunks</a> invariant.
--   
--   When both the custom and <a>NoThunks</a> invariants are broken, only
--   the error related to the custom invariant is reported.
newTVarWithInvariant :: (HasCallStack, MonadSTM m, NoThunks a) => (a -> Maybe String) -> a -> STM m (StrictTVar m a)

-- | Create a <a>StrictTVar</a> with a custom invariant <i>and</i> a
--   <a>NoThunks</a> invariant.
--   
--   When both the custom and <a>NoThunks</a> invariants are broken, only
--   the error related to the custom invariant is reported.
newTVarWithInvariantIO :: (HasCallStack, MonadSTM m, NoThunks a) => (a -> Maybe String) -> a -> m (StrictTVar m a)
noThunksInvariant :: NoThunks a => a -> Maybe String

-- | Like <a>newTVarIO</a>, but without a <a>NoThunks</a> invariant.
uncheckedNewTVarM :: MonadSTM m => a -> m (StrictTVar m a)
data () => StrictTVar (m :: Type -> Type) a
type LazyTVar (m :: Type -> Type) = LazyTVar m
readTVarIO :: MonadSTM m => StrictTVar m a -> m a
readTVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTVar m a -> STM m a
writeTVar :: forall (m :: Type -> Type) a. (MonadSTM m, HasCallStack) => StrictTVar m a -> a -> STM m ()
modifyTVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTVar m a -> (a -> a) -> STM m ()
stateTVar :: forall (m :: Type -> Type) s a. MonadSTM m => StrictTVar m s -> (s -> (a, s)) -> STM m a
swapTVar :: forall (m :: Type -> Type) a. MonadSTM m => StrictTVar m a -> a -> STM m a
traceTVar :: forall (m :: Type -> Type) proxy a. MonadTraceSTM m => proxy m -> StrictTVar m a -> (Maybe a -> a -> InspectMonad m TraceValue) -> STM m ()
traceTVarIO :: MonadTraceSTM m => StrictTVar m a -> (Maybe a -> a -> InspectMonad m TraceValue) -> m ()
labelTVar :: forall (m :: Type -> Type) a. MonadLabelledSTM m => StrictTVar m a -> String -> STM m ()
labelTVarIO :: MonadLabelledSTM m => StrictTVar m a -> String -> m ()
castStrictTVar :: forall (m :: Type -> Type) (n :: Type -> Type) a. LazyTVar m ~ LazyTVar n => StrictTVar m a -> StrictTVar n a
fromLazyTVar :: forall (m :: Type -> Type) a. LazyTVar m a -> StrictTVar m a
toLazyTVar :: forall (m :: Type -> Type) a. StrictTVar m a -> LazyTVar m a
unsafeToUncheckedStrictTVar :: forall (m :: Type -> Type) a. StrictTVar m a -> StrictTVar m a
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Control.Concurrent.Class.MonadSTM.Strict.TVar.Checked.StrictTVar GHC.Types.IO a)

module Ouroboros.Consensus.Util.Orphans
instance NoThunks.Class.NoThunks (Codec.CBOR.Decoding.Decoder s a)
instance NoThunks.Class.NoThunks (Control.Tracer.Tracer m ev)
instance NoThunks.Class.NoThunks (System.FS.API.Types.Handle h)
instance NoThunks.Class.NoThunks System.FS.API.Types.FsPath
instance NoThunks.Class.NoThunks (System.FS.API.SomeHasFS m)
instance NoThunks.Class.NoThunks System.FS.CRC.CRC
instance Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.HeaderHash block) => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.Point block)
instance Ouroboros.Consensus.Util.Condense.Condense block => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Mock.Chain.Chain block)
instance (Ouroboros.Consensus.Util.Condense.Condense block, Ouroboros.Network.Block.HasHeader block, Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.Block.HeaderHash block)) => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Network.AnchoredFragment.AnchoredFragment block)
instance Codec.Serialise.Class.Serialise (Cardano.Crypto.Hash.Class.Hash h a)
instance Codec.Serialise.Class.Serialise (Cardano.Crypto.DSIGN.Class.VerKeyDSIGN Cardano.Crypto.DSIGN.Mock.MockDSIGN)
instance Ouroboros.Network.Util.ShowProxy.ShowProxy Cardano.Slotting.Slot.SlotNo
instance (NoThunks.Class.NoThunks k, NoThunks.Class.NoThunks v) => NoThunks.Class.NoThunks (Data.Bimap.Bimap k v)
instance (NoThunks.Class.NoThunks p, NoThunks.Class.NoThunks v, GHC.Classes.Ord p) => NoThunks.Class.NoThunks (Data.IntPSQ.Internal.IntPSQ p v)
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Data.SOP.BasicFunctors.K a b)

module Ouroboros.Consensus.Util.IOLike
class (MonadAsync m, MonadMVar m, MonadEventlog m, MonadFork m, MonadST m, MonadDelay m, MonadThread m, MonadThrow m, MonadCatch m, MonadMask m, MonadMonotonicTime m, MonadEvaluate m, Alternative (STM m), MonadCatch (STM m), forall a. NoThunks (m a), forall a. NoThunks a => NoThunks (StrictTVar m a), forall a. NoThunks a => NoThunks (StrictSVar m a), forall a. NoThunks a => NoThunks (StrictMVar m a)) => IOLike m

-- | Securely forget a KES signing key.
--   
--   No-op for the IOSim, but <a>forgetSignKeyKES</a> for IO.
forgetSignKeyKES :: (IOLike m, KESAlgorithm v) => SignKeyKES v -> m ()

-- | Any type that you wish to throw or catch as an exception must be an
--   instance of the <tt>Exception</tt> class. The simplest case is a new
--   exception type directly below the root:
--   
--   <pre>
--   data MyException = ThisException | ThatException
--       deriving Show
--   
--   instance Exception MyException
--   </pre>
--   
--   The default method definitions in the <tt>Exception</tt> class do what
--   we need in this case. You can now throw and catch
--   <tt>ThisException</tt> and <tt>ThatException</tt> as exceptions:
--   
--   <pre>
--   *Main&gt; throw ThisException `catch` \e -&gt; putStrLn ("Caught " ++ show (e :: MyException))
--   Caught ThisException
--   </pre>
--   
--   In more complicated examples, you may wish to define a whole hierarchy
--   of exceptions:
--   
--   <pre>
--   ---------------------------------------------------------------------
--   -- Make the root exception type for all the exceptions in a compiler
--   
--   data SomeCompilerException = forall e . Exception e =&gt; SomeCompilerException e
--   
--   instance Show SomeCompilerException where
--       show (SomeCompilerException e) = show e
--   
--   instance Exception SomeCompilerException
--   
--   compilerExceptionToException :: Exception e =&gt; e -&gt; SomeException
--   compilerExceptionToException = toException . SomeCompilerException
--   
--   compilerExceptionFromException :: Exception e =&gt; SomeException -&gt; Maybe e
--   compilerExceptionFromException x = do
--       SomeCompilerException a &lt;- fromException x
--       cast a
--   
--   ---------------------------------------------------------------------
--   -- Make a subhierarchy for exceptions in the frontend of the compiler
--   
--   data SomeFrontendException = forall e . Exception e =&gt; SomeFrontendException e
--   
--   instance Show SomeFrontendException where
--       show (SomeFrontendException e) = show e
--   
--   instance Exception SomeFrontendException where
--       toException = compilerExceptionToException
--       fromException = compilerExceptionFromException
--   
--   frontendExceptionToException :: Exception e =&gt; e -&gt; SomeException
--   frontendExceptionToException = toException . SomeFrontendException
--   
--   frontendExceptionFromException :: Exception e =&gt; SomeException -&gt; Maybe e
--   frontendExceptionFromException x = do
--       SomeFrontendException a &lt;- fromException x
--       cast a
--   
--   ---------------------------------------------------------------------
--   -- Make an exception type for a particular frontend compiler exception
--   
--   data MismatchedParentheses = MismatchedParentheses
--       deriving Show
--   
--   instance Exception MismatchedParentheses where
--       toException   = frontendExceptionToException
--       fromException = frontendExceptionFromException
--   </pre>
--   
--   We can now catch a <tt>MismatchedParentheses</tt> exception as
--   <tt>MismatchedParentheses</tt>, <tt>SomeFrontendException</tt> or
--   <tt>SomeCompilerException</tt>, but not other types, e.g.
--   <tt>IOException</tt>:
--   
--   <pre>
--   *Main&gt; throw MismatchedParentheses `catch` \e -&gt; putStrLn ("Caught " ++ show (e :: MismatchedParentheses))
--   Caught MismatchedParentheses
--   *Main&gt; throw MismatchedParentheses `catch` \e -&gt; putStrLn ("Caught " ++ show (e :: SomeFrontendException))
--   Caught MismatchedParentheses
--   *Main&gt; throw MismatchedParentheses `catch` \e -&gt; putStrLn ("Caught " ++ show (e :: SomeCompilerException))
--   Caught MismatchedParentheses
--   *Main&gt; throw MismatchedParentheses `catch` \e -&gt; putStrLn ("Caught " ++ show (e :: IOException))
--   *** Exception: MismatchedParentheses
--   </pre>
class (Typeable e, Show e) => Exception e
toException :: Exception e => e -> SomeException
fromException :: Exception e => SomeException -> Maybe e

-- | Render this exception value in a human-friendly manner.
--   
--   Default implementation: <tt><a>show</a></tt>.
displayException :: Exception e => e -> String
data () => ExitCase a
ExitCaseSuccess :: a -> ExitCase a
ExitCaseException :: SomeException -> ExitCase a
ExitCaseAbort :: ExitCase a
class MonadThrow m => MonadCatch (m :: Type -> Type)
catch :: (MonadCatch m, Exception e) => m a -> (e -> m a) -> m a
catchJust :: (MonadCatch m, Exception e) => (e -> Maybe b) -> m a -> (b -> m a) -> m a
try :: (MonadCatch m, Exception e) => m a -> m (Either e a)
tryJust :: (MonadCatch m, Exception e) => (e -> Maybe b) -> m a -> m (Either b a)
handle :: (MonadCatch m, Exception e) => (e -> m a) -> m a -> m a
handleJust :: (MonadCatch m, Exception e) => (e -> Maybe b) -> (b -> m a) -> m a -> m a
onException :: MonadCatch m => m a -> m b -> m a
bracketOnError :: MonadCatch m => m a -> (a -> m b) -> (a -> m c) -> m c
generalBracket :: MonadCatch m => m a -> (a -> ExitCase b -> m c) -> (a -> m b) -> m (b, c)
class MonadCatch m => MonadMask (m :: Type -> Type)
mask :: MonadMask m => ((forall a. () => m a -> m a) -> m b) -> m b
uninterruptibleMask :: MonadMask m => ((forall a. () => m a -> m a) -> m b) -> m b
mask_ :: MonadMask m => m a -> m a
uninterruptibleMask_ :: MonadMask m => m a -> m a
class Monad m => MonadThrow (m :: Type -> Type)
throwIO :: (MonadThrow m, Exception e) => e -> m a
bracket :: MonadThrow m => m a -> (a -> m b) -> (a -> m c) -> m c
bracket_ :: MonadThrow m => m a -> m b -> m c -> m c
finally :: MonadThrow m => m a -> m b -> m a

-- | The <tt>SomeException</tt> type is the root of the exception type
--   hierarchy. When an exception of type <tt>e</tt> is thrown, behind the
--   scenes it is encapsulated in a <tt>SomeException</tt>.
data () => SomeException
class MonadThread m => MonadFork (m :: Type -> Type)
forkIO :: MonadFork m => m () -> m (ThreadId m)
forkOn :: MonadFork m => Int -> m () -> m (ThreadId m)
forkIOWithUnmask :: MonadFork m => ((forall a. () => m a -> m a) -> m ()) -> m (ThreadId m)
forkFinally :: MonadFork m => m a -> (Either SomeException a -> m ()) -> m (ThreadId m)
throwTo :: (MonadFork m, Exception e) => ThreadId m -> e -> m ()
killThread :: MonadFork m => ThreadId m -> m ()
yield :: MonadFork m => m ()
class (Monad m, Eq ThreadId m, Ord ThreadId m, Show ThreadId m) => MonadThread (m :: Type -> Type) where {
    type family ThreadId (m :: Type -> Type);
}
myThreadId :: MonadThread m => m (ThreadId m)
labelThread :: MonadThread m => ThreadId m -> String -> m ()
labelThisThread :: MonadThread m => String -> m ()
data () => ExceptionInLinkedThread
ExceptionInLinkedThread :: String -> SomeException -> ExceptionInLinkedThread
class (MonadSTM m, MonadThread m) => MonadAsync (m :: Type -> Type) where {
    type family Async (m :: Type -> Type) = (async :: Type -> Type) | async -> m;
}
async :: MonadAsync m => m a -> m (Async m a)
asyncBound :: MonadAsync m => m a -> m (Async m a)
asyncOn :: MonadAsync m => Int -> m a -> m (Async m a)
asyncThreadId :: MonadAsync m => Async m a -> ThreadId m
withAsync :: MonadAsync m => m a -> (Async m a -> m b) -> m b
withAsyncBound :: MonadAsync m => m a -> (Async m a -> m b) -> m b
withAsyncOn :: MonadAsync m => Int -> m a -> (Async m a -> m b) -> m b
waitSTM :: MonadAsync m => Async m a -> STM m a
pollSTM :: MonadAsync m => Async m a -> STM m (Maybe (Either SomeException a))
waitCatchSTM :: MonadAsync m => Async m a -> STM m (Either SomeException a)
waitAnySTM :: MonadAsync m => [Async m a] -> STM m (Async m a, a)
waitAnyCatchSTM :: MonadAsync m => [Async m a] -> STM m (Async m a, Either SomeException a)
waitEitherSTM :: MonadAsync m => Async m a -> Async m b -> STM m (Either a b)
waitEitherSTM_ :: MonadAsync m => Async m a -> Async m b -> STM m ()
waitEitherCatchSTM :: MonadAsync m => Async m a -> Async m b -> STM m (Either (Either SomeException a) (Either SomeException b))
waitBothSTM :: MonadAsync m => Async m a -> Async m b -> STM m (a, b)
wait :: MonadAsync m => Async m a -> m a
poll :: MonadAsync m => Async m a -> m (Maybe (Either SomeException a))
waitCatch :: MonadAsync m => Async m a -> m (Either SomeException a)
cancel :: MonadAsync m => Async m a -> m ()
cancelWith :: (MonadAsync m, Exception e) => Async m a -> e -> m ()
uninterruptibleCancel :: MonadAsync m => Async m a -> m ()
waitAny :: MonadAsync m => [Async m a] -> m (Async m a, a)
waitAnyCatch :: MonadAsync m => [Async m a] -> m (Async m a, Either SomeException a)
waitAnyCancel :: MonadAsync m => [Async m a] -> m (Async m a, a)
waitAnyCatchCancel :: MonadAsync m => [Async m a] -> m (Async m a, Either SomeException a)
waitEither :: MonadAsync m => Async m a -> Async m b -> m (Either a b)
waitEitherCatch :: MonadAsync m => Async m a -> Async m b -> m (Either (Either SomeException a) (Either SomeException b))
waitEitherCancel :: MonadAsync m => Async m a -> Async m b -> m (Either a b)
waitEitherCatchCancel :: MonadAsync m => Async m a -> Async m b -> m (Either (Either SomeException a) (Either SomeException b))
waitEither_ :: MonadAsync m => Async m a -> Async m b -> m ()
waitBoth :: MonadAsync m => Async m a -> Async m b -> m (a, b)
race :: MonadAsync m => m a -> m b -> m (Either a b)
race_ :: MonadAsync m => m a -> m b -> m ()
concurrently :: MonadAsync m => m a -> m b -> m (a, b)
concurrently_ :: MonadAsync m => m a -> m b -> m ()
asyncWithUnmask :: MonadAsync m => ((forall b. () => m b -> m b) -> m a) -> m (Async m a)
asyncOnWithUnmask :: MonadAsync m => Int -> ((forall b. () => m b -> m b) -> m a) -> m (Async m a)
withAsyncWithUnmask :: MonadAsync m => ((forall c. () => m c -> m c) -> m a) -> (Async m a -> m b) -> m b
withAsyncOnWithUnmask :: MonadAsync m => Int -> ((forall c. () => m c -> m c) -> m a) -> (Async m a -> m b) -> m b
compareAsyncs :: MonadAsync m => Async m a -> Async m b -> Ordering
link :: (MonadAsync m, MonadFork m, MonadMask m) => Async m a -> m ()

-- | Generalization of <a>link</a> that links an async to an arbitrary
--   thread.
--   
--   Non standard (not in <a>async</a> library)
linkTo :: (MonadAsync m, MonadFork m, MonadMask m) => ThreadId m -> Async m a -> m ()
class Monad m => MonadST (m :: Type -> Type)
withLiftST :: MonadST m => (forall s. () => (forall a. () => ST s a -> m a) -> b) -> b

-- | This is a length of time, as measured by a clock. Conversion functions
--   such as <a>fromInteger</a> and <a>realToFrac</a> will treat it as
--   seconds. For example, <tt>(0.010 :: DiffTime)</tt> corresponds to 10
--   milliseconds.
--   
--   It has a precision of one picosecond (= 10^-12 s). Enumeration
--   functions will treat it as picoseconds.
data () => DiffTime
class MonadMonotonicTimeNSec m => MonadMonotonicTime (m :: Type -> Type)
getMonotonicTime :: MonadMonotonicTime m => m Time
newtype () => Time
Time :: DiffTime -> Time
addTime :: DiffTime -> Time -> Time
diffTime :: Time -> Time -> DiffTime
class (MonadDelay m, MonadMonotonicTime m) => MonadDelay (m :: Type -> Type)
threadDelay :: MonadDelay m => DiffTime -> m ()
class Monad m => MonadEventlog (m :: Type -> Type)
traceEventIO :: MonadEventlog m => String -> m ()
traceMarkerIO :: MonadEventlog m => String -> m ()
class MonadThrow m => MonadEvaluate (m :: Type -> Type)
evaluate :: MonadEvaluate m => a -> m a
class () => NoThunks a
noThunks :: NoThunks a => Context -> a -> IO (Maybe ThunkInfo)
wNoThunks :: NoThunks a => Context -> a -> IO (Maybe ThunkInfo)
showTypeOf :: NoThunks a => Proxy a -> String
instance Ouroboros.Consensus.Util.IOLike.IOLike GHC.Types.IO


-- | A Read-Append-Write (RAW) lock
--   
--   Intended for qualified import
module Ouroboros.Consensus.Util.MonadSTM.RAWLock

-- | A Read-Append-Write (RAW) lock
--   
--   A RAW lock allows multiple concurrent readers, at most one appender,
--   which is allowed to run concurrently with the readers, and at most one
--   writer, which has exclusive access to the lock.
--   
--   The following table summarises which roles are allowed to concurrently
--   access the RAW lock:
--   
--   <pre>
--            │ Reader │ Appender │ Writer │
--   ─────────┼────────┼──────────┼────────┤
--   Reader   │   V    │     V    │    X   │
--   Appender │░░░░░░░░│     X    │    X   │
--   Writer   │░░░░░░░░│░░░░░░░░░░│    X   │
--   </pre>
--   
--   It is important to realise that a RAW lock is intended to control
--   access to a piece of in-memory state that should remain in sync with
--   some other state that can only be modified using side-effects, e.g.,
--   the file system. If, for example, you're only maintaining a counter
--   shared by threads, then simply use a <tt>TVar</tt> or an
--   <tt>MVar</tt>.
--   
--   <h1>Example use case: log files</h1>
--   
--   A RAW lock is useful, for example, to maintain an in-memory index of
--   log files stored on disk.
--   
--   <ul>
--   <li>To read data from a log file, you need "read" access to the index
--   to find out the file and offset where the requested piece of data is
--   stored. While holding the RAW lock as a reader, you can perform the IO
--   operation to read the data from the right log file. This can safely
--   happen concurrently with other read operations.</li>
--   <li>To append data to the current log file, you need "append" access
--   to the index so you can append an entry to the index and even to add a
--   new log file to the index when necessary. While holding the RAW lock
--   as an appender, you can perform the IO operation to append the piece
--   of data to the current log file and, if necessary start a new log
--   file. Only one append can happen concurrently. However, reads can
--   safely happen concurrently with appends. Note that the in-memory index
--   is only updated <i>after</i> writing to disk.</li>
--   <li>To remove the oldest log files, you need "write" access to the
--   index, so you can remove files from the index. While holding the RAW
--   lock as a writer, you can perform the IO operations to delete the
--   oldest log files. No other operations can run concurrently with this
--   operation: concurrent reads might try to read from deleted files and a
--   concurrent append could try to append to a deleted file.</li>
--   </ul>
--   
--   <h1>Analogy: Chicken coop</h1>
--   
--   Think of readers as chickens, the appender as the rooster, and the
--   writer as the fox. All of them want access to the chicken coop, i.e.,
--   the state protected by the RAW lock.
--   
--   We can allow multiple chickens (readers) together in the chicken coop,
--   they get along (reasonably) fine. We can also let one rooster
--   (appender) in, but not more than one, otherwise he would start
--   fighting with the other rooster (conflict with the other appender). We
--   can only let the fox in when all chickens and the rooster (if present)
--   have left the chicken coop, otherwise the fox would eat them (conflict
--   with the appender and invalidate the results of readers, e.g, closing
--   resources readers try to access).
--   
--   <h1>Usage</h1>
--   
--   To use the lock, use any of the three following operations:
--   
--   <ul>
--   <li><a>withReadAccess</a></li>
--   <li><a>withAppendAccess</a></li>
--   <li><a>withWriteAccess</a></li>
--   </ul>
--   
--   If the standard bracketing the above three operations use doesn't
--   suffice, use the following three acquire-release pairs:
--   
--   <ul>
--   <li><a>unsafeAcquireReadAccess</a> &amp;
--   <a>unsafeReleaseReadAccess</a></li>
--   <li><a>unsafeAcquireAppendAccess</a> &amp;
--   <a>unsafeReleaseAppendAccess</a></li>
--   <li><a>unsafeAcquireWriteAccess</a> &amp;
--   <a>unsafeReleaseWriteAccess</a></li>
--   </ul>
--   
--   NOTE: an acquire <b>must</b> be followed by the corresponding release,
--   otherwise the correctness of the lock is not guaranteed and a
--   dead-lock can happen.
--   
--   NOTE: nested locking of the same lock is not allowed, as you might be
--   blocked on yourself.
--   
--   <h1>Notes</h1>
--   
--   <ul>
--   <li>Only use a RAW lock when it is safe to concurrently read and
--   append.</li>
--   </ul>
--   
--   <ul>
--   <li>We do not guarantee fairness for appenders and writers. They will
--   race for access each time the RAW lock changes.</li>
--   <li>When you have many writers and/or very frequent writes, readers
--   and appenders will starve. You could say we have "unfairness", as
--   writers win over readers and appenders. A RAW lock will not be the
--   best fit in such a scenario.</li>
--   <li>When you have no writers and you only need a read-append lock,
--   consider using a <tt>StrictSVar</tt> instead. The "stale" state can be
--   used by the readers.</li>
--   <li>The state <tt>st</tt> is always evaluated to WHNF and is subject
--   to the <a>NoThunks</a> check when enabled.</li>
--   <li>All public functions are exception-safe.</li>
--   </ul>
data RAWLock m st

-- | Create a new <a>RAWLock</a>
new :: (IOLike m, NoThunks st) => st -> m (RAWLock m st)

-- | Poison the lock with the given exception. All subsequent access to the
--   lock will result in the given exception being thrown.
--   
--   Unless the lock has already been poisoned, in which case the original
--   exception with which the lock was poisoned will be thrown.
poison :: (IOLike m, Exception e, HasCallStack) => RAWLock m st -> (CallStack -> e) -> m (Maybe st)

-- | Read the contents of the <a>RAWLock</a> in an STM transaction.
--   
--   Will retry when there is a writer.
--   
--   In contrast to <a>withReadAccess</a>, this transaction will succeed
--   when there is a writer waiting to write, as there is no IO-operation
--   during which the lock must be held.
read :: IOLike m => RAWLock m st -> STM m st

-- | Access the state stored in the <a>RAWLock</a> as an appender.
--   
--   NOTE: it must be safe to run the given append action concurrently with
--   readers.
--   
--   Will block when there is another appender, a writer, or when a writer
--   is waiting to take the lock.
withAppendAccess :: forall m st a. IOLike m => RAWLock m st -> (st -> m (st, a)) -> m a

-- | Access the state stored in the <a>RAWLock</a> as a reader.
--   
--   Will block when there is a writer or when a writer is waiting to take
--   the lock.
withReadAccess :: forall m st a. IOLike m => RAWLock m st -> (st -> m a) -> m a

-- | Access the state stored in the <a>RAWLock</a> as a writer.
--   
--   Will block when there is another writer or while there are readers
--   and/or an appender.
withWriteAccess :: forall m st a. IOLike m => RAWLock m st -> (st -> m (st, a)) -> m a

-- | Access the state stored in the <a>RAWLock</a> as an appender.
--   
--   Will block when there is another appender, a writer, or when a writer
--   is waiting to take the lock.
--   
--   Composable with other <a>STM</a> transactions.
--   
--   NOTE: <b>must</b> be followed by a call to
--   <a>unsafeReleaseAppendAccess</a>.
unsafeAcquireAppendAccess :: IOLike m => RAWLock m st -> STM m st

-- | Acquire the <a>RAWLock</a> as a reader.
--   
--   Will block when there is a writer or when a writer is waiting to take
--   the lock.
--   
--   Composable with other <a>STM</a> transactions.
--   
--   NOTE: <b>must</b> be followed by a call to
--   <a>unsafeReleaseReadAccess</a>.
unsafeAcquireReadAccess :: IOLike m => RAWLock m st -> STM m st

-- | Access the state stored in the <a>RAWLock</a> as a writer.
--   
--   Will block when there is another writer or while there are readers
--   and/or an appender.
--   
--   Does <i>not</i> compose with other <a>STM</a> transactions.
--   
--   NOTE: <b>must</b> be followed by a call to
--   <a>unsafeReleaseWriteAccess</a>.
unsafeAcquireWriteAccess :: IOLike m => RAWLock m st -> m st

-- | Release the <a>RAWLock</a> as an appender.
--   
--   Doesn't block.
--   
--   Composable with other <a>STM</a> transactions.
--   
--   NOTE: <b>must</b> be preceded by a call to
--   <a>unsafeAcquireAppendAccess</a>.
unsafeReleaseAppendAccess :: IOLike m => RAWLock m st -> st -> STM m ()

-- | Release the <a>RAWLock</a> as a reader.
--   
--   Doesn't block.
--   
--   Composable with other <a>STM</a> transactions.
--   
--   NOTE: <b>must</b> be preceded by a call to
--   <a>unsafeAcquireReadAccess</a>.
unsafeReleaseReadAccess :: IOLike m => RAWLock m st -> STM m ()

-- | Release the <a>RAWLock</a> as a writer.
--   
--   Doesn't block.
--   
--   Does <i>not</i> compose with other <a>STM</a> transactions.
--   
--   NOTE: <b>must</b> be preceded by a call to
--   <a>unsafeAcquireWriteAccess</a>.
unsafeReleaseWriteAccess :: IOLike m => RAWLock m st -> st -> m ()
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Util.MonadSTM.RAWLock.Readers
instance GHC.Num.Num Ouroboros.Consensus.Util.MonadSTM.RAWLock.Readers
instance GHC.Enum.Enum Ouroboros.Consensus.Util.MonadSTM.RAWLock.Readers
instance GHC.Classes.Ord Ouroboros.Consensus.Util.MonadSTM.RAWLock.Readers
instance GHC.Classes.Eq Ouroboros.Consensus.Util.MonadSTM.RAWLock.Readers
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Util.MonadSTM.RAWLock.Appender
instance GHC.Generics.Generic Ouroboros.Consensus.Util.MonadSTM.RAWLock.Appender
instance NoThunks.Class.NoThunks st => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.MonadSTM.RAWLock.RAWState st)
instance GHC.Generics.Generic (Ouroboros.Consensus.Util.MonadSTM.RAWLock.RAWState st)

module Ouroboros.Consensus.Util.EarlyExit
exitEarly :: Applicative m => WithEarlyExit m a
withEarlyExit :: WithEarlyExit m a -> m (Maybe a)
withEarlyExit_ :: Functor m => WithEarlyExit m () -> m ()

-- | Lift a computation from the argument monad to the constructed monad.
lift :: (MonadTrans t, Monad m) => m a -> t m a
data WithEarlyExit m a
instance GHC.Base.Monad m => GHC.Base.MonadPlus (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Trans.Class.MonadTrans Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit
instance GHC.Base.Monad m => GHC.Base.Monad (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance GHC.Base.Monad m => GHC.Base.Alternative (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance GHC.Base.Monad m => GHC.Base.Applicative (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance GHC.Base.Functor m => GHC.Base.Functor (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance (forall a'. NoThunks.Class.NoThunks (m a')) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m a)
instance Control.Monad.Class.MonadSTM.Internal.MonadSTM m => Control.Monad.Class.MonadSTM.Internal.MonadSTM (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance (Control.Concurrent.Class.MonadMVar.MonadMVar m, Control.Monad.Class.MonadThrow.MonadMask m, Control.Monad.Class.MonadThrow.MonadEvaluate m) => Control.Concurrent.Class.MonadMVar.MonadMVar (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadThrow.MonadCatch m => Control.Monad.Class.MonadThrow.MonadThrow (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadThrow.MonadCatch m => Control.Monad.Class.MonadThrow.MonadCatch (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadThrow.MonadMask m => Control.Monad.Class.MonadThrow.MonadMask (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadFork.MonadThread m => Control.Monad.Class.MonadFork.MonadThread (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance (Control.Monad.Class.MonadThrow.MonadMask m, Control.Monad.Class.MonadAsync.MonadAsync m, Control.Monad.Class.MonadThrow.MonadCatch (Control.Monad.Class.MonadSTM.Internal.STM m)) => Control.Monad.Class.MonadAsync.MonadAsync (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadFork.MonadFork m => Control.Monad.Class.MonadFork.MonadFork (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadST.MonadST m => Control.Monad.Class.MonadST.MonadST (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadTime.MonadMonotonicTimeNSec m => Control.Monad.Class.MonadTime.MonadMonotonicTimeNSec (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadTime.SI.MonadMonotonicTime m => Control.Monad.Class.MonadTime.SI.MonadMonotonicTime (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadTimer.MonadDelay m => Control.Monad.Class.MonadTimer.MonadDelay (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadTimer.SI.MonadDelay m => Control.Monad.Class.MonadTimer.SI.MonadDelay (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance (Control.Monad.Class.MonadThrow.MonadEvaluate m, Control.Monad.Class.MonadThrow.MonadCatch m) => Control.Monad.Class.MonadThrow.MonadEvaluate (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance Control.Monad.Class.MonadEventlog.MonadEventlog m => Control.Monad.Class.MonadEventlog.MonadEventlog (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)
instance (Ouroboros.Consensus.Util.IOLike.IOLike m, forall a. NoThunks.Class.NoThunks (Control.Concurrent.Class.MonadSTM.Strict.TVar.Checked.StrictTVar (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m) a), forall a. NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.MonadSTM.StrictSVar.StrictSVar (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m) a), forall a. NoThunks.Class.NoThunks (Control.Concurrent.Class.MonadMVar.Strict.Checked.StrictMVar (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m) a)) => Ouroboros.Consensus.Util.IOLike.IOLike (Ouroboros.Consensus.Util.EarlyExit.WithEarlyExit m)

module Ouroboros.Consensus.Util.CBOR
data IDecodeIO a
Partial :: (Maybe ByteString -> IO (IDecodeIO a)) -> IDecodeIO a
Done :: !ByteString -> !ByteOffset -> a -> IDecodeIO a
Fail :: !ByteString -> !ByteOffset -> DeserialiseFailure -> IDecodeIO a
deserialiseIncrementalIO :: (forall s. Decoder s a) -> IO (IDecodeIO a)
fromIDecode :: IDecode RealWorld a -> IDecodeIO a
data Decoder m
Decoder :: (forall a. (forall s. Decoder s a) -> m a) -> Decoder m

-- | Decode next failure
--   
--   May throw <a>DeserialiseFailure</a>
[decodeNext] :: Decoder m -> forall a. (forall s. Decoder s a) -> m a

-- | Construct incremental decoder given a way to get chunks
--   
--   Resulting decoder is not thread safe.
initDecoderIO :: IO ByteString -> IO (Decoder IO)
decodeAsFlatTerm :: ByteString -> Either DeserialiseFailure FlatTerm
data ReadIncrementalErr

-- | Could not deserialise the data
ReadFailed :: DeserialiseFailure -> ReadIncrementalErr

-- | Deserialisation was successful, but there was additional data
TrailingBytes :: ByteString -> ReadIncrementalErr

-- | Read a file incrementally
--   
--   NOTE: The <a>MonadThrow</a> constraint is only needed for
--   <a>bracket</a>. This function does not actually throw anything.
--   
--   NOTE: This uses a chunk size of roughly 32k. If we use this function
--   to read small things this might not be ideal.
--   
--   NOTE: This currently expects the file to contain precisely one value;
--   see also <a>withStreamIncrementalOffsets</a>.
readIncremental :: forall m a. IOLike m => SomeHasFS m -> (forall s. Decoder s a) -> FsPath -> m (Either ReadIncrementalErr a)

-- | Read multiple <tt>a</tt>s incrementally from a file in a streaming
--   way.
--   
--   Continuation-passing style to ensure proper closure of the file.
--   
--   Reads the offset (<a>Word64</a>) of the start of each <tt>a</tt>, the
--   size (<a>Word64</a>) of each <tt>a</tt>, and each <tt>a</tt> itself.
--   When deserialising fails, it passes all already deserialised
--   <tt>a</tt>s, the error, and the offset after which the failure
--   occurred.
--   
--   NOTE: f we introduce user-facing streaming API also, the fact that we
--   are using <tt>streaming</tt> here should not dictate that we should
--   stick with it later; rather, we should revisit this code at that
--   point.
withStreamIncrementalOffsets :: forall m h a r. (IOLike m, HasCallStack) => HasFS m h -> (forall s. Decoder s (ByteString -> a)) -> FsPath -> (Stream (Of (Word64, (Word64, a))) m (Maybe (ReadIncrementalErr, Word64)) -> m r) -> m r
decodeList :: Decoder s a -> Decoder s [a]
decodeMaybe :: Decoder s a -> Decoder s (Maybe a)
decodeSeq :: Decoder s a -> Decoder s (StrictSeq a)
decodeWithOrigin :: Decoder s a -> Decoder s (WithOrigin a)
encodeList :: (a -> Encoding) -> [a] -> Encoding
encodeMaybe :: (a -> Encoding) -> Maybe a -> Encoding
encodeSeq :: (a -> Encoding) -> StrictSeq a -> Encoding
encodeWithOrigin :: (a -> Encoding) -> WithOrigin a -> Encoding
instance GHC.Show.Show Ouroboros.Consensus.Util.CBOR.ReadIncrementalErr
instance GHC.Classes.Eq Ouroboros.Consensus.Util.CBOR.ReadIncrementalErr

module Ouroboros.Consensus.Ledger.SupportsMempool

-- | Updating the ledger with a single transaction may result in a
--   different error type as when updating it with a block
type family ApplyTxErr blk :: Type

-- | Generalized transaction
--   
--   The mempool (and, accordingly, blocks) consist of "generalized
--   transactions"; this could be "proper" transactions (transferring
--   funds) but also other kinds of things such as update proposals,
--   delegations, etc.
data family GenTx blk :: Type

-- | Shorthand: ID of a generalized transaction
type GenTxId blk = TxId (GenTx blk)

-- | Transactions with an identifier
--   
--   The mempool will use these to locate transactions, so two different
--   transactions should have different identifiers.
class (Show (TxId tx), Ord (TxId tx), NoThunks (TxId tx)) => HasTxId tx

-- | Return the <a>TxId</a> of a <a>GenTx</a>.
--   
--   NOTE: a <a>TxId</a> must be unique up to ledger rules, i.e., two
--   <a>GenTx</a>s with the same <a>TxId</a> must be the same transaction
--   <i>according to the ledger</i>. However, we do not assume that a
--   <a>TxId</a> uniquely determines a <a>GenTx</a>: two <a>GenTx</a>s with
--   the same <a>TxId</a> can differ in, e.g., witnesses.
--   
--   Should be cheap as this will be called often.
txId :: HasTxId tx => tx -> TxId tx

-- | Collect all transactions from a block
--   
--   This is used for tooling only. We don't require it as part of RunNode
--   (and cannot, because we cannot give an instance for the dual ledger).
class HasTxs blk

-- | Return the transactions part of the given block in no particular
--   order.
extractTxs :: HasTxs blk => blk -> [GenTx blk]
class (UpdateLedger blk, NoThunks (GenTx blk), NoThunks (Validated (GenTx blk)), NoThunks (Ticked (LedgerState blk)), Show (GenTx blk), Show (Validated (GenTx blk)), Show (ApplyTxErr blk)) => LedgerSupportsMempool blk

-- | Check whether the internal invariants of the transaction hold.
txInvariant :: LedgerSupportsMempool blk => GenTx blk -> Bool

-- | Apply an unvalidated transaction
--   
--   The mempool expects that the ledger checks the sanity of the
--   transaction' size. The mempool implementation will add any valid
--   transaction as long as there is at least one byte free in the mempool.
applyTx :: LedgerSupportsMempool blk => LedgerConfig blk -> WhetherToIntervene -> SlotNo -> GenTx blk -> TickedLedgerState blk -> Except (ApplyTxErr blk) (TickedLedgerState blk, Validated (GenTx blk))

-- | Apply a previously validated transaction to a potentially different
--   ledger state
--   
--   When we re-apply a transaction to a potentially different ledger state
--   expensive checks such as cryptographic hashes can be skipped, but
--   other checks (such as checking for double spending) must still be
--   done.
reapplyTx :: (LedgerSupportsMempool blk, HasCallStack) => LedgerConfig blk -> SlotNo -> Validated (GenTx blk) -> TickedLedgerState blk -> Except (ApplyTxErr blk) (TickedLedgerState blk)

-- | The maximum number of bytes worth of transactions that can be put into
--   a block according to the currently adopted protocol parameters of the
--   ledger state.
--   
--   This is (conservatively) computed by subtracting the header size and
--   any other fixed overheads from the maximum block size.
txsMaxBytes :: LedgerSupportsMempool blk => TickedLedgerState blk -> Word32

-- | Return the post-serialisation size in bytes of a <a>GenTx</a> /when it
--   is embedded in a block/. This size might differ from the size of the
--   serialisation used to send and receive the transaction across the
--   network.
--   
--   This size is used to compute how many transaction we can put in a
--   block when forging one.
--   
--   For example, CBOR-in-CBOR could be used when sending the transaction
--   across the network, requiring a few extra bytes compared to the actual
--   in-block serialisation. Another example is the transaction of the
--   hard-fork combinator which will include an envelope indicating its era
--   when sent across the network. However, when embedded in the respective
--   era's block, there is no need for such envelope.
--   
--   Can be implemented by serialising the <a>GenTx</a>, but, ideally, this
--   is implement more efficiently. E.g., by returning the length of the
--   annotation.
txInBlockSize :: LedgerSupportsMempool blk => GenTx blk -> Word32

-- | Discard the evidence that transaction has been previously validated
txForgetValidated :: LedgerSupportsMempool blk => Validated (GenTx blk) -> GenTx blk

-- | A generalized transaction, <a>GenTx</a>, identifier.
data family TxId tx :: Type

-- | " Validated " transaction or block
--   
--   The ledger defines how to validate transactions and blocks. It's
--   possible the type before and after validation may be distinct (eg
--   Alonzo transactions), which originally motivated this family.
--   
--   We also gain the related benefit that certain interface functions,
--   such as those that <i>reapply</i> blocks, can have a more precise type
--   now. TODO
--   
--   Similarly, the Node-to-Client mini protocols can explicitly indicate
--   that the client trusts the blocks from the local server, by having the
--   server send <a>Validated</a> blocks to the client. TODO
--   
--   Note that validation has different implications for a transaction than
--   for a block. In particular, a validated transaction can be " reapplied
--   " to different ledger states, whereas a validated block must only be "
--   reapplied " to the exact same ledger state (eg as part of rebuilding
--   from an on-disk ledger snapshot).
--   
--   Since the ledger defines validation, see the ledger details for
--   concrete examples of what determines the validity (wrt to a
--   <a>LedgerState</a>) of a transaction and/or block. Example properties
--   include: a transaction's claimed inputs exist and are still unspent, a
--   block carries a sufficient cryptographic signature, etc.
data family Validated x :: Type

-- | A flag indicating whether the mempool should reject a
--   valid-but-problematic transaction, in order to to protect its author
--   from penalties etc
--   
--   The primary example is that, as of the Alonzo ledger, a valid
--   transaction can carry an invalid script. If a remote peer sends us
--   such a transaction (over a Node-to-Node protocol), we include it in a
--   block so that the ledger will penalize them them for the invalid
--   script: they wasted our resources by forcing us to run the script to
--   determine it's invalid. But if our local wallet -- which we trust by
--   assumption -- sends us such a transaction (over a Node-to-Client
--   protocol), we would be a good neighbor by rejecting that transaction:
--   they must have made some sort of mistake, and we don't want the ledger
--   to penalize them.
data WhetherToIntervene

-- | We do not trust remote peers, so if a problematic-yet-valid
--   transaction arrives over NTN, we accept it; it will end up in a block
--   and the ledger will penalize them for it.
DoNotIntervene :: WhetherToIntervene

-- | We trust local clients, so if a problematic-yet-valid transaction
--   arrives over NTC, we reject it in order to avoid the ledger penalizing
--   them for it.
Intervene :: WhetherToIntervene


-- | Mempool capacity, size and transaction size datatypes.
--   
--   This module also defines how to manually override the mempool
--   capacity.
--   
--   <pre>
--   import           Ouroboros.Consensus.Mempool.Capacity (Capacity)
--   import qualified Ouroboros.Consensus.Mempool.Capacity as Capacity
--   </pre>
module Ouroboros.Consensus.Mempool.Capacity

-- | Represents the maximum number of bytes worth of transactions that a
--   <tt>Mempool</tt> can contain.
newtype MempoolCapacityBytes
MempoolCapacityBytes :: Word32 -> MempoolCapacityBytes
[getMempoolCapacityBytes] :: MempoolCapacityBytes -> Word32

-- | An override for the default <a>MempoolCapacityBytes</a> which is 2x
--   the maximum transaction capacity
data MempoolCapacityBytesOverride

-- | Use 2x the maximum transaction capacity of a block. This will change
--   dynamically with the protocol parameters adopted in the current
--   ledger.
NoMempoolCapacityBytesOverride :: MempoolCapacityBytesOverride

-- | Use the following <a>MempoolCapacityBytes</a>.
MempoolCapacityBytesOverride :: !MempoolCapacityBytes -> MempoolCapacityBytesOverride

-- | If no override is provided, calculate the default mempool capacity as
--   2x the current ledger's maximum transaction capacity of a block.
computeMempoolCapacity :: LedgerSupportsMempool blk => TickedLedgerState blk -> MempoolCapacityBytesOverride -> MempoolCapacityBytes

-- | Create an override for the mempool capacity using the provided number
--   of bytes.
mkCapacityBytesOverride :: Word32 -> MempoolCapacityBytesOverride

-- | The size of a mempool.
data MempoolSize
MempoolSize :: !Word32 -> !Word32 -> MempoolSize

-- | The number of transactions in the mempool.
[msNumTxs] :: MempoolSize -> !Word32

-- | The summed byte size of all the transactions in the mempool.
[msNumBytes] :: MempoolSize -> !Word32
newtype ByteSize
ByteSize :: Word32 -> ByteSize
[unByteSize] :: ByteSize -> Word32

-- | Each block has its limits of how many transactions it can hold. That
--   limit is compared against the sum of measurements taken of each of the
--   transactions in that block.
--   
--   How we measure the transaction depends of the era that this
--   transaction belongs to (more specifically it depends on the block type
--   to which this transaction will be added). For initial eras (like Byron
--   and initial generations of Shelley based eras) this measure was simply
--   a ByteSize (block could not be bigger then given size - in bytes -
--   specified by the ledger state). In future eras (starting with Alonzo)
--   this measure was a bit more complex as it had to take other factors
--   into account (like execution units). For details please see the
--   individual instances for the TxLimits.
class BoundedMeasure (TxMeasure blk) => TxLimits blk where {
    type TxMeasure blk;
}

-- | What is the measure an individual tx?
txMeasure :: TxLimits blk => Validated (GenTx blk) -> TxMeasure blk

-- | What is the allowed capacity for txs in an individual block?
txsBlockCapacity :: TxLimits blk => Ticked (LedgerState blk) -> TxMeasure blk

-- | Is every component of the first value less-than-or-equal-to the
--   corresponding component of the second value?
(<=) :: Measure a => a -> a -> Bool

-- | An override that lowers a capacity limit
--   
--   Specifically, we use this override to let the node operator limit the
--   total <a>TxMeasure</a> of transactions in blocks even more severely
--   than would the ledger state's <a>txsBlockCapacity</a>. The forge logic
--   will use the <a>min</a> (ie the lattice's <tt>meet</tt> operator) to
--   combine this override with the capacity given by the ledger state.
--   More concretely, that will typically be a componentwise minimum
--   operation, along each of the components/dimensions of
--   <tt><a>TxMeasure</a> blk</tt>.
--   
--   This newtype wrapper distinguishes the intention of this particular
--   <a>TxMeasure</a> as such an override. We use <a>TxMeasure</a> in
--   different ways in this code base. The newtype also allows us to
--   distinguish the one most appropriate monoid among many offered by the
--   <a>TxLimits</a> superclass constraints: it is the monoid induced by
--   the bounded meet-semilattice (see <a>BoundedMeasure</a>) that is
--   relevant to the notion of <i>overriding</i> the ledger's block
--   capacity.
data TxOverrides blk

-- | Apply the override
applyOverrides :: TxLimits blk => TxOverrides blk -> TxMeasure blk -> TxMeasure blk
getOverrides :: TxOverrides blk -> TxMeasure blk

-- | Smart constructor for <tt>Overrides</tt>.
mkOverrides :: TxMeasure blk -> TxOverrides blk

-- | <pre>
--   <a>applyOverrides</a> <tt>noOverrides</tt> m = m
--   </pre>
noOverridesMeasure :: BoundedMeasure a => a
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Mempool.Capacity.MempoolCapacityBytes
instance GHC.Show.Show Ouroboros.Consensus.Mempool.Capacity.MempoolCapacityBytes
instance GHC.Classes.Eq Ouroboros.Consensus.Mempool.Capacity.MempoolCapacityBytes
instance GHC.Show.Show Ouroboros.Consensus.Mempool.Capacity.MempoolCapacityBytesOverride
instance GHC.Classes.Eq Ouroboros.Consensus.Mempool.Capacity.MempoolCapacityBytesOverride
instance GHC.Show.Show Ouroboros.Consensus.Mempool.Capacity.MempoolSize
instance GHC.Classes.Eq Ouroboros.Consensus.Mempool.Capacity.MempoolSize
instance Data.Measure.Class.Measure Ouroboros.Consensus.Mempool.Capacity.ByteSize
instance Data.Measure.Class.BoundedMeasure Ouroboros.Consensus.Mempool.Capacity.ByteSize
instance GHC.Classes.Ord Ouroboros.Consensus.Mempool.Capacity.ByteSize
instance GHC.Classes.Eq Ouroboros.Consensus.Mempool.Capacity.ByteSize
instance GHC.Show.Show Ouroboros.Consensus.Mempool.Capacity.ByteSize
instance Ouroboros.Consensus.Mempool.Capacity.TxLimits blk => GHC.Base.Monoid (Ouroboros.Consensus.Mempool.Capacity.TxOverrides blk)
instance Ouroboros.Consensus.Mempool.Capacity.TxLimits blk => GHC.Base.Semigroup (Ouroboros.Consensus.Mempool.Capacity.TxOverrides blk)
instance GHC.Base.Semigroup Ouroboros.Consensus.Mempool.Capacity.MempoolSize
instance GHC.Base.Monoid Ouroboros.Consensus.Mempool.Capacity.MempoolSize


-- | Intended for qualified import.
--   
--   <pre>
--   import           Ouroboros.Consensus.Mempool.TxSeq (TxSeq (..))
--   import qualified Ouroboros.Consensus.Mempool.TxSeq as TxSeq
--   </pre>
module Ouroboros.Consensus.Mempool.TxSeq

-- | We allocate each transaction a (monotonically increasing) ticket
--   number as it enters the mempool.
newtype TicketNo
TicketNo :: Word64 -> TicketNo

-- | The mempool is a sequence of transactions with their ticket numbers
--   and size in bytes.
--   
--   Transactions are allocated monotonically increasing ticket numbers as
--   they are appended to the mempool sequence. Transactions can be removed
--   from any position, not just the front.
--   
--   The sequence is thus ordered by the ticket numbers. We can use the
--   ticket numbers as a compact representation for a "reader" location in
--   the sequence. If a reader knows it has seen all txs with a lower
--   ticket number then it is only interested in transactions with higher
--   ticket numbers.
--   
--   The mempool sequence is represented by a fingertree. We use a
--   fingertree measure to allow not just normal sequence operations but
--   also efficient splitting and indexing by the ticket number.
data TxSeq tx

-- | An empty mempool sequence.
pattern Empty :: TxSeq tx

-- | &lt;math&gt;. Access or add a tx at the back of the mempool sequence.
--   
--   New txs are always added at the back.
pattern (:>) :: TxSeq tx -> TxTicket tx -> TxSeq tx

-- | &lt;math&gt;. Access a tx at the front of the mempool sequence.
--   
--   Note that we never add txs at the front. We access txs from front to
--   back when forwarding txs to other peers, or when adding txs to blocks.
pattern (:<) :: TxTicket tx -> TxSeq tx -> TxSeq tx
infixl 5 :<
infixl 5 :>

-- | We associate transactions in the mempool with their ticket number and
--   size in bytes.
data TxTicket tx
TxTicket :: !tx -> !TicketNo -> !TxSizeInBytes -> TxTicket tx

-- | The transaction associated with this ticket.
[txTicketTx] :: TxTicket tx -> !tx

-- | The ticket number.
[txTicketNo] :: TxTicket tx -> !TicketNo

-- | The byte size of the transaction (<a>txTicketTx</a>) associated with
--   this ticket.
[txTicketTxSizeInBytes] :: TxTicket tx -> !TxSizeInBytes

-- | Given a list of <a>TxTicket</a>s, construct a <a>TxSeq</a>.
fromList :: [TxTicket tx] -> TxSeq tx

-- | &lt;math&gt;. Look up a transaction in the sequence by its
--   <a>TicketNo</a>.
lookupByTicketNo :: TxSeq tx -> TicketNo -> Maybe tx

-- | &lt;math&gt;. Split the sequence of transactions into two parts based
--   on the given <a>TicketNo</a>. The first part has transactions with
--   tickets less than or equal to the given ticket, and the second part
--   has transactions with tickets strictly greater than the given ticket.
splitAfterTicketNo :: TxSeq tx -> TicketNo -> (TxSeq tx, TxSeq tx)

-- | &lt;math&gt;. Split the sequence of transactions into two parts based
--   on the given <a>TxSizeInBytes</a>. The first part has transactions
--   whose summed <a>TxSizeInBytes</a> is less than or equal to the given
--   <a>TxSizeInBytes</a>, and the second part has the remaining
--   transactions in the sequence.
splitAfterTxSize :: TxSeq tx -> TxSizeInBytes -> (TxSeq tx, TxSeq tx)

-- | Convert a <a>TxSeq</a> to a list of <a>TxTicket</a>s.
toList :: TxSeq tx -> [TxTicket tx]

-- | &lt;math&gt;. Return the <a>MempoolSize</a> of the given <a>TxSeq</a>.
toMempoolSize :: TxSeq tx -> MempoolSize

-- | Convert a <a>TxSeq</a> to a list of pairs of transactions and their
--   associated <a>TicketNo</a>s.
toTuples :: TxSeq tx -> [(tx, TicketNo)]

-- | The transaction ticket number from which our counter starts.
zeroTicketNo :: TicketNo

-- | &lt;math&gt;. Specification of <a>splitAfterTxSize</a>.
--   
--   Use <a>splitAfterTxSize</a> as it should be faster.
--   
--   This function is used to verify whether <a>splitAfterTxSize</a>
--   behaves as expected.
splitAfterTxSizeSpec :: TxSeq tx -> TxSizeInBytes -> (TxSeq tx, TxSeq tx)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Mempool.TxSeq.TicketNo
instance GHC.Enum.Bounded Ouroboros.Consensus.Mempool.TxSeq.TicketNo
instance GHC.Enum.Enum Ouroboros.Consensus.Mempool.TxSeq.TicketNo
instance GHC.Show.Show Ouroboros.Consensus.Mempool.TxSeq.TicketNo
instance GHC.Classes.Ord Ouroboros.Consensus.Mempool.TxSeq.TicketNo
instance GHC.Classes.Eq Ouroboros.Consensus.Mempool.TxSeq.TicketNo
instance NoThunks.Class.NoThunks tx => NoThunks.Class.NoThunks (Ouroboros.Consensus.Mempool.TxSeq.TxTicket tx)
instance GHC.Generics.Generic (Ouroboros.Consensus.Mempool.TxSeq.TxTicket tx)
instance GHC.Show.Show tx => GHC.Show.Show (Ouroboros.Consensus.Mempool.TxSeq.TxTicket tx)
instance GHC.Classes.Eq tx => GHC.Classes.Eq (Ouroboros.Consensus.Mempool.TxSeq.TxTicket tx)
instance GHC.Show.Show Ouroboros.Consensus.Mempool.TxSeq.TxSeqMeasure
instance NoThunks.Class.NoThunks tx => NoThunks.Class.NoThunks (Ouroboros.Consensus.Mempool.TxSeq.TxSeq tx)
instance GHC.Show.Show tx => GHC.Show.Show (Ouroboros.Consensus.Mempool.TxSeq.TxSeq tx)
instance Data.Foldable.Foldable Ouroboros.Consensus.Mempool.TxSeq.TxSeq
instance Data.FingerTree.Measured Ouroboros.Consensus.Mempool.TxSeq.TxSeqMeasure (Ouroboros.Consensus.Mempool.TxSeq.TxTicket tx)
instance GHC.Base.Semigroup Ouroboros.Consensus.Mempool.TxSeq.TxSeqMeasure
instance GHC.Base.Monoid Ouroboros.Consensus.Mempool.TxSeq.TxSeqMeasure

module Ouroboros.Consensus.Block.Forging

-- | Stateful wrapper around block production
--   
--   NOTE: do not refer to the consensus or ledger config in the closure of
--   this record because they might contain an <tt>EpochInfo Identity</tt>,
--   which will be incorrect when used as part of the hard fork combinator.
data BlockForging m blk
BlockForging :: Text -> CanBeLeader (BlockProtocol blk) -> (TopLevelConfig blk -> SlotNo -> Ticked (ChainDepState (BlockProtocol blk)) -> m (ForgeStateUpdateInfo blk)) -> (TopLevelConfig blk -> SlotNo -> Ticked (ChainDepState (BlockProtocol blk)) -> IsLeader (BlockProtocol blk) -> ForgeStateInfo blk -> Either (CannotForge blk) ()) -> (TopLevelConfig blk -> BlockNo -> SlotNo -> TickedLedgerState blk -> [Validated (GenTx blk)] -> IsLeader (BlockProtocol blk) -> m blk) -> BlockForging m blk

-- | Identifier used in the trace messages produced for this
--   <a>BlockForging</a> record.
--   
--   Useful when the node is running with multiple sets of credentials.
[forgeLabel] :: BlockForging m blk -> Text

-- | Proof that the node can be a leader
--   
--   NOTE: the other fields of this record may refer to this value (or a
--   value derived from it) in their closure, which means one should not
--   override this field independently from the others.
[canBeLeader] :: BlockForging m blk -> CanBeLeader (BlockProtocol blk)

-- | Update the forge state.
--   
--   When the node can be a leader, this will be called at the start of
--   each slot, right before calling <a>checkCanForge</a>.
--   
--   When <a>Updated</a> is returned, we trace the <a>ForgeStateInfo</a>.
--   
--   When <a>UpdateFailed</a> is returned, we trace the
--   <a>ForgeStateUpdateError</a> and don't call <a>checkCanForge</a>.
[updateForgeState] :: BlockForging m blk -> TopLevelConfig blk -> SlotNo -> Ticked (ChainDepState (BlockProtocol blk)) -> m (ForgeStateUpdateInfo blk)

-- | After checking that the node indeed is a leader (<a>checkIsLeader</a>
--   returned <a>Just</a>) and successfully updating the forge state
--   (<a>updateForgeState</a> did not return <a>UpdateFailed</a>), do
--   another check to see whether we can actually forge a block.
--   
--   When <a>CannotForge</a> is returned, we don't call <a>forgeBlock</a>.
[checkCanForge] :: BlockForging m blk -> TopLevelConfig blk -> SlotNo -> Ticked (ChainDepState (BlockProtocol blk)) -> IsLeader (BlockProtocol blk) -> ForgeStateInfo blk -> Either (CannotForge blk) ()

-- | Forge a block
--   
--   The function is passed the contents of the mempool; this is a set of
--   transactions that is guaranteed to be consistent with the ledger state
--   (also provided as an argument) and with each other (when applied in
--   order). In principle <i>all</i> of them could be included in the block
--   (up to maximum block size).
--   
--   NOTE: do not refer to the consensus or ledger config in the closure,
--   because they might contain an <tt>EpochInfo Identity</tt>, which will
--   be incorrect when used as part of the hard fork combinator. Use the
--   given <a>TopLevelConfig</a> instead, as it is guaranteed to be correct
--   even when used as part of the hard fork combinator.
--   
--   PRECONDITION: <a>checkCanForge</a> returned <tt>Right ()</tt>.
[forgeBlock] :: BlockForging m blk -> TopLevelConfig blk -> BlockNo -> SlotNo -> TickedLedgerState blk -> [Validated (GenTx blk)] -> IsLeader (BlockProtocol blk) -> m blk

-- | Information about why we <i>cannot</i> forge a block, although we are
--   a leader
--   
--   This should happen only rarely. An example might be that our hot key
--   does not (yet/anymore) match the delegation state.
type family CannotForge blk :: Type

-- | Returned when a call to <a>updateForgeState</a> succeeded and caused
--   the forge state to change. This info is traced.
type family ForgeStateInfo blk :: Type

-- | Returned when a call <a>updateForgeState</a> failed, e.g., because the
--   KES key is no longer valid. This info is traced.
type family ForgeStateUpdateError blk :: Type

-- | The result of <a>updateForgeState</a>.
--   
--   Note: the forge state itself is implicit and not reflected in the
--   types.
data ForgeStateUpdateInfo blk

-- | NB The update might have not changed the forge state.
ForgeStateUpdated :: ForgeStateInfo blk -> ForgeStateUpdateInfo blk
ForgeStateUpdateFailed :: ForgeStateUpdateError blk -> ForgeStateUpdateInfo blk

-- | A node was prevented from forging for an artificial reason, such as
--   testing, benchmarking, etc. It's <i>artificial</i> in that this
--   constructor should never occur in a production deployment.
ForgeStateUpdateSuppressed :: ForgeStateUpdateInfo blk
data ShouldForge blk

-- | Before check whether we are a leader in this slot, we tried to update
--   our forge state (<a>updateForgeState</a>), but it failed. We will not
--   check whether we are leader and will thus not forge a block either.
--   
--   E.g., we could not evolve our KES key.
ForgeStateUpdateError :: ForgeStateUpdateError blk -> ShouldForge blk

-- | We are a leader in this slot, but we cannot forge for a certain
--   reason.
--   
--   E.g., our KES key is not yet valid in this slot or we are not the
--   current delegate of the genesis key we have a delegation certificate
--   from.
CannotForge :: CannotForge blk -> ShouldForge blk

-- | We are not a leader in this slot
NotLeader :: ShouldForge blk

-- | We are a leader in this slot and we should forge a block.
ShouldForge :: IsLeader (BlockProtocol blk) -> ShouldForge blk
castForgeStateUpdateInfo :: (ForgeStateInfo blk ~ ForgeStateInfo blk', ForgeStateUpdateError blk ~ ForgeStateUpdateError blk') => ForgeStateUpdateInfo blk -> ForgeStateUpdateInfo blk'
checkShouldForge :: forall m blk. (Monad m, ConsensusProtocol (BlockProtocol blk), HasCallStack) => BlockForging m blk -> Tracer m (ForgeStateInfo blk) -> TopLevelConfig blk -> SlotNo -> Ticked (ChainDepState (BlockProtocol blk)) -> m (ShouldForge blk)

-- | Embed <a>UpdateInfo</a> into <a>ForgeStateUpdateInfo</a>
forgeStateUpdateInfoFromUpdateInfo :: UpdateInfo (ForgeStateInfo blk) (ForgeStateUpdateError blk) -> ForgeStateUpdateInfo blk

-- | The result of updating something, e.g., the forge state.
data UpdateInfo updated failed

-- | NOTE: The update may have induced no change.
Updated :: updated -> UpdateInfo updated failed
UpdateFailed :: failed -> UpdateInfo updated failed

-- | The prefix of transactions to include in the block
--   
--   Filters out all transactions that do not fit the maximum size of total
--   transactions in a single block, which is determined by querying the
--   ledger state for the current limit and the given override. The result
--   is the pointwise minimum of the ledger-specific capacity and the
--   result of the override. In other words, the override can only reduce
--   (parts of) the <a>TxMeasure</a>.
takeLargestPrefixThatFits :: TxLimits blk => TxOverrides blk -> TickedLedgerState blk -> [Validated (GenTx blk)] -> [Validated (GenTx blk)]
instance (GHC.Show.Show updated, GHC.Show.Show failed) => GHC.Show.Show (Ouroboros.Consensus.Block.Forging.UpdateInfo updated failed)
instance (GHC.Show.Show (Ouroboros.Consensus.Block.Forging.ForgeStateInfo blk), GHC.Show.Show (Ouroboros.Consensus.Block.Forging.ForgeStateUpdateError blk)) => GHC.Show.Show (Ouroboros.Consensus.Block.Forging.ForgeStateUpdateInfo blk)


-- | The consensus layer's abstract view of blocks
module Ouroboros.Consensus.Block

module Ouroboros.Consensus.Storage.LedgerDB.Stream

-- | Next block returned during streaming
data NextBlock blk
NoMoreBlocks :: NextBlock blk
NextBlock :: blk -> NextBlock blk

-- | Stream blocks from the immutable DB
--   
--   When we initialize the ledger DB, we try to find a snapshot close to
--   the tip of the immutable DB, and then stream blocks from the immutable
--   DB to its tip to bring the ledger up to date with the tip of the
--   immutable DB.
--   
--   In CPS form to enable the use of <tt>withXYZ</tt> style iterator init
--   functions.
data StreamAPI m blk
StreamAPI :: (forall a. HasCallStack => Point blk -> (Either (RealPoint blk) (m (NextBlock blk)) -> m a) -> m a) -> StreamAPI m blk

-- | Start streaming after the specified block
[streamAfter] :: StreamAPI m blk -> forall a. HasCallStack => Point blk -> (Either (RealPoint blk) (m (NextBlock blk)) -> m a) -> m a

-- | Stream all blocks
streamAll :: forall m blk e a. (Monad m, HasCallStack) => StreamAPI m blk -> Point blk -> (RealPoint blk -> e) -> a -> (blk -> a -> m a) -> ExceptT e m a

module Ouroboros.Consensus.Storage.Common
tipIsGenesis :: WithOrigin r -> Bool

-- | Number of bytes from the start of a block needed to reconstruct the
--   nested context.
--   
--   See <tt>reconstructPrefixLen</tt>.
newtype PrefixLen
PrefixLen :: Word8 -> PrefixLen
[getPrefixLen] :: PrefixLen -> Word8
addPrefixLen :: Word8 -> PrefixLen -> PrefixLen
takePrefix :: PrefixLen -> ByteString -> ShortByteString

-- | Information about the serialised block.
data BinaryBlockInfo
BinaryBlockInfo :: !Word16 -> !Word16 -> BinaryBlockInfo

-- | The offset within the serialised block at which the header starts.
[headerOffset] :: BinaryBlockInfo -> !Word16

-- | How many bytes the header is long. Extracting the <a>headerSize</a>
--   bytes from serialised block starting from <a>headerOffset</a> should
--   yield the header. Before passing the extracted bytes to the decoder
--   for headers, an envelope can be around using
--   <tt>nodeAddHeaderEnvelope</tt>.
[headerSize] :: BinaryBlockInfo -> !Word16

-- | Extract the header from the given <a>ByteString</a> using the
--   <a>BinaryBlockInfo</a>.
extractHeader :: BinaryBlockInfo -> ByteString -> ByteString

-- | The lower bound for an iterator
--   
--   Hint: use <tt><a>StreamFromExclusive</a> <tt>genesisPoint</tt></tt> to
--   start streaming from Genesis.
data StreamFrom blk
StreamFromInclusive :: !RealPoint blk -> StreamFrom blk
StreamFromExclusive :: !Point blk -> StreamFrom blk
newtype StreamTo blk
StreamToInclusive :: RealPoint blk -> StreamTo blk

-- | Check whether the bounds make sense
--   
--   An example of bounds that don't make sense:
--   
--   <pre>
--   StreamFromExclusive (BlockPoint 3 ..)
--   StreamToInclusive   (RealPoint  3 ..)
--   </pre>
--   
--   This function does not check whether the bounds correspond to existing
--   blocks.
validBounds :: StandardHash blk => StreamFrom blk -> StreamTo blk -> Bool

-- | Which component of the block to read from a database: the whole block,
--   its header, its hash, the block size, ..., or combinations thereof.
--   
--   NOTE: when requesting multiple components, we will not optimise/cache
--   them.
data BlockComponent blk a

-- | Verify the integrity of the block by checking its signature and/or
--   hashes. The interpreter should throw an exception when the block does
--   not pass the check.
[GetVerifiedBlock] :: BlockComponent blk blk
[GetBlock] :: BlockComponent blk blk
[GetRawBlock] :: BlockComponent blk ByteString
[GetHeader] :: BlockComponent blk (Header blk)
[GetRawHeader] :: BlockComponent blk ByteString
[GetHash] :: BlockComponent blk (HeaderHash blk)
[GetSlot] :: BlockComponent blk SlotNo
[GetIsEBB] :: BlockComponent blk IsEBB
[GetBlockSize] :: BlockComponent blk SizeInBytes
[GetHeaderSize] :: BlockComponent blk Word16
[GetNestedCtxt] :: BlockComponent blk (SomeSecond (NestedCtxt Header) blk)
[GetPure] :: a -> BlockComponent blk a
[GetApply] :: BlockComponent blk (a -> b) -> BlockComponent blk a -> BlockComponent blk b
data () => SizeInBytes
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.Common.PrefixLen
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.Common.PrefixLen
instance GHC.Show.Show Ouroboros.Consensus.Storage.Common.PrefixLen
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.Common.PrefixLen
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.Common.PrefixLen
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.Common.BinaryBlockInfo
instance GHC.Show.Show Ouroboros.Consensus.Storage.Common.BinaryBlockInfo
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.Common.BinaryBlockInfo
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.Common.StreamFrom blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.Common.StreamFrom blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.Common.StreamFrom blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.Common.StreamFrom blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.Common.StreamTo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.Common.StreamTo blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.Common.StreamTo blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.Common.StreamTo blk)
instance GHC.Base.Functor (Ouroboros.Consensus.Storage.Common.BlockComponent blk)
instance GHC.Base.Applicative (Ouroboros.Consensus.Storage.Common.BlockComponent blk)

module Ouroboros.Consensus.Storage.VolatileDB.API
data VolatileDB m blk
VolatileDB :: (HasCallStack => m ()) -> (forall b. HasCallStack => BlockComponent blk b -> HeaderHash blk -> m (Maybe b)) -> (HasCallStack => blk -> m ()) -> (HasCallStack => STM m (ChainHash blk -> Set (HeaderHash blk))) -> (HasCallStack => STM m (HeaderHash blk -> Maybe (BlockInfo blk))) -> (HasCallStack => SlotNo -> m ()) -> (HasCallStack => STM m MaxSlotNo) -> VolatileDB m blk

-- | Close the VolatileDB.
--   
--   NOTE: idempotent after a manual closure, but not after an automatic
--   closure in case of an <a>VolatileDBError</a>. In that case, closing it
--   again will cause a <a>ClosedDBError</a> wrapping the original
--   <a>VolatileDBError</a> to be thrown.
[closeDB] :: VolatileDB m blk -> HasCallStack => m ()

-- | Return the request block component for the block with the given hash.
--   When not in the VolatileDB, <a>Nothing</a> is returned.
[getBlockComponent] :: VolatileDB m blk -> forall b. HasCallStack => BlockComponent blk b -> HeaderHash blk -> m (Maybe b)

-- | Store the given block in the VolatileDB.
--   
--   Returns after the block has been written to disk.
[putBlock] :: VolatileDB m blk -> HasCallStack => blk -> m ()

-- | Return a function that returns the successors of the block with the
--   given hash.
--   
--   This function will return a non-empty set for any block of which a
--   successor has been added to the VolatileDB and will return an empty
--   set if no successors for the given block have been added to the
--   VolatileDB (yet).
--   
--   Note that it is not required that the given block has been added to
--   the VolatileDB.
[filterByPredecessor] :: VolatileDB m blk -> HasCallStack => STM m (ChainHash blk -> Set (HeaderHash blk))

-- | Return a function that returns the <a>BlockInfo</a> of the block with
--   the given hash or <a>Nothing</a> if the block is not found in the
--   VolatileDB.
[getBlockInfo] :: VolatileDB m blk -> HasCallStack => STM m (HeaderHash blk -> Maybe (BlockInfo blk))

-- | Try to remove all blocks with a slot number less than the given one.
--   
--   <h1>Context</h1>
--   
--   When the current chain changes, blocks older than <tt>k</tt>, i.e.,
--   blocks that are followed by <tt>k</tt> blocks or more, become
--   <i>immutable</i>. Whenever this happens, we schedule a garbage
--   collection on the VolatileDB that will try to remove blocks older than
--   the most recent immutable block, as such blocks will never be adopted.
--   There's no point in storing them anymore.
--   
--   <h1>Block number vs slot number</h1>
--   
--   While we typically talk in terms of <i>block numbers</i> when
--   discussing immutability, i.e., <i><tt>k</tt> blocks</i>, we use
--   <i>slot number</i> for garbage collection. We schedule a garbage
--   collection for blocks with a /slot number/ less than the slot number
--   of the immutable block, as opposed to the block number. The reason for
--   this is that the VolatileDB is not aware of block numbers, only of
--   slot numbers.
--   
--   By using slot numbers for garbage collection, we might not <i>yet</i>
--   have garbage collected some blocks that could never be adopted again
--   and that we would have garbage collected when using block numbers.
--   This is harmless. The opposite direction is more important and
--   problematic: garbage collecting a block that we might want to adopt
--   after all. Say we have mistakenly garbage collected such a block, in
--   that case the following would be true:
--   
--   <ol>
--   <li>The block has a slot number older than the immutable block's slot
--   number: otherwise we wouldn't have mistakenly garbage collected
--   it.</li>
--   <li>The block has a block number greater than the immutable block's
--   block number: otherwise we wouldn't want to adopt it, as it would have
--   been older than <tt>k</tt>.</li>
--   <li>The block is a part of a fork fitting on the immutable block. As
--   we cannot roll back this block, all forks we could ever adopt would
--   have to go through this block.</li>
--   </ol>
--   
--   As slot numbers grow monotonically within a chain, all forks starting
--   after the immutable block will only contain blocks with slot numbers
--   greater (or equal to in case of EBBs) than the immutable block's slot
--   number. This directly contradicts (1), so we will <i>never</i> garbage
--   collect a block that we might still want to adopt.
--   
--   <h1>Less than vs. less than or equal to</h1>
--   
--   Note that we remove blocks with a slot number <i>less than</i> the
--   given slot number, but not <i>equal to</i> it. In practice, this
--   off-by-one difference will not matter in terms of disk space usage,
--   because as soon as the chain grows again by at least one block, those
--   blocks will be removed anyway. The reason for <tt>&lt;</tt> opposed to
--   <tt>&lt;=</tt> is to avoid issues with <i>EBBs</i>, which have the
--   same slot number as the block after it.
[garbageCollect] :: VolatileDB m blk -> HasCallStack => SlotNo -> m ()

-- | Return the highest slot number ever stored by the VolatileDB.
[getMaxSlotNo] :: VolatileDB m blk -> HasCallStack => STM m MaxSlotNo

-- | The information that the user has to provide for each new block.
data BlockInfo blk
BlockInfo :: !HeaderHash blk -> !SlotNo -> !BlockNo -> !ChainHash blk -> !IsEBB -> !Word16 -> !Word16 -> BlockInfo blk
[biHash] :: BlockInfo blk -> !HeaderHash blk
[biSlotNo] :: BlockInfo blk -> !SlotNo
[biBlockNo] :: BlockInfo blk -> !BlockNo
[biPrevHash] :: BlockInfo blk -> !ChainHash blk
[biIsEBB] :: BlockInfo blk -> !IsEBB
[biHeaderOffset] :: BlockInfo blk -> !Word16
[biHeaderSize] :: BlockInfo blk -> !Word16
newtype ApiMisuse

-- | The VolatileDB was closed. In case it was automatically closed because
--   an unexpected error was thrown during a read operation or any
--   exception was thrown during a write operation, that exception is
--   embedded.
ClosedDBError :: Maybe SomeException -> ApiMisuse
data UnexpectedFailure blk
FileSystemError :: FsError -> UnexpectedFailure blk

-- | A block failed to parse
ParseError :: FsPath -> RealPoint blk -> DeserialiseFailure -> UnexpectedFailure blk

-- | When parsing a block we got some trailing data
TrailingDataError :: FsPath -> RealPoint blk -> ByteString -> UnexpectedFailure blk

-- | Block missing
--   
--   This exception gets thrown when a block that we <i>know</i> it should
--   be in the VolatileDB, nonetheless was not found.
--   
--   This exception will be thrown by <tt>getKnownBlockComponent</tt>.
MissingBlockError :: HeaderHash blk -> UnexpectedFailure blk

-- | A (parsed) block did not pass the integrity check.
--   
--   This exception gets thrown when a block doesn't pass the integrity
--   check done for <a>GetVerifiedBlock</a>.
--   
--   NOTE: we do not check the integrity of a block when it is added to the
--   VolatileDB. While this exception typically means the block has been
--   corrupted, it could also mean the block didn't pass the check at the
--   time it was added.
CorruptBlockError :: HeaderHash blk -> UnexpectedFailure blk

-- | Errors which might arise when working with this database.
data VolatileDBError blk

-- | An error thrown because of incorrect usage of the VolatileDB by the
--   user.
ApiMisuse :: ApiMisuse -> VolatileDBError blk

-- | An unexpected failure thrown because something went wrong.
UnexpectedFailure :: UnexpectedFailure blk -> VolatileDBError blk
getIsMember :: Functor (STM m) => VolatileDB m blk -> STM m (HeaderHash blk -> Bool)
getKnownBlockComponent :: (MonadThrow m, HasHeader blk) => VolatileDB m blk -> BlockComponent blk b -> HeaderHash blk -> m b
getPredecessor :: Functor (STM m) => VolatileDB m blk -> STM m (HeaderHash blk -> Maybe (ChainHash blk))

-- | Open the database using the given function, perform the given action
--   using the database, and closes the database using its <a>closeDB</a>
--   function, in case of success or when an exception was raised.
withDB :: (HasCallStack, MonadThrow m) => m (VolatileDB m blk) -> (VolatileDB m blk -> m a) -> m a
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.VolatileDB.API.BlockInfo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.VolatileDB.API.BlockInfo blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.VolatileDB.API.BlockInfo blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.VolatileDB.API.BlockInfo blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.VolatileDB.API.VolatileDB m blk)
instance GHC.Show.Show Ouroboros.Consensus.Storage.VolatileDB.API.ApiMisuse
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.VolatileDB.API.VolatileDBError blk)
instance (Data.Typeable.Internal.Typeable blk, Ouroboros.Network.Block.StandardHash blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.VolatileDB.API.UnexpectedFailure blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => GHC.Exception.Type.Exception (Ouroboros.Consensus.Storage.VolatileDB.API.VolatileDBError blk)

module Ouroboros.Consensus.Storage.VolatileDB.Impl.Types

-- | Create a <a>BlocksPerFile</a>.
--   
--   PRECONDITION: the given number must be greater than 0, if not, this
--   function will throw an <a>error</a>.
mkBlocksPerFile :: Word32 -> BlocksPerFile
unBlocksPerFile :: BlocksPerFile -> Word32

-- | The maximum number of blocks to store per file.
data BlocksPerFile

-- | When block validation is enabled, the parser checks for each block a
--   number of properties and stops parsing if it finds any invalid blocks.
data BlockValidationPolicy
NoValidation :: BlockValidationPolicy
ValidateAll :: BlockValidationPolicy

-- | Note that we recover from the error, and thus never throw it as an
--   <tt>Exception</tt>.
--   
--   Defined here instead of in the <tt>Parser</tt> module because
--   <a>TraceEvent</a> depends on it.
data ParseError blk

-- | A block could not be parsed.
BlockReadErr :: ReadIncrementalErr -> ParseError blk

-- | A block was corrupted, e.g., checking its signature and/or hash
--   failed.
BlockCorruptedErr :: HeaderHash blk -> ParseError blk

-- | A block with the same hash occurred twice in the VolatileDB files.
--   
--   We include the file in which it occurred first and the file in which
--   it occured the second time. The two files can be the same.
DuplicatedBlock :: HeaderHash blk -> FsPath -> FsPath -> ParseError blk
data TraceEvent blk
DBAlreadyClosed :: TraceEvent blk
BlockAlreadyHere :: HeaderHash blk -> TraceEvent blk
Truncate :: ParseError blk -> FsPath -> BlockOffset -> TraceEvent blk
InvalidFileNames :: [FsPath] -> TraceEvent blk

-- | The offset at which a block is stored in a file.
newtype BlockOffset
BlockOffset :: Word64 -> BlockOffset
[unBlockOffset] :: BlockOffset -> Word64
newtype BlockSize
BlockSize :: Word32 -> BlockSize
[unBlockSize] :: BlockSize -> Word32

-- | The <a>FileId</a> is the unique identifier of each file found in the
--   db. For example, the file <tt>blocks-42.dat</tt> has <a>FileId</a>
--   <tt>42</tt>.
type FileId = Int

-- | The internal information the db keeps for each block.
data InternalBlockInfo blk
InternalBlockInfo :: !FsPath -> !BlockOffset -> !BlockSize -> !BlockInfo blk -> !SomeSecond (NestedCtxt Header) blk -> InternalBlockInfo blk
[ibiFile] :: InternalBlockInfo blk -> !FsPath
[ibiBlockOffset] :: InternalBlockInfo blk -> !BlockOffset
[ibiBlockSize] :: InternalBlockInfo blk -> !BlockSize
[ibiBlockInfo] :: InternalBlockInfo blk -> !BlockInfo blk
[ibiNestedCtxt] :: InternalBlockInfo blk -> !SomeSecond (NestedCtxt Header) blk

-- | We map the header hash of each block to the <a>InternalBlockInfo</a>.
type ReverseIndex blk = Map (HeaderHash blk) (InternalBlockInfo blk)

-- | For each block, we store the set of all blocks which have this block
--   as a predecessor (set of successors).
type SuccessorsIndex blk = Map (ChainHash blk) (Set (HeaderHash blk))
instance GHC.Show.Show Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlocksPerFile
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlocksPerFile
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockValidationPolicy
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockSize
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockSize
instance GHC.Show.Show Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockSize
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockSize
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockOffset
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockOffset
instance GHC.Show.Show Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockOffset
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.BlockOffset
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.TraceEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.TraceEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.TraceEvent blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.InternalBlockInfo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.InternalBlockInfo blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.ParseError blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.VolatileDB.Impl.Types.ParseError blk)

module Ouroboros.Consensus.Storage.VolatileDB.Impl.Util
filePath :: FileId -> FsPath

-- | This also returns any <a>FsPath</a> which failed to parse.
findLastFd :: [FsPath] -> (Maybe FileId, [FsPath])

-- | Parses the <a>FileId</a> of each <a>FsPath</a> and zips them together.
--   Returns the results sorted on the <a>FileId</a>.
--   
--   Return separately any <a>FsPath</a> which failed to parse.
parseAllFds :: [FsPath] -> ([(FileId, FsPath)], [FsPath])
parseFd :: FsPath -> Maybe FileId

-- | Execute an action and catch the <a>VolatileDBError</a> and
--   <a>FsError</a> that can be thrown by it, and wrap the <a>FsError</a>
--   in an <a>VolatileDBError</a> using the <a>FileSystemError</a>
--   constructor.
--   
--   This should be used whenever you want to run an action on the
--   VolatileDB and catch the <a>VolatileDBError</a> and the <a>FsError</a>
--   (wrapped in the former) it may thrown.
tryVolatileDB :: forall m a blk. (MonadCatch m, Typeable blk, StandardHash blk) => Proxy blk -> m a -> m (Either (VolatileDBError blk) a)
wrapFsError :: forall m a blk. (MonadCatch m, StandardHash blk, Typeable blk) => Proxy blk -> m a -> m a
deleteMapSet :: forall k v. (Ord k, Ord v) => k -> v -> Map k (Set v) -> Map k (Set v)
insertMapSet :: forall k v. (Ord k, Ord v) => k -> v -> Map k (Set v) -> Map k (Set v)


-- | Cache blocks in memory
--   
--   Intended for qualified import.
--   
--   <pre>
--   import           Ouroboros.Consensus.Storage.ChainDB.Impl.BlockCache (BlockCache)
--   import qualified Ouroboros.Consensus.Storage.ChainDB.Impl.BlockCache as BlockCache
--   </pre>
module Ouroboros.Consensus.Storage.ChainDB.Impl.BlockCache
cacheBlock :: HasHeader blk => blk -> BlockCache blk -> BlockCache blk
empty :: BlockCache blk
lookup :: HasHeader blk => HeaderHash blk -> BlockCache blk -> Maybe blk
singleton :: HasHeader blk => blk -> BlockCache blk
data BlockCache blk

module Ouroboros.Consensus.Protocol.LeaderSchedule
newtype LeaderSchedule
LeaderSchedule :: Map SlotNo [CoreNodeId] -> LeaderSchedule
[getLeaderSchedule] :: LeaderSchedule -> Map SlotNo [CoreNodeId]

-- | The <tt>Slots</tt> a given node is supposed to lead in
leaderScheduleFor :: CoreNodeId -> LeaderSchedule -> Set SlotNo
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Protocol.LeaderSchedule.LeaderSchedule
instance GHC.Generics.Generic Ouroboros.Consensus.Protocol.LeaderSchedule.LeaderSchedule
instance GHC.Classes.Ord Ouroboros.Consensus.Protocol.LeaderSchedule.LeaderSchedule
instance GHC.Classes.Eq Ouroboros.Consensus.Protocol.LeaderSchedule.LeaderSchedule
instance GHC.Show.Show Ouroboros.Consensus.Protocol.LeaderSchedule.LeaderSchedule
instance GHC.Base.Semigroup Ouroboros.Consensus.Protocol.LeaderSchedule.LeaderSchedule
instance Ouroboros.Consensus.Util.Condense.Condense Ouroboros.Consensus.Protocol.LeaderSchedule.LeaderSchedule


-- | Exposes the <tt><a>Mempool</a></tt> datatype which captures the public
--   API of the Mempool. Also exposes all the types used to interact with
--   said API.
--   
--   The interface is then initialized in
--   <a>Ouroboros.Consensus.Mempool.Init</a> with the functions from
--   <a>Ouroboros.Consensus.Mempool.Update</a> and
--   <a>Ouroboros.Consensus.Mempool.Query</a>.
module Ouroboros.Consensus.Mempool.API

-- | Mempool
--   
--   The mempool is the set of transactions that should be included in the
--   next block. In principle this is a <i>set</i> of all the transactions
--   that we receive from our peers. In order to avoid flooding the network
--   with invalid transactions, however, we only want to keep <i>valid</i>
--   transactions in the mempool. That raises the question: valid with
--   respect to which ledger state?
--   
--   We opt for a very simple answer to this: the mempool will be
--   interpreted as a <i>list</i> of transactions; which are validated
--   strictly in order, starting from the current ledger state. This has a
--   number of advantages:
--   
--   <ul>
--   <li>It's simple to implement and it's efficient. In particular, no
--   search for a valid subset is ever required.</li>
--   <li>When producing a block, we can simply take the longest possible
--   prefix of transactions that fits in a block.</li>
--   <li>It supports wallets that submit dependent transactions (where
--   later transaction depends on outputs from earlier ones).</li>
--   </ul>
--   
--   The mempool provides fairness guarantees for the case of multiple
--   threads performing <a>addTx</a> concurrently. Implementations of this
--   interface must provide this guarantee, and users of this interface may
--   rely on it. Specifically, multiple threads that continuously use
--   <a>addTx</a> will, over time, get a share of the mempool resource
--   (measured by the number of txs only, not their sizes) roughly
--   proportional to their "weight". The weight depends on the
--   <a>AddTxOnBehalfOf</a>: either acting on behalf of remote peers
--   (<a>AddTxForRemotePeer</a>) or on behalf of a local client
--   (<a>AddTxForLocalClient</a>). The weighting for threads acting on
--   behalf of remote peers is the same for all remote peers, so all remote
--   peers will get a roughly equal share of the resource. The weighting
--   for local clients is the same for all local clients but <i>may</i> be
--   higher than the weighting for remote peers. The weighting is not
--   unboundedly higher however, so there is still (weighted) fairness
--   between remote peers and local clients. Thus local clients will also
--   get a roughly equal share of the resource, but that share may be
--   strictly greater than the share for each remote peer. Furthermore,
--   this implies local clients cannot starve remote peers, despite their
--   higher weighting.
--   
--   This fairness specification in terms of weighting is deliberately
--   non-specific, which allows multiple strategies. The existing default
--   strategy (for the implementation in
--   <a>Ouroboros.Consensus.Mempool</a>) is as follows. The design uses two
--   FIFOs, to give strictly in-order behaviour. All remote peers get equal
--   weight and all local clients get equal weight. The relative weight
--   between remote and local is that if there are N remote peers and M
--   local clients, each local client gets weight 1/(M+1), while all of the
--   N remote peers together also get total weight 1/(M+1). This means
--   individual remote peers get weight 1/(N * (M+1)). Intuitively: a
--   single local client has the same weight as all the remote peers put
--   together.
data Mempool m blk
Mempool :: (AddTxOnBehalfOf -> GenTx blk -> m (MempoolAddTxResult blk)) -> ([GenTxId blk] -> m ()) -> m (MempoolSnapshot blk) -> STM m (MempoolSnapshot blk) -> (ForgeLedgerState blk -> STM m (MempoolSnapshot blk)) -> STM m MempoolCapacityBytes -> (GenTx blk -> TxSizeInBytes) -> Mempool m blk

-- | Add a single transaction to the mempool.
--   
--   The new transaction provided will be validated, <i>in order</i>,
--   against the ledger state obtained by applying all the transactions
--   already in the Mempool to it. Transactions which are found to be
--   invalid, with respect to the ledger state, are dropped, whereas valid
--   transactions are added to the mempool.
--   
--   Note that transactions that are invalid, with respect to the ledger
--   state, will <i>never</i> be added to the mempool. However, it is
--   possible that, at a given point in time, transactions which were once
--   valid but are now invalid, with respect to the current ledger state,
--   could exist within the mempool until they are revalidated and dropped
--   from the mempool via a call to <a>syncWithLedger</a> or by the
--   background thread that watches the ledger for changes.
--   
--   This action returns one of two results
--   
--   <ul>
--   <li>A <a>MempoolTxAdded</a> value if the transaction provided was
--   found to be valid. This transactions is now in the Mempool.</li>
--   <li>A <a>MempoolTxRejected</a> value if the transaction provided was
--   found to be invalid, along with its accompanying validation errors.
--   This transactions is not in the Mempool.</li>
--   </ul>
--   
--   Note that this is a blocking action. It will block until the
--   transaction fits into the mempool. This includes transactions that
--   turn out to be invalid: the action waits for there to be space for the
--   transaction before it gets validated.
--   
--   Note that it is safe to use this from multiple threads concurrently.
--   
--   POSTCONDITION: &gt; let prj = case &gt; MempoolTxAdded vtx -&gt;
--   txForgetValidated vtx &gt; MempoolTxRejected tx _err -&gt; tx &gt;
--   processed &lt;- addTx wti txs &gt; prj processed == tx
--   
--   Note that previously valid transaction that are now invalid with
--   respect to the current ledger state are dropped from the mempool, but
--   are not part of the first returned list (nor the second).
--   
--   In principle it is possible that validation errors are transient; for
--   example, it is possible that a transaction is rejected because one of
--   its inputs is not <i>yet</i> available in the UTxO (the transaction it
--   depends on is not yet in the chain, nor in the mempool). In practice
--   however it is likely that rejected transactions will still be rejected
--   later, and should just be dropped.
--   
--   It is important to note one important special case of transactions
--   being "invalid": a transaction will <i>also</i> be considered invalid
--   if <i>that very same transaction</i> is already included on the
--   blockchain (after all, by definition that must mean its inputs have
--   been used). Rejected transactions are therefore not necessarily a sign
--   of malicious behaviour. Indeed, we would expect <i>most</i>
--   transactions that are reported as invalid by <tt>tryAddTxs</tt> to be
--   invalid precisely because they have already been included.
--   Distinguishing between these two cases can be done in theory, but it
--   is expensive unless we have an index of transaction hashes that have
--   been included on the blockchain.
--   
--   As long as we keep the mempool entirely in-memory this could live in
--   <tt>STM m</tt>; we keep it in <tt>m</tt> instead to leave open the
--   possibility of persistence.
[addTx] :: Mempool m blk -> AddTxOnBehalfOf -> GenTx blk -> m (MempoolAddTxResult blk)

-- | Manually remove the given transactions from the mempool.
[removeTxs] :: Mempool m blk -> [GenTxId blk] -> m ()

-- | Sync the transactions in the mempool with the current ledger state of
--   the <tt>ChainDB</tt>.
--   
--   The transactions that exist within the mempool will be revalidated
--   against the current ledger state. Transactions which are found to be
--   invalid with respect to the current ledger state, will be dropped from
--   the mempool, whereas valid transactions will remain.
--   
--   We keep this in <tt>m</tt> instead of <tt>STM m</tt> to leave open the
--   possibility of persistence. Additionally, this makes it possible to
--   trace the removal of invalid transactions.
--   
--   n.b. in our current implementation, when one opens a mempool, we spawn
--   a thread which performs this action whenever the <tt>ChainDB</tt> tip
--   point changes.
[syncWithLedger] :: Mempool m blk -> m (MempoolSnapshot blk)

-- | Get a snapshot of the current mempool state. This allows for further
--   pure queries on the snapshot.
--   
--   This doesn't look at the ledger state at all.
[getSnapshot] :: Mempool m blk -> STM m (MempoolSnapshot blk)

-- | Get a snapshot of the mempool state that is valid with respect to the
--   given ledger state
--   
--   This does not update the state of the mempool.
[getSnapshotFor] :: Mempool m blk -> ForgeLedgerState blk -> STM m (MempoolSnapshot blk)

-- | Get the mempool's capacity in bytes.
--   
--   Note that the capacity of the Mempool, unless it is overridden with
--   <tt>MempoolCapacityBytesOverride</tt>, can dynamically change when the
--   ledger state is updated: it will be set to twice the current ledger's
--   maximum transaction capacity of a block.
--   
--   When the capacity happens to shrink at some point, we <i>do not</i>
--   remove transactions from the Mempool to satisfy this new lower limit.
--   Instead, we treat it the same way as a Mempool which is <i>at</i>
--   capacity, i.e., we won't admit new transactions until some have been
--   removed because they have become invalid.
[getCapacity] :: Mempool m blk -> STM m MempoolCapacityBytes

-- | Return the post-serialisation size in bytes of a <a>GenTx</a>.
[getTxSize] :: Mempool m blk -> GenTx blk -> TxSizeInBytes

-- | Who are we adding a tx on behalf of, a remote peer or a local client?
--   
--   This affects two things:
--   
--   <ol>
--   <li>how certain errors are treated: we want to be helpful to local
--   clients.</li>
--   <li>priority of service: local clients are prioritised over remote
--   peers.</li>
--   </ol>
--   
--   See <a>Mempool</a> for a discussion of fairness and priority.
data AddTxOnBehalfOf
AddTxForRemotePeer :: AddTxOnBehalfOf
AddTxForLocalClient :: AddTxOnBehalfOf

-- | The result of attempting to add a transaction to the mempool.
data MempoolAddTxResult blk

-- | The transaction was added to the mempool.
MempoolTxAdded :: !Validated (GenTx blk) -> MempoolAddTxResult blk

-- | The transaction was rejected and could not be added to the mempool for
--   the specified reason.
MempoolTxRejected :: !GenTx blk -> !ApplyTxErr blk -> MempoolAddTxResult blk

-- | A wrapper around <a>addTx</a> that adds a sequence of transactions on
--   behalf of a local client. This reports more errors for local clients,
--   see <a>Intervene</a>.
--   
--   Note that transactions are added one by one, and can interleave with
--   other concurrent threads using <a>addTx</a>.
--   
--   See <a>addTx</a> for further details.
addLocalTxs :: forall m blk. MonadSTM m => Mempool m blk -> [GenTx blk] -> m [MempoolAddTxResult blk]

-- | A wrapper around <a>addTx</a> that adds a sequence of transactions on
--   behalf of a remote peer.
--   
--   Note that transactions are added one by one, and can interleave with
--   other concurrent threads using <a>addTx</a>.
--   
--   See <a>addTx</a> for further details.
addTxs :: forall m blk. MonadSTM m => Mempool m blk -> [GenTx blk] -> m [MempoolAddTxResult blk]
isMempoolTxAdded :: MempoolAddTxResult blk -> Bool
isMempoolTxRejected :: MempoolAddTxResult blk -> Bool
mempoolTxAddedToMaybe :: MempoolAddTxResult blk -> Maybe (Validated (GenTx blk))

-- | The ledger state wrt to which we should produce a block
--   
--   The transactions in the mempool will be part of the body of a block,
--   but a block consists of a header and a body, and the full validation
--   of a block consists of first processing its header and only then
--   processing the body. This is important, because processing the header
--   may change the state of the ledger: the update system might be
--   updated, scheduled delegations might be applied, etc., and such
--   changes should take effect before we validate any transactions.
data ForgeLedgerState blk

-- | The slot number of the block is known
--   
--   This will only be the case when we realized that we are the slot
--   leader and we are actually producing a block. It is the caller's
--   responsibility to call <a>applyChainTick</a> and produce the ticked
--   ledger state.
ForgeInKnownSlot :: SlotNo -> TickedLedgerState blk -> ForgeLedgerState blk

-- | The slot number of the block is not yet known
--   
--   When we are validating transactions before we know in which block they
--   will end up, we have to make an assumption about which slot number to
--   use for <a>applyChainTick</a> to prepare the ledger state; we will
--   assume that they will end up in the slot after the slot at the tip of
--   the ledger.
ForgeInUnknownSlot :: LedgerState blk -> ForgeLedgerState blk

-- | A pure snapshot of the contents of the mempool. It allows fetching
--   information about transactions in the mempool, and fetching individual
--   transactions.
--   
--   This uses a transaction sequence number type for identifying
--   transactions within the mempool sequence. The sequence number is local
--   to this mempool, unlike the transaction hash. This allows us to ask
--   for all transactions after a known sequence number, to get new
--   transactions. It is also used to look up individual transactions.
--   
--   Note that it is expected that <tt>getTx</tt> will often return
--   <a>Nothing</a> even for tx sequence numbers returned in previous
--   snapshots. This happens when the transaction has been removed from the
--   mempool between snapshots.
data MempoolSnapshot blk
MempoolSnapshot :: [(Validated (GenTx blk), TicketNo)] -> (TicketNo -> [(Validated (GenTx blk), TicketNo)]) -> (TicketNo -> Maybe (Validated (GenTx blk))) -> (GenTxId blk -> Bool) -> MempoolSize -> SlotNo -> TickedLedgerState blk -> MempoolSnapshot blk

-- | Get all transactions (oldest to newest) in the mempool snapshot along
--   with their ticket number.
[snapshotTxs] :: MempoolSnapshot blk -> [(Validated (GenTx blk), TicketNo)]

-- | Get all transactions (oldest to newest) in the mempool snapshot, along
--   with their ticket number, which are associated with a ticket number
--   greater than the one provided.
[snapshotTxsAfter] :: MempoolSnapshot blk -> TicketNo -> [(Validated (GenTx blk), TicketNo)]

-- | Get a specific transaction from the mempool snapshot by its ticket
--   number, if it exists.
[snapshotLookupTx] :: MempoolSnapshot blk -> TicketNo -> Maybe (Validated (GenTx blk))

-- | Determine whether a specific transaction exists within the mempool
--   snapshot.
[snapshotHasTx] :: MempoolSnapshot blk -> GenTxId blk -> Bool

-- | Get the size of the mempool snapshot.
[snapshotMempoolSize] :: MempoolSnapshot blk -> MempoolSize

-- | The block number of the "virtual block" under construction
[snapshotSlotNo] :: MempoolSnapshot blk -> SlotNo

-- | The ledger state after all transactions in the snapshot
[snapshotLedgerState] :: MempoolSnapshot blk -> TickedLedgerState blk

-- | We allocate each transaction a (monotonically increasing) ticket
--   number as it enters the mempool.
data TicketNo
type TxSizeInBytes = Word32

-- | The transaction ticket number from which our counter starts.
zeroTicketNo :: TicketNo
instance (GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk), GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)), GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.ApplyTxErr blk)) => GHC.Classes.Eq (Ouroboros.Consensus.Mempool.API.MempoolAddTxResult blk)
instance (GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk), GHC.Show.Show (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)), GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.ApplyTxErr blk)) => GHC.Show.Show (Ouroboros.Consensus.Mempool.API.MempoolAddTxResult blk)

module Ouroboros.Consensus.MiniProtocol.LocalTxSubmission.Server

-- | Local transaction submission server, for adding txs to the
--   <a>Mempool</a>
localTxSubmissionServer :: MonadSTM m => Tracer m (TraceLocalTxSubmissionServerEvent blk) -> Mempool m blk -> LocalTxSubmissionServer (GenTx blk) (ApplyTxErr blk) m ()
data TraceLocalTxSubmissionServerEvent blk

-- | A transaction was received.
TraceReceivedTx :: GenTx blk -> TraceLocalTxSubmissionServerEvent blk
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk) => GHC.Classes.Eq (Ouroboros.Consensus.MiniProtocol.LocalTxSubmission.Server.TraceLocalTxSubmissionServerEvent blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk) => GHC.Show.Show (Ouroboros.Consensus.MiniProtocol.LocalTxSubmission.Server.TraceLocalTxSubmissionServerEvent blk)

module Ouroboros.Consensus.HardFork.History.EraParams

-- | Parameters that can vary across hard forks
data EraParams
EraParams :: !EpochSize -> !SlotLength -> !SafeZone -> EraParams
[eraEpochSize] :: EraParams -> !EpochSize
[eraSlotLength] :: EraParams -> !SlotLength
[eraSafeZone] :: EraParams -> !SafeZone

-- | Zone in which it is guaranteed that no hard fork can take place
data SafeZone

-- | Standard safe zone
--   
--   We record
--   
--   <ul>
--   <li>Number of slots from the tip of the ledger. This should be (at
--   least) the number of slots in which we are guaranteed to have
--   <tt>k</tt> blocks.</li>
--   <li>Optionally, an <a>EpochNo</a> before which no hard fork can take
--   place.</li>
--   </ul>
StandardSafeZone :: !Word64 -> SafeZone

-- | Pretend the transition to the next era will not take place.
--   
--   This constructor is marked as unsafe because it effectively extends
--   the safe zone of this era indefinitely into the future. This means
--   that we might reach invalid conclusions when doing
--   
--   <ul>
--   <li>slot to time conversions for blocks that are past the actual safe
--   zone</li>
--   <li>time to slot conversions for the current time, when behind in
--   syncing</li>
--   </ul>
--   
--   This is safe when the code is simply not yet ready to transition to
--   the next era, because in that case, we can be sure that blocks that
--   come in are still from this era. It also means that we can always
--   <i>produce</i> a block, no matter how far ahead of the current ledger
--   we are.
--   
--   If the code is ready for the transition, just awaiting an update
--   proposal, then <tt>LowerBound</tt> can be used instead.
--   
--   This constructor can be regarded as an " extreme " version of
--   <tt>LowerBound</tt>, and can be used for similar reasons.
UnsafeIndefiniteSafeZone :: SafeZone

-- | Default <a>EraParams</a>
--   
--   We set
--   
--   <ul>
--   <li>epoch size to <tt>10k</tt> slots</li>
--   <li>the safe zone to <tt>2k</tt> slots</li>
--   <li>the upper bound to <tt>NoLowerBound</tt></li>
--   </ul>
--   
--   This is primarily useful for tests.
defaultEraParams :: SecurityParam -> SlotLength -> EraParams
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.History.EraParams.SafeZone
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.EraParams.SafeZone
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.History.EraParams.SafeZone
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.EraParams.SafeZone
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.History.EraParams.EraParams
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.EraParams.EraParams
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.History.EraParams.EraParams
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.EraParams.EraParams
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.HardFork.History.EraParams.EraParams
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.HardFork.History.EraParams.SafeZone


-- | Intended for qualified import
--   
--   <pre>
--   import Ouroboros.Consensus.Fragment.Diff (ChainDiff (..))
--   import qualified Ouroboros.Consensus.Fragment.Diff as Diff
--   </pre>
module Ouroboros.Consensus.Fragment.Diff

-- | A diff of a chain (fragment).
--   
--   Typical instantiations of the type argument <tt>b</tt>: a block type
--   <tt>blk</tt>, <tt>Header blk</tt>, <tt>HeaderFields</tt>, ...,
--   anything that supports <a>HasHeader</a>.
--   
--   Note: we allow the suffix to be shorter than the number of blocks to
--   roll back. In other words, applying a <a>ChainDiff</a> can result in a
--   chain shorter than the chain to which the diff was applied.
data ChainDiff b
ChainDiff :: !Word64 -> !AnchoredFragment b -> ChainDiff b

-- | The number of blocks/headers to roll back the current chain
[getRollback] :: ChainDiff b -> !Word64

-- | The new blocks/headers to add after rolling back the current chain.
[getSuffix] :: ChainDiff b -> !AnchoredFragment b

-- | Return the anchor point of the new suffix
getAnchorPoint :: ChainDiff b -> Point b

-- | Return the tip of the new suffix
getTip :: HasHeader b => ChainDiff b -> Point b

-- | Return <a>True</a> iff applying the <a>ChainDiff</a> to a chain
--   <tt>C</tt> will result in a chain shorter than <tt>C</tt>, i.e., the
--   number of blocks to roll back is greater than the length of the new
--   elements in the suffix to add.
rollbackExceedsSuffix :: HasHeader b => ChainDiff b -> Bool

-- | Diff a candidate chain with the current chain.
--   
--   If the candidate fragment is shorter than the current chain,
--   <a>Nothing</a> is returned (this would violate the invariant of
--   <a>ChainDiff</a>).
--   
--   PRECONDITION: the candidate fragment must intersect with the current
--   chain fragment.
diff :: (HasHeader b, HasCallStack) => AnchoredFragment b -> AnchoredFragment b -> ChainDiff b

-- | Make an extension-only (no rollback) <a>ChainDiff</a>.
extend :: AnchoredFragment b -> ChainDiff b

-- | Apply the <a>ChainDiff</a> on the given chain fragment.
--   
--   The fragment is first rolled back a number of blocks before appending
--   the new suffix.
--   
--   If the <a>ChainDiff</a> doesn't fit (anchor point mismatch),
--   <a>Nothing</a> is returned.
--   
--   The returned fragment will have the same anchor point as the given
--   fragment.
apply :: HasHeader b => AnchoredFragment b -> ChainDiff b -> Maybe (AnchoredFragment b)

-- | Append a <tt>b</tt> to a <a>ChainDiff</a>.
--   
--   PRECONDITION: it must fit onto the end of the suffix.
append :: HasHeader b => ChainDiff b -> b -> ChainDiff b
mapM :: forall a b m. (HasHeader b, HeaderHash a ~ HeaderHash b, Monad m) => (a -> m b) -> ChainDiff a -> m (ChainDiff b)

-- | Return the longest prefix of the suffix matching the given predicate,
--   starting from the left, i.e., the "oldest" blocks.
--   
--   If the new suffix is shorter than the diff's rollback, return
--   <a>Nothing</a>.
takeWhileOldest :: HasHeader b => (b -> Bool) -> ChainDiff b -> ChainDiff b

-- | Truncate the diff by rolling back the new suffix to the given point.
--   
--   PRECONDITION: the given point must correspond to one of the new
--   blocks/headers of the new suffix or its anchor (i.e,
--   <tt><a>withinFragmentBounds</a> pt (getSuffix diff)</tt>).
--   
--   If the length of the truncated suffix is shorter than the rollback,
--   <a>Nothing</a> is returned.
truncate :: (HasHeader b, HasCallStack) => Point b -> ChainDiff b -> ChainDiff b
instance (Ouroboros.Network.Block.StandardHash b, GHC.Classes.Eq b) => GHC.Classes.Eq (Ouroboros.Consensus.Fragment.Diff.ChainDiff b)
instance (Ouroboros.Network.Block.StandardHash b, GHC.Show.Show b) => GHC.Show.Show (Ouroboros.Consensus.Fragment.Diff.ChainDiff b)

module Ouroboros.Consensus.Util.RedundantConstraints

-- | Can be used to silence individual "redundant constraint" warnings
--   
--   <pre>
--   foo :: ConstraintUsefulForDebugging =&gt; ...
--   foo =
--       ..
--     where
--       _ = keepRedundantConstraint (Proxy @ConstraintUsefulForDebugging))
--   </pre>
keepRedundantConstraint :: c => proxy c -> ()

-- | <a>Proxy</a> is a type that holds no data, but has a phantom parameter
--   of arbitrary type (or even kind). Its use is to provide type
--   information, even though there is no value available of that type (or
--   it may be too costly to create one).
--   
--   Historically, <tt><a>Proxy</a> :: <a>Proxy</a> a</tt> is a safer
--   alternative to the <tt><a>undefined</a> :: a</tt> idiom.
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy (Void, Int -&gt; Int)
--   Proxy
--   </pre>
--   
--   Proxy can even hold types of higher kinds,
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Either
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Functor
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy complicatedStructure
--   Proxy
--   </pre>
data () => Proxy (t :: k)
Proxy :: Proxy (t :: k)

module Ouroboros.Consensus.Util.Assert
assertEqWithMsg :: (Eq b, Show b, HasCallStack) => (b, b) -> a -> a
assertWithMsg :: HasCallStack => Either String () -> a -> a


-- | Utility functions on anchored fragments
--   
--   Intended for qualified import &gt; import qualified
--   Ouroboros.Consensus.Util.AnchoredFragment as AF
module Ouroboros.Consensus.Util.AnchoredFragment

-- | Compare two (potentially empty!) <a>AnchoredFragment</a>s.
--   
--   PRECONDITION: Either both fragments are non-empty or they intersect.
--   
--   For a detailed discussion of this precondition, and a justification
--   for the definition of this function, please refer to the Consensus
--   Report.
--   
--   Usage note: the primary user of this function is the chain database.
--   It establishes the precondition in two different ways:
--   
--   <ul>
--   <li>When comparing a candidate fragment to our current chain, the
--   fragment is guaranteed (by the chain sync client) to intersect with
--   our chain (indeed, within at most <tt>k</tt> blocks from our tip,
--   although the exact distance does not matter for
--   <a>compareAnchoredFragments</a>).</li>
--   <li>It will only compare candidate fragments that it has previously
--   verified are preferable to our current chain. Since these fragments
--   intersect with our current chain, they must by transitivity also
--   intersect each other.</li>
--   </ul>
compareAnchoredFragments :: forall blk. (BlockSupportsProtocol blk, HasCallStack) => BlockConfig blk -> AnchoredFragment (Header blk) -> AnchoredFragment (Header blk) -> Ordering

-- | Compare the <tt>headBlockNo</tt>, which is a measure of the length of
--   the chain, of two anchored fragments.
--   
--   A fragment with a head is always "greater" than one without. When both
--   fragments have no head (i.e. are empty), they are <a>EQ</a>.
--   
--   Note that an EBB can share its <tt>BlockNo</tt> with another regular
--   block. If such an EBB is the head of one fragment and the regular
--   block with the same <tt>BlockNo</tt> is the head of the other
--   fragment, then this function will say they are <a>EQ</a>, while in
--   fact one fragment should be preferred over the other.
--   
--   This is not a big deal as we won't be seeing new EBBs, so they will
--   not be the head of a fragment very often anyway, only when catching
--   up. As soon as a new block/header is added to the fragment, the right
--   decision will be made again (<a>GT</a> or <a>LT</a>).
compareHeadBlockNo :: HasHeader b => AnchoredFragment b -> AnchoredFragment b -> Ordering
forksAtMostKBlocks :: HasHeader b => Word64 -> AnchoredFragment b -> AnchoredFragment b -> Bool

-- | Lift <tt>preferCandidate</tt> to <a>AnchoredFragment</a>
--   
--   See discussion for <a>compareAnchoredFragments</a>.
preferAnchoredCandidate :: forall blk. (BlockSupportsProtocol blk, HasCallStack) => BlockConfig blk -> AnchoredFragment (Header blk) -> AnchoredFragment (Header blk) -> Bool


-- | Header validation
module Ouroboros.Consensus.HeaderValidation

-- | Header revalidation
--   
--   Same as <a>validateHeader</a> but used when the header has been
--   validated before w.r.t. the same exact <a>HeaderState</a>.
--   
--   Expensive validation checks are skipped (<a>reupdateChainDepState</a>
--   vs. <a>updateChainDepState</a>).
revalidateHeader :: forall blk. (BlockSupportsProtocol blk, ValidateEnvelope blk, HasCallStack) => TopLevelConfig blk -> LedgerView (BlockProtocol blk) -> Header blk -> Ticked (HeaderState blk) -> HeaderState blk

-- | Header validation
--   
--   Header validation (as opposed to block validation) is done by the
--   chain sync client: as we download headers from other network nodes, we
--   validate those headers before deciding whether or not to download the
--   corresponding blocks.
--   
--   Before we <i>adopt</i> any blocks we have downloaded, however, we will
--   do a full block validation. As such, the header validation check can
--   omit some checks (provided that we do those checks when we do the full
--   validation); at worst, this would mean we might download some blocks
--   that we will reject as being invalid where we could have detected that
--   sooner.
--   
--   For this reason, the header validation currently only checks two
--   things:
--   
--   <ul>
--   <li>It verifies the consensus part of the header.</li>
--   </ul>
--   
--   For example, for Praos this means checking the VRF proofs.
--   
--   <ul>
--   <li>It verifies the <a>HasHeader</a> part of the header.</li>
--   </ul>
--   
--   By default, we verify that
--   
--   <ul>
--   <li>Block numbers are consecutive</li>
--   <li>The block number of the first block is <tt>firstBlockNo</tt></li>
--   <li>Slot numbers are strictly increasing</li>
--   <li>The slot number of the first block is at least
--   <a>minimumPossibleSlotNo</a></li>
--   <li>Hashes line up</li>
--   </ul>
--   
--   <i>If</i> a particular ledger wants to verify additional fields in the
--   header, it will get the chance to do so in
--   <tt>applyBlockLedgerResult</tt>, which is passed the entire block (not
--   just the block body).
validateHeader :: (BlockSupportsProtocol blk, ValidateEnvelope blk) => TopLevelConfig blk -> LedgerView (BlockProtocol blk) -> Header blk -> Ticked (HeaderState blk) -> Except (HeaderError blk) (HeaderState blk)

-- | Annotated information about the tip of the chain
--   
--   The annotation is the additional information we need to validate the
--   header envelope. Under normal circumstances no additional information
--   is required, but for instance for Byron we need to know if the
--   previous header was an EBB.
data AnnTip blk
AnnTip :: !SlotNo -> !BlockNo -> !TipInfo blk -> AnnTip blk
[annTipSlotNo] :: AnnTip blk -> !SlotNo
[annTipBlockNo] :: AnnTip blk -> !BlockNo
[annTipInfo] :: AnnTip blk -> !TipInfo blk
class (StandardHash blk, Show (TipInfo blk), Eq (TipInfo blk), NoThunks (TipInfo blk)) => HasAnnTip blk where {
    type TipInfo blk :: Type;
    type TipInfo blk = HeaderHash blk;
}

-- | Extract <a>TipInfo</a> from a block header
getTipInfo :: HasAnnTip blk => Header blk -> TipInfo blk

-- | The tip info must at least include the hash
tipInfoHash :: HasAnnTip blk => proxy blk -> TipInfo blk -> HeaderHash blk

-- | The tip info must at least include the hash
tipInfoHash :: (HasAnnTip blk, TipInfo blk ~ HeaderHash blk) => proxy blk -> TipInfo blk -> HeaderHash blk

-- | Extract <a>TipInfo</a> from a block header
getTipInfo :: (HasAnnTip blk, TipInfo blk ~ HeaderHash blk, HasHeader (Header blk)) => Header blk -> TipInfo blk
annTipHash :: forall blk. HasAnnTip blk => AnnTip blk -> HeaderHash blk
annTipPoint :: forall blk. HasAnnTip blk => AnnTip blk -> Point blk
annTipRealPoint :: forall blk. HasAnnTip blk => AnnTip blk -> RealPoint blk
castAnnTip :: TipInfo blk ~ TipInfo blk' => AnnTip blk -> AnnTip blk'
getAnnTip :: (HasHeader (Header blk), HasAnnTip blk) => Header blk -> AnnTip blk
mapAnnTip :: (TipInfo blk -> TipInfo blk') -> AnnTip blk -> AnnTip blk'

-- | State required to validate the header
--   
--   See <a>validateHeader</a> for details
data HeaderState blk
HeaderState :: !WithOrigin (AnnTip blk) -> !ChainDepState (BlockProtocol blk) -> HeaderState blk
[headerStateTip] :: HeaderState blk -> !WithOrigin (AnnTip blk)
[headerStateChainDep] :: HeaderState blk -> !ChainDepState (BlockProtocol blk)
castHeaderState :: (Coercible (ChainDepState (BlockProtocol blk)) (ChainDepState (BlockProtocol blk')), TipInfo blk ~ TipInfo blk') => HeaderState blk -> HeaderState blk'
genesisHeaderState :: ChainDepState (BlockProtocol blk) -> HeaderState blk
headerStateBlockNo :: HeaderState blk -> WithOrigin BlockNo
headerStatePoint :: HasAnnTip blk => HeaderState blk -> Point blk

-- | Tick the <a>ChainDepState</a> inside the <a>HeaderState</a>
tickHeaderState :: ConsensusProtocol (BlockProtocol blk) => ConsensusConfig (BlockProtocol blk) -> LedgerView (BlockProtocol blk) -> SlotNo -> HeaderState blk -> Ticked (HeaderState blk)

-- | Ledger-independent envelope validation (block, slot, hash)
class (HasHeader (Header blk), HasAnnTip blk) => BasicEnvelopeValidation blk

-- | The block number of the first block on the chain
expectedFirstBlockNo :: BasicEnvelopeValidation blk => proxy blk -> BlockNo

-- | Next block number
expectedNextBlockNo :: BasicEnvelopeValidation blk => proxy blk -> TipInfo blk -> TipInfo blk -> BlockNo -> BlockNo

-- | The smallest possible <a>SlotNo</a>
--   
--   NOTE: This does not affect the translation between <a>SlotNo</a> and
--   <a>EpochNo</a>. <a>Ouroboros.Consensus.HardFork.History</a> for
--   details.
minimumPossibleSlotNo :: BasicEnvelopeValidation blk => Proxy blk -> SlotNo

-- | Minimum next slot number
minimumNextSlotNo :: BasicEnvelopeValidation blk => proxy blk -> TipInfo blk -> TipInfo blk -> SlotNo -> SlotNo
data HeaderEnvelopeError blk

-- | Invalid block number
--   
--   We record both the expected and actual block number
UnexpectedBlockNo :: !BlockNo -> !BlockNo -> HeaderEnvelopeError blk

-- | Invalid slot number
--   
--   We record both the expected (minimum) and actual slot number
UnexpectedSlotNo :: !SlotNo -> !SlotNo -> HeaderEnvelopeError blk

-- | Invalid hash (in the reference to the previous block)
--   
--   We record the current tip as well as the prev hash of the new block.
UnexpectedPrevHash :: !WithOrigin (HeaderHash blk) -> !ChainHash blk -> HeaderEnvelopeError blk

-- | Block specific envelope error
OtherHeaderEnvelopeError :: !OtherHeaderEnvelopeError blk -> HeaderEnvelopeError blk

-- | Validate header envelope
class (BasicEnvelopeValidation blk, GetPrevHash blk, Eq (OtherHeaderEnvelopeError blk), Show (OtherHeaderEnvelopeError blk), NoThunks (OtherHeaderEnvelopeError blk)) => ValidateEnvelope blk where {
    
    -- | A block-specific error that <a>validateEnvelope</a> can return.
    type OtherHeaderEnvelopeError blk :: Type;
    type OtherHeaderEnvelopeError blk = Void;
}

-- | Do additional envelope checks
additionalEnvelopeChecks :: ValidateEnvelope blk => TopLevelConfig blk -> LedgerView (BlockProtocol blk) -> Header blk -> Except (OtherHeaderEnvelopeError blk) ()
castHeaderEnvelopeError :: (HeaderHash blk ~ HeaderHash blk', OtherHeaderEnvelopeError blk ~ OtherHeaderEnvelopeError blk') => HeaderEnvelopeError blk -> HeaderEnvelopeError blk'

-- | Invalid header
data HeaderError blk

-- | Invalid consensus protocol fields
HeaderProtocolError :: !ValidationErr (BlockProtocol blk) -> HeaderError blk

-- | Failed to validate the envelope
HeaderEnvelopeError :: !HeaderEnvelopeError blk -> HeaderError blk
castHeaderError :: (ValidationErr (BlockProtocol blk) ~ ValidationErr (BlockProtocol blk'), HeaderHash blk ~ HeaderHash blk', OtherHeaderEnvelopeError blk ~ OtherHeaderEnvelopeError blk') => HeaderError blk -> HeaderError blk'

-- | Reusable strict data type for <a>TipInfo</a> in case the
--   <a>TipInfo</a> should contain <a>IsEBB</a> in addition to the
--   <a>HeaderHash</a>.
data TipInfoIsEBB blk
TipInfoIsEBB :: !HeaderHash blk -> !IsEBB -> TipInfoIsEBB blk
decodeAnnTipIsEBB :: TipInfo blk ~ TipInfoIsEBB blk => (forall s. Decoder s (HeaderHash blk)) -> forall s. Decoder s (AnnTip blk)
decodeHeaderState :: (forall s. Decoder s (ChainDepState (BlockProtocol blk))) -> (forall s. Decoder s (AnnTip blk)) -> forall s. Decoder s (HeaderState blk)
defaultDecodeAnnTip :: TipInfo blk ~ HeaderHash blk => (forall s. Decoder s (HeaderHash blk)) -> forall s. Decoder s (AnnTip blk)
defaultEncodeAnnTip :: TipInfo blk ~ HeaderHash blk => (HeaderHash blk -> Encoding) -> AnnTip blk -> Encoding
encodeAnnTipIsEBB :: TipInfo blk ~ TipInfoIsEBB blk => (HeaderHash blk -> Encoding) -> AnnTip blk -> Encoding
encodeHeaderState :: (ChainDepState (BlockProtocol blk) -> Encoding) -> (AnnTip blk -> Encoding) -> HeaderState blk -> Encoding

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type
instance GHC.Generics.Generic (Ouroboros.Consensus.HeaderValidation.AnnTip blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.HeaderValidation.HeaderState blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.HeaderValidation.HeaderEnvelopeError blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.HeaderValidation.HeaderError blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.HeaderValidation.TipInfoIsEBB blk)
instance Ouroboros.Consensus.HeaderValidation.HasAnnTip blk => GHC.Show.Show (Ouroboros.Consensus.HeaderValidation.AnnTip blk)
instance Ouroboros.Consensus.HeaderValidation.HasAnnTip blk => GHC.Classes.Eq (Ouroboros.Consensus.HeaderValidation.AnnTip blk)
instance Ouroboros.Consensus.HeaderValidation.HasAnnTip blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderValidation.AnnTip blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.HasAnnTip blk) => GHC.Classes.Eq (Ouroboros.Consensus.HeaderValidation.HeaderState blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.HasAnnTip blk) => GHC.Show.Show (Ouroboros.Consensus.HeaderValidation.HeaderState blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.HasAnnTip blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderValidation.HeaderState blk)
instance Ouroboros.Consensus.HeaderValidation.ValidateEnvelope blk => GHC.Classes.Eq (Ouroboros.Consensus.HeaderValidation.HeaderEnvelopeError blk)
instance Ouroboros.Consensus.HeaderValidation.ValidateEnvelope blk => GHC.Show.Show (Ouroboros.Consensus.HeaderValidation.HeaderEnvelopeError blk)
instance Ouroboros.Consensus.HeaderValidation.ValidateEnvelope blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderValidation.HeaderEnvelopeError blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.ValidateEnvelope blk) => GHC.Classes.Eq (Ouroboros.Consensus.HeaderValidation.HeaderError blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.ValidateEnvelope blk) => GHC.Show.Show (Ouroboros.Consensus.HeaderValidation.HeaderError blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.ValidateEnvelope blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderValidation.HeaderError blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.HeaderValidation.TipInfoIsEBB blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.HeaderValidation.TipInfoIsEBB blk)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderValidation.TipInfoIsEBB blk)
instance Ouroboros.Network.AnchoredSeq.Anchorable (Cardano.Slotting.Slot.WithOrigin Cardano.Slotting.Slot.SlotNo) (Ouroboros.Consensus.HeaderValidation.HeaderState blk) (Ouroboros.Consensus.HeaderValidation.HeaderState blk)


-- | Intended for qualified import
--   
--   <pre>
--   import Ouroboros.Consensus.Fragment.Validated (ValidatedFragment)
--   import qualified Ouroboros.Consensus.Fragment.Validated as VF
--   </pre>
module Ouroboros.Consensus.Fragment.Validated

-- | Validated chain fragment along with the ledger state after validation
--   
--   INVARIANT:
--   
--   <pre>
--   AF.headPoint validatedFragment == ledgerTipPoint validatedLedger
--   </pre>
data ValidatedFragment b l
pattern ValidatedFragment :: (GetTip l, HasHeader b, HeaderHash b ~ HeaderHash l, HasCallStack) => AnchoredFragment b -> l -> ValidatedFragment b l

-- | Chain fragment
validatedFragment :: ValidatedFragment b l -> AnchoredFragment b

-- | Ledger after after validation
validatedLedger :: ValidatedFragment b l -> l
validatedTip :: HasHeader b => ValidatedFragment b l -> Point b
instance GHC.Base.Functor (Ouroboros.Consensus.Fragment.Validated.ValidatedFragment b)


-- | Intended for qualified import
--   
--   <pre>
--   import Ouroboros.Consensus.Fragment.ValidatedDiff (ValidatedChainDiff (..))
--   import qualified Ouroboros.Consensus.Fragment.ValidatedDiff as ValidatedDiff
--   </pre>
module Ouroboros.Consensus.Fragment.ValidatedDiff

-- | A <a>ChainDiff</a> along with the ledger state after validation.
--   
--   INVARIANT:
--   
--   <pre>
--   getTip chainDiff == ledgerTipPoint ledger
--   </pre>
data ValidatedChainDiff b l

-- | Allow for pattern matching on a <a>ValidatedChainDiff</a> without
--   exposing the (unsafe) constructor. Use <a>new</a> to construct a
--   <a>ValidatedChainDiff</a>.
pattern ValidatedChainDiff :: ChainDiff b -> l -> ValidatedChainDiff b l
getChainDiff :: ValidatedChainDiff b l -> ChainDiff b
getLedger :: ValidatedChainDiff b l -> l

-- | Create a <a>ValidatedChainDiff</a>.
--   
--   PRECONDITION:
--   
--   <pre>
--   getTip chainDiff == ledgerTipPoint ledger
--   </pre>
new :: forall b l. (GetTip l, HasHeader b, HeaderHash l ~ HeaderHash b, HasCallStack) => ChainDiff b -> l -> ValidatedChainDiff b l
rollbackExceedsSuffix :: HasHeader b => ValidatedChainDiff b l -> Bool
toValidatedFragment :: (GetTip l, HasHeader b, HeaderHash l ~ HeaderHash b, HasCallStack) => ValidatedChainDiff b l -> ValidatedFragment b l

module Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal

-- | Size of the chunks of the immutable DB
--   
--   This is the key data structure that drives all layout functions.
--   
--   TODO: Add support for non-uniform <a>ChunkInfo</a>
--   <a>https://github.com/IntersectMBO/ouroboros-network/issues/1754</a>
data ChunkInfo

-- | A single, uniform, chunk size
--   
--   If EBBs are present, the chunk size must line up precisely with the
--   epoch size (that is, the number of regular blocks in the chunk must
--   equal the number of regular blocks in an epoch).
UniformChunkSize :: !ChunkSize -> ChunkInfo

-- | Can we store EBBs in the chunks described by this <a>ChunkInfo</a>?
--   
--   This is only used for tests. This API will need to change (and the
--   tests will become more complicated) once we support non-uniform
--   <a>ChunkInfo</a>.
chunkInfoSupportsEBBs :: ChunkInfo -> Bool

-- | Simple chunk config with a single chunk size
--   
--   This intentionally takes <a>EpochSize</a> (number of slots) rather
--   than <a>ChunkSize</a>: the translation from <a>EpochSize</a> to
--   <a>ChunkSize</a> (number of available entries in a chunk) should not
--   be done by client code.
simpleChunkInfo :: EpochSize -> ChunkInfo

-- | <a>ChunkInfo</a> for a single <a>ChunkSize</a>
--   
--   See also <a>simpleChunkInfo</a>.
singleChunkInfo :: ChunkSize -> ChunkInfo

-- | Chunk number
newtype ChunkNo
ChunkNo :: Word64 -> ChunkNo
[unChunkNo] :: ChunkNo -> Word64

-- | Convert <a>Int</a> to <a>ChunkNo</a>
--   
--   See <a>chunkNoToInt</a> for motivation.
chunkNoFromInt :: Int -> ChunkNo

-- | Convert <a>ChunkNo</a> to <a>Int</a>
--   
--   This is primarily useful for the immutable DB, which uses an
--   <tt>IntPSQ</tt>.
chunkNoToInt :: ChunkNo -> Int

-- | Enumerate all chunks
--   
--   <pre>
--   chunksBetween x              x  == [x]
--   chunksBetween x (nextChunkNo x) == [x, nextChunkNo x]
--   </pre>
chunksBetween :: ChunkNo -> ChunkNo -> [ChunkNo]

-- | Count number of chunks between two indices
--   
--   <pre>
--   countChunks x              x  == 0
--   countChunks x (nextChunkNo x) == 1
--   </pre>
countChunks :: ChunkNo -> ChunkNo -> Word64

-- | First chunk
firstChunkNo :: ChunkNo
nextChunkNo :: ChunkNo -> ChunkNo
prevChunkNo :: ChunkNo -> Maybe ChunkNo

-- | Translate <a>ChunkNo</a> to <a>EpochNo</a>
--   
--   This should <i>ONLY</i> be used for chunks that contain EBBs. See
--   <a>unsafeEpochNoToChunkNo</a> and <a>ChunkInfo</a> for details.
unsafeChunkNoToEpochNo :: ChunkNo -> EpochNo

-- | Translate <a>EpochNo</a> to <a>ChunkNo</a>
--   
--   This should <i>ONLY</i> be used to translate the <a>EpochNo</a> of an
--   EBB, since the invariant says EBBs can only exist in the first period
--   of the DB, where the chunk size must equal the epoch size. See
--   <a>ChunkInfo</a> for details.
unsafeEpochNoToChunkNo :: EpochNo -> ChunkNo

-- | Size of a chunk
--   
--   The total number of slots available in a chunk is equal to
--   <a>numRegularBlocks</a> if <tt>not</tt> <a>chunkCanContainEBB</a>, and
--   <a>numRegularBlocks</a> <tt>+ 1</tt> otherwise.
data ChunkSize
ChunkSize :: !Bool -> !Word64 -> ChunkSize

-- | Does this chunk also accomodate an EBB?
[chunkCanContainEBB] :: ChunkSize -> !Bool

-- | The number of regular blocks in this chunk
[numRegularBlocks] :: ChunkSize -> !Word64
getChunkSize :: ChunkInfo -> ChunkNo -> ChunkSize

-- | A <i>relative</i> slot within a chunk
data RelativeSlot
RelativeSlot :: !ChunkNo -> !ChunkSize -> !Word64 -> RelativeSlot

-- | The chunk index of the chunk this slot is in
--   
--   Recorded primarily to be able to define a semi-sensible <a>Ord</a>
--   instance.
[relativeSlotChunkNo] :: RelativeSlot -> !ChunkNo

-- | The size of the chunk that this slot is in
--   
--   We record this for bounds checking as well as to be able to answer
--   questions such as <tt>relativeSlotIsEBB</tt>.
[relativeSlotChunkSize] :: RelativeSlot -> !ChunkSize

-- | The index within the chunk
[relativeSlotIndex] :: RelativeSlot -> !Word64
assertRelativeSlotInChunk :: HasCallStack => ChunkNo -> RelativeSlot -> Word64

-- | <a>RelativeSlot</a> is partially ordered, not totally ordered
--   
--   It makes no sense to compare <tt>RelativeSlots</tt> from different
--   chunks. Doing so will result in an assertion failure.
compareRelativeSlot :: HasCallStack => RelativeSlot -> RelativeSlot -> Ordering

-- | Maximum relative index within a chunk
maxRelativeIndex :: ChunkSize -> Word64

-- | Smart constructor for <a>RelativeSlot</a>
mkRelativeSlot :: HasCallStack => ChunkInfo -> ChunkNo -> Word64 -> RelativeSlot
data ChunkAssertionFailure
assertChunkCanContainEBB :: HasCallStack => ChunkNo -> ChunkSize -> a -> a
assertSameChunk :: HasCallStack => ChunkNo -> ChunkNo -> a -> a
assertWithinBounds :: HasCallStack => Word64 -> ChunkSize -> a -> a
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkSize
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkSize
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkSize
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkInfo
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkInfo
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkInfo
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkNo
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkNo
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkNo
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkNo
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkNo
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.RelativeSlot
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.RelativeSlot
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.RelativeSlot
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkAssertionFailure
instance GHC.Exception.Type.Exception Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.ChunkAssertionFailure
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Internal.RelativeSlot

module Ouroboros.Consensus.Ledger.Inspect
class (Show (LedgerWarning blk), Show (LedgerUpdate blk), Eq (LedgerWarning blk), Eq (LedgerUpdate blk), Condense (LedgerUpdate blk)) => InspectLedger blk where {
    type LedgerWarning blk :: Type;
    type LedgerUpdate blk :: Type;
    type LedgerWarning blk = Void;
    type LedgerUpdate blk = Void;
}

-- | Inspect the ledger
--   
--   The point of the inspection is to see if the state of the ledger might
--   indicate a potential misconfiguration of the node.
--   
--   TODO: We might at some point need to generalize this to
--   <tt>ExtLedgerState</tt> instead. That doesn't fit quite so neatly with
--   the HFC at present, so leaving it at this for now.
inspectLedger :: InspectLedger blk => TopLevelConfig blk -> LedgerState blk -> LedgerState blk -> [LedgerEvent blk]

-- | Inspect the ledger
--   
--   The point of the inspection is to see if the state of the ledger might
--   indicate a potential misconfiguration of the node.
--   
--   TODO: We might at some point need to generalize this to
--   <tt>ExtLedgerState</tt> instead. That doesn't fit quite so neatly with
--   the HFC at present, so leaving it at this for now.
inspectLedger :: (InspectLedger blk, LedgerWarning blk ~ Void, LedgerUpdate blk ~ Void) => TopLevelConfig blk -> LedgerState blk -> LedgerState blk -> [LedgerEvent blk]
data LedgerEvent blk
LedgerWarning :: LedgerWarning blk -> LedgerEvent blk
LedgerUpdate :: LedgerUpdate blk -> LedgerEvent blk
castLedgerEvent :: (LedgerWarning blk ~ LedgerWarning blk', LedgerUpdate blk ~ LedgerUpdate blk') => LedgerEvent blk -> LedgerEvent blk'
partitionLedgerEvents :: [LedgerEvent blk] -> ([LedgerWarning blk], [LedgerUpdate blk])
instance Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk => GHC.Show.Show (Ouroboros.Consensus.Ledger.Inspect.LedgerEvent blk)
instance Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Inspect.LedgerEvent blk)


-- | Newtypes around type families so that they can be partially applied
module Ouroboros.Consensus.TypeFamilyWrappers
newtype WrapApplyTxErr blk
WrapApplyTxErr :: ApplyTxErr blk -> WrapApplyTxErr blk
[unwrapApplyTxErr] :: WrapApplyTxErr blk -> ApplyTxErr blk
newtype WrapCannotForge blk
WrapCannotForge :: CannotForge blk -> WrapCannotForge blk
[unwrapCannotForge] :: WrapCannotForge blk -> CannotForge blk
newtype WrapEnvelopeErr blk
WrapEnvelopeErr :: OtherHeaderEnvelopeError blk -> WrapEnvelopeErr blk
[unwrapEnvelopeErr] :: WrapEnvelopeErr blk -> OtherHeaderEnvelopeError blk
newtype WrapForgeStateInfo blk
WrapForgeStateInfo :: ForgeStateInfo blk -> WrapForgeStateInfo blk
[unwrapForgeStateInfo] :: WrapForgeStateInfo blk -> ForgeStateInfo blk
newtype WrapForgeStateUpdateError blk
WrapForgeStateUpdateError :: ForgeStateUpdateError blk -> WrapForgeStateUpdateError blk
[unwrapForgeStateUpdateError] :: WrapForgeStateUpdateError blk -> ForgeStateUpdateError blk
newtype WrapGenTxId blk
WrapGenTxId :: GenTxId blk -> WrapGenTxId blk
[unwrapGenTxId] :: WrapGenTxId blk -> GenTxId blk
newtype WrapHeaderHash blk
WrapHeaderHash :: HeaderHash blk -> WrapHeaderHash blk
[unwrapHeaderHash] :: WrapHeaderHash blk -> HeaderHash blk
newtype WrapLedgerConfig blk
WrapLedgerConfig :: LedgerConfig blk -> WrapLedgerConfig blk
[unwrapLedgerConfig] :: WrapLedgerConfig blk -> LedgerConfig blk
newtype WrapLedgerErr blk
WrapLedgerErr :: LedgerError blk -> WrapLedgerErr blk
[unwrapLedgerErr] :: WrapLedgerErr blk -> LedgerError blk
newtype WrapLedgerEvent blk
WrapLedgerEvent :: AuxLedgerEvent (LedgerState blk) -> WrapLedgerEvent blk
[unwrapLedgerEvent] :: WrapLedgerEvent blk -> AuxLedgerEvent (LedgerState blk)
newtype WrapLedgerUpdate blk
WrapLedgerUpdate :: LedgerUpdate blk -> WrapLedgerUpdate blk
[unwrapLedgerUpdate] :: WrapLedgerUpdate blk -> LedgerUpdate blk
newtype WrapLedgerWarning blk
WrapLedgerWarning :: LedgerWarning blk -> WrapLedgerWarning blk
[unwrapLedgerWarning] :: WrapLedgerWarning blk -> LedgerWarning blk
newtype WrapTipInfo blk
WrapTipInfo :: TipInfo blk -> WrapTipInfo blk
[unwrapTipInfo] :: WrapTipInfo blk -> TipInfo blk
newtype WrapCanBeLeader blk
WrapCanBeLeader :: CanBeLeader (BlockProtocol blk) -> WrapCanBeLeader blk
[unwrapCanBeLeader] :: WrapCanBeLeader blk -> CanBeLeader (BlockProtocol blk)
newtype WrapChainDepState blk
WrapChainDepState :: ChainDepState (BlockProtocol blk) -> WrapChainDepState blk
[unwrapChainDepState] :: WrapChainDepState blk -> ChainDepState (BlockProtocol blk)
newtype WrapConsensusConfig blk
WrapConsensusConfig :: ConsensusConfig (BlockProtocol blk) -> WrapConsensusConfig blk
[unwrapConsensusConfig] :: WrapConsensusConfig blk -> ConsensusConfig (BlockProtocol blk)
newtype WrapIsLeader blk
WrapIsLeader :: IsLeader (BlockProtocol blk) -> WrapIsLeader blk
[unwrapIsLeader] :: WrapIsLeader blk -> IsLeader (BlockProtocol blk)
newtype WrapLedgerView blk
WrapLedgerView :: LedgerView (BlockProtocol blk) -> WrapLedgerView blk
[unwrapLedgerView] :: WrapLedgerView blk -> LedgerView (BlockProtocol blk)
newtype WrapSelectView blk
WrapSelectView :: SelectView (BlockProtocol blk) -> WrapSelectView blk
[unwrapSelectView] :: WrapSelectView blk -> SelectView (BlockProtocol blk)
newtype WrapValidateView blk
WrapValidateView :: ValidateView (BlockProtocol blk) -> WrapValidateView blk
[unwrapValidateView] :: WrapValidateView blk -> ValidateView (BlockProtocol blk)

-- | A data family wrapper for <tt><a>Validated</a> . <a>GenTx</a></tt>
--   
--   <a>Validated</a> is is data family, so this is an outlier in this
--   module full of type family wrappers. However, the standard functor
--   composition operator <tt>f :.: g</tt> incurs some type classes
--   instances that are inappropriate when the outer type constructor
--   <tt>f</tt> is a family and hence non-parametric (eg <tt><a>Eq</a> (f
--   :.: g)</tt> requires @<a>Eq1</a> f)). The bespoke composition
--   <a>WrapValidatedGenTx</a> therefore serves much the same purpose as
--   the other wrappers in this module.
newtype WrapValidatedGenTx blk
WrapValidatedGenTx :: Validated (GenTx blk) -> WrapValidatedGenTx blk
[unwrapValidatedGenTx] :: WrapValidatedGenTx blk -> Validated (GenTx blk)
newtype WrapValidationErr blk
WrapValidationErr :: ValidationErr (BlockProtocol blk) -> WrapValidationErr blk
[unwrapValidationErr] :: WrapValidationErr blk -> ValidationErr (BlockProtocol blk)
newtype WrapNodeToClientVersion blk
WrapNodeToClientVersion :: BlockNodeToClientVersion blk -> WrapNodeToClientVersion blk
[unwrapNodeToClientVersion] :: WrapNodeToClientVersion blk -> BlockNodeToClientVersion blk
newtype WrapNodeToNodeVersion blk
WrapNodeToNodeVersion :: BlockNodeToNodeVersion blk -> WrapNodeToNodeVersion blk
[unwrapNodeToNodeVersion] :: WrapNodeToNodeVersion blk -> BlockNodeToNodeVersion blk

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.ApplyTxErr blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapApplyTxErr blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Basics.LedgerError blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerErr blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Inspect.LedgerUpdate blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerUpdate blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Inspect.LedgerWarning blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerWarning blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.HeaderValidation.OtherHeaderEnvelopeError blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapEnvelopeErr blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.HeaderValidation.TipInfo blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapTipInfo blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapValidatedGenTx blk)
instance GHC.Classes.Ord (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => GHC.Classes.Ord (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.ApplyTxErr blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapApplyTxErr blk)
instance GHC.Show.Show (Ouroboros.Consensus.Block.Forging.CannotForge blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapCannotForge blk)
instance GHC.Show.Show (Ouroboros.Consensus.Block.Forging.ForgeStateInfo blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapForgeStateInfo blk)
instance GHC.Show.Show (Ouroboros.Consensus.Block.Forging.ForgeStateUpdateError blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapForgeStateUpdateError blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.Basics.LedgerError blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerErr blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.Inspect.LedgerUpdate blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerUpdate blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.Inspect.LedgerWarning blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerWarning blk)
instance GHC.Show.Show (Ouroboros.Consensus.HeaderValidation.OtherHeaderEnvelopeError blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapEnvelopeErr blk)
instance GHC.Show.Show (Ouroboros.Consensus.HeaderValidation.TipInfo blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapTipInfo blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapValidatedGenTx blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Basics.LedgerError blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerErr blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderValidation.OtherHeaderEnvelopeError blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapEnvelopeErr blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderValidation.TipInfo blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapTipInfo blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapValidatedGenTx blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Protocol.Abstract.ChainDepState (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapSelectView blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Protocol.Abstract.ValidationErr (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapValidationErr blk)
instance GHC.Classes.Ord (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Classes.Ord (Ouroboros.Consensus.TypeFamilyWrappers.WrapSelectView blk)
instance GHC.Show.Show (Ouroboros.Consensus.Protocol.Abstract.ChainDepState (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState blk)
instance GHC.Show.Show (Ouroboros.Consensus.Protocol.Abstract.LedgerView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerView blk)
instance GHC.Show.Show (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapSelectView blk)
instance GHC.Show.Show (Ouroboros.Consensus.Protocol.Abstract.ValidationErr (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapValidationErr blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.ChainDepState (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapSelectView blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.ValidationErr (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.TypeFamilyWrappers.WrapValidationErr blk)
instance GHC.Show.Show (Ouroboros.Consensus.Node.NetworkProtocolVersion.BlockNodeToNodeVersion blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapNodeToNodeVersion blk)
instance GHC.Show.Show (Ouroboros.Consensus.Node.NetworkProtocolVersion.BlockNodeToClientVersion blk) => GHC.Show.Show (Ouroboros.Consensus.TypeFamilyWrappers.WrapNodeToClientVersion blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Node.NetworkProtocolVersion.BlockNodeToNodeVersion blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapNodeToNodeVersion blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Node.NetworkProtocolVersion.BlockNodeToClientVersion blk) => GHC.Classes.Eq (Ouroboros.Consensus.TypeFamilyWrappers.WrapNodeToClientVersion blk)
instance Codec.Serialise.Class.Serialise (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)
instance Codec.Serialise.Class.Serialise (Ouroboros.Consensus.Protocol.Abstract.ChainDepState (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState blk)
instance Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HeaderValidation.TipInfo blk) => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.TypeFamilyWrappers.WrapTipInfo blk)


-- | Serialisation for on-disk storage.
--   
--   We have separate classes for on-disk and on-the-wire serialisation,
--   because the encoding formats of the same type may differ, depending on
--   the context.
--   
--   We separate the encoder from the decoder, because sometimes the
--   encoded type will differ from the decoded one. For example, we encode
--   a <tt>blk</tt>, but decode an <tt><a>ByteString</a> -&gt; blk</tt>
--   (when reading something from disk, we have the precise bytestring that
--   we can pass in as the annotation). If we coupled the encoder to the
--   decoder, we wouldn't be able to cleanly model this use case. Moreover,
--   sometimes we only need a single direction.
module Ouroboros.Consensus.Storage.Serialisation

-- | Decode a type <tt>a</tt> read from disk.
--   
--   There is no version negotiation for on disk serialisation. However,
--   instances can still decide to perform versioning internally to
--   maintain compatibility.
class DecodeDisk blk a
decodeDisk :: DecodeDisk blk a => CodecConfig blk -> forall s. Decoder s a
decodeDisk :: (DecodeDisk blk a, Serialise a) => CodecConfig blk -> forall s. Decoder s a

-- | Encode a type <tt>a</tt> so that it can be stored on disk.
--   
--   There is no version negotiation for on disk serialisation. However,
--   instances can still decide to perform versioning internally to
--   maintain compatibility.
class EncodeDisk blk a
encodeDisk :: EncodeDisk blk a => CodecConfig blk -> a -> Encoding
encodeDisk :: (EncodeDisk blk a, Serialise a) => CodecConfig blk -> a -> Encoding

-- | Decode a dependent value
--   
--   Typical usage: <tt>f = NestedCtxt Header</tt>.
class DecodeDiskDep f blk
decodeDiskDep :: DecodeDiskDep f blk => CodecConfig blk -> f blk a -> forall s. Decoder s (ByteString -> a)
decodeDiskDep :: (DecodeDiskDep f blk, TrivialDependency (f blk), DecodeDisk blk (ByteString -> TrivialIndex (f blk))) => CodecConfig blk -> f blk a -> forall s. Decoder s (ByteString -> a)

-- | Decode dependent index
class DecodeDiskDepIx f blk
decodeDiskDepIx :: DecodeDiskDepIx f blk => CodecConfig blk -> Decoder s (SomeSecond f blk)
decodeDiskDepIx :: (DecodeDiskDepIx f blk, TrivialDependency (f blk)) => CodecConfig blk -> Decoder s (SomeSecond f blk)

-- | Encode a dependent value
class EncodeDiskDep f blk
encodeDiskDep :: EncodeDiskDep f blk => CodecConfig blk -> f blk a -> a -> Encoding
encodeDiskDep :: (EncodeDiskDep f blk, TrivialDependency (f blk), EncodeDisk blk (TrivialIndex (f blk))) => CodecConfig blk -> f blk a -> a -> Encoding

-- | Encode dependent index
class EncodeDiskDepIx f blk
encodeDiskDepIx :: EncodeDiskDepIx f blk => CodecConfig blk -> SomeSecond f blk -> Encoding
encodeDiskDepIx :: (EncodeDiskDepIx f blk, TrivialDependency (f blk)) => CodecConfig blk -> SomeSecond f blk -> Encoding

-- | A <a>Serialised</a> header along with context identifying what kind of
--   header it is.
--   
--   The <tt>SerialiseNodeToNodeDep</tt> for <a>Header</a> will decide how
--   to actually encode this.
newtype SerialisedHeader blk
SerialisedHeaderFromDepPair :: GenDepPair Serialised (NestedCtxt Header blk) -> SerialisedHeader blk
[serialisedHeaderToDepPair] :: SerialisedHeader blk -> GenDepPair Serialised (NestedCtxt Header blk)
castSerialisedHeader :: (forall a. NestedCtxt_ blk Header a -> NestedCtxt_ blk' Header a) -> SerialisedHeader blk -> SerialisedHeader blk'

-- | Inverse to <a>encodeTrivialSerialisedHeader</a>
decodeTrivialSerialisedHeader :: forall blk. TrivialDependency (NestedCtxt_ blk Header) => forall s. Decoder s (SerialisedHeader blk)

-- | Encode the header without the <a>NestedCtxt</a>
--   
--   Uses CBOR-in-CBOR
encodeTrivialSerialisedHeader :: forall blk. TrivialDependency (NestedCtxt_ blk Header) => SerialisedHeader blk -> Encoding
serialisedHeaderFromPair :: (SomeSecond (NestedCtxt Header) blk, ByteString) -> SerialisedHeader blk
serialisedHeaderToPair :: SerialisedHeader blk -> (SomeSecond (NestedCtxt Header) blk, ByteString)

-- | Number of bytes from the start of a block needed to reconstruct the
--   nested context.
--   
--   See <tt>reconstructPrefixLen</tt>.
newtype PrefixLen
PrefixLen :: Word8 -> PrefixLen
[getPrefixLen] :: PrefixLen -> Word8
class HasNestedContent f blk => ReconstructNestedCtxt f blk

-- | Number of bytes required to reconstruct the nested context.
--   
--   This will be the <i>minimum</i> length of the <a>ShortByteString</a>
--   passed to <a>reconstructNestedCtxt</a>.
reconstructPrefixLen :: ReconstructNestedCtxt f blk => proxy (f blk) -> PrefixLen

-- | Reconstruct the type of nested contents
--   
--   TODO: Allow to fail.
reconstructNestedCtxt :: ReconstructNestedCtxt f blk => proxy (f blk) -> ShortByteString -> SizeInBytes -> SomeSecond (NestedCtxt f) blk

-- | Number of bytes required to reconstruct the nested context.
--   
--   This will be the <i>minimum</i> length of the <a>ShortByteString</a>
--   passed to <a>reconstructNestedCtxt</a>.
reconstructPrefixLen :: (ReconstructNestedCtxt f blk, TrivialDependency (NestedCtxt_ blk f)) => proxy (f blk) -> PrefixLen

-- | Reconstruct the type of nested contents
--   
--   TODO: Allow to fail.
reconstructNestedCtxt :: (ReconstructNestedCtxt f blk, TrivialDependency (NestedCtxt_ blk f)) => proxy (f blk) -> ShortByteString -> SizeInBytes -> SomeSecond (NestedCtxt f) blk
addPrefixLen :: Word8 -> PrefixLen -> PrefixLen
takePrefix :: PrefixLen -> ByteString -> ShortByteString

-- | Information about the serialised block.
data BinaryBlockInfo
BinaryBlockInfo :: !Word16 -> !Word16 -> BinaryBlockInfo

-- | The offset within the serialised block at which the header starts.
[headerOffset] :: BinaryBlockInfo -> !Word16

-- | How many bytes the header is long. Extracting the <a>headerSize</a>
--   bytes from serialised block starting from <a>headerOffset</a> should
--   yield the header. Before passing the extracted bytes to the decoder
--   for headers, an envelope can be around using
--   <tt>nodeAddHeaderEnvelope</tt>.
[headerSize] :: BinaryBlockInfo -> !Word16
class HasBinaryBlockInfo blk

-- | Return information about the serialised block, i.e., how to extract
--   the bytes corresponding to the header from the serialised block.
getBinaryBlockInfo :: HasBinaryBlockInfo blk => blk -> BinaryBlockInfo
data () => SizeInBytes
decodeDepPair :: DecodeDiskDep f blk => CodecConfig blk -> GenDepPair Serialised (f blk) -> Decoder s (DepPair (f blk))
encodeDepPair :: EncodeDiskDep f blk => CodecConfig blk -> DepPair (f blk) -> GenDepPair Serialised (f blk)
instance Ouroboros.Consensus.Block.NestedContent.HasNestedContent Ouroboros.Consensus.Block.Abstract.Header blk => GHC.Show.Show (Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader blk)
instance Ouroboros.Network.Util.ShowProxy.ShowProxy blk => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader blk)
instance Ouroboros.Network.Block.StandardHash blk => Ouroboros.Network.Block.StandardHash (Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader blk)
instance Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDepIx (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) blk => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk blk (Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader blk)
instance Ouroboros.Consensus.Storage.Serialisation.DecodeDiskDepIx (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) blk => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk (Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader blk)
instance (Ouroboros.Consensus.Storage.Serialisation.DecodeDiskDepIx f blk, Ouroboros.Consensus.Storage.Serialisation.DecodeDiskDep f blk) => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk (Ouroboros.Consensus.Util.DepPair.DepPair (f blk))
instance Ouroboros.Consensus.Storage.Serialisation.DecodeDiskDepIx f blk => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk (Ouroboros.Consensus.Util.DepPair.GenDepPair Ouroboros.Network.Block.Serialised (f blk))
instance (Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDepIx f blk, Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDep f blk) => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk blk (Ouroboros.Consensus.Util.DepPair.DepPair (f blk))
instance Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDepIx f blk => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk blk (Ouroboros.Consensus.Util.DepPair.GenDepPair Ouroboros.Network.Block.Serialised (f blk))
instance Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk (Ouroboros.Consensus.Protocol.Abstract.ChainDepState (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk (Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState blk)
instance Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk blk => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk (Data.SOP.BasicFunctors.I blk)
instance Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk (a -> f blk) => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk blk ((Data.SOP.BasicFunctors.:.:) ((->) a) f blk)
instance Ouroboros.Consensus.Storage.Serialisation.EncodeDisk blk (Ouroboros.Consensus.Protocol.Abstract.ChainDepState (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk blk (Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState blk)
instance Ouroboros.Consensus.Storage.Serialisation.EncodeDisk blk blk => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk blk (Data.SOP.BasicFunctors.I blk)

module Ouroboros.Consensus.Storage.VolatileDB.Impl.Parser

-- | Note that we recover from the error, and thus never throw it as an
--   <tt>Exception</tt>.
--   
--   Defined here instead of in the <tt>Parser</tt> module because
--   <a>TraceEvent</a> depends on it.
data ParseError blk

-- | A block could not be parsed.
BlockReadErr :: ReadIncrementalErr -> ParseError blk

-- | A block was corrupted, e.g., checking its signature and/or hash
--   failed.
BlockCorruptedErr :: HeaderHash blk -> ParseError blk

-- | A block with the same hash occurred twice in the VolatileDB files.
--   
--   We include the file in which it occurred first and the file in which
--   it occured the second time. The two files can be the same.
DuplicatedBlock :: HeaderHash blk -> FsPath -> FsPath -> ParseError blk

-- | Information returned by the parser about a single block.
--   
--   The parser returns for each block, its offset, its size and its
--   <a>BlockInfo</a>
--   
--   The fields of this record are strict to make sure that by evaluating
--   this record to WHNF, we no longer hold on to the entire block.
--   Otherwise, we might accidentally keep all blocks in a single file in
--   memory during parsing.
data ParsedBlockInfo blk
ParsedBlockInfo :: !BlockOffset -> !BlockSize -> !BlockInfo blk -> !SomeSecond (NestedCtxt Header) blk -> ParsedBlockInfo blk
[pbiBlockOffset] :: ParsedBlockInfo blk -> !BlockOffset
[pbiBlockSize] :: ParsedBlockInfo blk -> !BlockSize
[pbiBlockInfo] :: ParsedBlockInfo blk -> !BlockInfo blk
[pbiNestedCtxt] :: ParsedBlockInfo blk -> !SomeSecond (NestedCtxt Header) blk

-- | Parse the given file containing blocks.
--   
--   Return the <a>ParsedBlockInfo</a> for all the valid blocks in the
--   file. Stop when encountering an error and include the offset to
--   truncate to.
parseBlockFile :: forall m blk h. (IOLike m, GetPrevHash blk, HasBinaryBlockInfo blk, HasNestedContent Header blk, DecodeDisk blk (ByteString -> blk)) => CodecConfig blk -> HasFS m h -> (blk -> Bool) -> BlockValidationPolicy -> FsPath -> m ([ParsedBlockInfo blk], Maybe (ParseError blk, BlockOffset))
extractBlockInfo :: (GetPrevHash blk, HasBinaryBlockInfo blk) => blk -> BlockInfo blk


-- | Information about the files stored by the volatile DB
--   
--   Intended for qualified import.
module Ouroboros.Consensus.Storage.VolatileDB.Impl.FileInfo

-- | The internal information the VolatileDB keeps for each file.
data FileInfo blk

-- | Adds a block to a <a>FileInfo</a>.
addBlock :: StandardHash blk => SlotNo -> HeaderHash blk -> FileInfo blk -> FileInfo blk
empty :: FileInfo blk

-- | Construct a <a>FileInfo</a> from the parser result.
fromParsedBlockInfos :: forall blk. StandardHash blk => [ParsedBlockInfo blk] -> FileInfo blk

-- | Checks if this file can be GCed.
canGC :: FileInfo blk -> SlotNo -> Bool
hashes :: FileInfo blk -> Set (HeaderHash blk)

-- | Has this file reached its maximum size?
isFull :: BlocksPerFile -> FileInfo blk -> Bool
maxSlotNo :: FileInfo blk -> MaxSlotNo
maxSlotNoInFiles :: [FileInfo blk] -> MaxSlotNo
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.VolatileDB.Impl.FileInfo.FileInfo blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.VolatileDB.Impl.FileInfo.FileInfo blk)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.VolatileDB.Impl.FileInfo.FileInfo blk)


-- | VolatileDB Index
--   
--   Intended for qualified import &gt; import qualified
--   Ouroboros.Consensus.Storage.VolatileDB.Impl.Index as Index
module Ouroboros.Consensus.Storage.VolatileDB.Impl.Index
delete :: FileId -> Index blk -> Index blk
elems :: Index blk -> [FileInfo blk]
empty :: Index blk
insert :: FileId -> FileInfo blk -> Index blk -> Index blk

-- | Return the last, i.e. the <i>highest</i>, <a>FileId</a> and
--   corresponding <a>FileInfo</a> stored in the <a>Index</a>. Return
--   <a>Nothing</a> when empty.
lastFile :: Index blk -> Maybe (FileId, FileInfo blk)
lookup :: FileId -> Index blk -> Maybe (FileInfo blk)
toAscList :: Index blk -> [(FileId, FileInfo blk)]

-- | Mapping from <a>FileId</a> to <a>FileInfo</a>
data Index blk
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.VolatileDB.Impl.Index.Index blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.VolatileDB.Impl.Index.Index blk)


-- | Serialisation for sending things across the network.
--   
--   We separate <tt>NodeToNode</tt> from <tt>NodeToClient</tt> to be very
--   explicit about what gets sent where.
--   
--   Unlike in <a>Ouroboros.Consensus.Storage.Serialisation</a>, we don't
--   separate the encoder from the decoder, because the reasons don't
--   apply: we always need both directions and we don't have access to the
--   bytestrings that could be used for the annotations (we use
--   CBOR-in-CBOR in those cases).
module Ouroboros.Consensus.Node.Serialisation

-- | Serialise a type <tt>a</tt> so that it can be sent across the network
--   via node-to-client protocol.
class SerialiseNodeToClient blk a
encodeNodeToClient :: SerialiseNodeToClient blk a => CodecConfig blk -> BlockNodeToClientVersion blk -> a -> Encoding
decodeNodeToClient :: SerialiseNodeToClient blk a => CodecConfig blk -> BlockNodeToClientVersion blk -> forall s. Decoder s a
encodeNodeToClient :: (SerialiseNodeToClient blk a, Serialise a) => CodecConfig blk -> BlockNodeToClientVersion blk -> a -> Encoding
decodeNodeToClient :: (SerialiseNodeToClient blk a, Serialise a) => CodecConfig blk -> BlockNodeToClientVersion blk -> forall s. Decoder s a

-- | Serialise a type <tt>a</tt> so that it can be sent across network via
--   a node-to-node protocol.
class SerialiseNodeToNode blk a
encodeNodeToNode :: SerialiseNodeToNode blk a => CodecConfig blk -> BlockNodeToNodeVersion blk -> a -> Encoding
decodeNodeToNode :: SerialiseNodeToNode blk a => CodecConfig blk -> BlockNodeToNodeVersion blk -> forall s. Decoder s a
encodeNodeToNode :: (SerialiseNodeToNode blk a, Serialise a) => CodecConfig blk -> BlockNodeToNodeVersion blk -> a -> Encoding
decodeNodeToNode :: (SerialiseNodeToNode blk a, Serialise a) => CodecConfig blk -> BlockNodeToNodeVersion blk -> forall s. Decoder s a

-- | How to serialise the result of the <tt>result</tt> of a query.
--   
--   The <tt>LocalStateQuery</tt> protocol is a node-to-client protocol,
--   hence the <a>NodeToClientVersion</a> argument.
class SerialiseResult blk query
encodeResult :: forall result. SerialiseResult blk query => CodecConfig blk -> BlockNodeToClientVersion blk -> query result -> result -> Encoding
decodeResult :: forall result. SerialiseResult blk query => CodecConfig blk -> BlockNodeToClientVersion blk -> query result -> forall s. Decoder s result

-- | Inverse of <a>defaultEncodeCBORinCBOR</a>
defaultDecodeCBORinCBOR :: Serialise a => Decoder s a

-- | Uses the <a>Serialise</a> instance, but wraps it in CBOR-in-CBOR.
--   
--   Use this for the <a>SerialiseNodeToNode</a> and/or
--   <a>SerialiseNodeToClient</a> instance of <tt>blk</tt> and/or
--   <tt><a>Header</a> blk</tt>, which require CBOR-in-CBOR to be
--   compatible with the corresponding <tt>Serialised</tt> instance.
defaultEncodeCBORinCBOR :: Serialise a => a -> Encoding
data () => Some (f :: k -> Type)
[Some] :: forall {k} (f :: k -> Type) (a :: k). f a -> Some f
instance Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient blk blk => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient blk (Data.SOP.BasicFunctors.I blk)
instance Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient blk (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient blk (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)
instance Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient blk (Ouroboros.Consensus.Ledger.SupportsMempool.ApplyTxErr blk) => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient blk (Ouroboros.Consensus.TypeFamilyWrappers.WrapApplyTxErr blk)
instance Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode blk blk => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode blk (Data.SOP.BasicFunctors.I blk)
instance Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode blk (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode blk (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)

module Ouroboros.Consensus.HardFork.History.Util
addEpochs :: Word64 -> EpochNo -> EpochNo
addSlots :: Word64 -> SlotNo -> SlotNo

-- | <tt>countEpochs to fr</tt> counts the epochs from <tt>fr</tt> to
--   <tt>to</tt> (<tt>to &gt;= fr</tt>)
countEpochs :: HasCallStack => EpochNo -> EpochNo -> Word64

-- | <tt>countSlots to fr</tt> counts the slots from <tt>fr</tt> to
--   <tt>to</tt> (<tt>to &gt;= fr</tt>)
countSlots :: HasCallStack => SlotNo -> SlotNo -> Word64
subSlots :: Word64 -> SlotNo -> SlotNo

module Ouroboros.Consensus.HardFork.History.Summary

-- | Detailed information about the time bounds of an era
data Bound
Bound :: !RelativeTime -> !SlotNo -> !EpochNo -> Bound
[boundTime] :: Bound -> !RelativeTime
[boundSlot] :: Bound -> !SlotNo
[boundEpoch] :: Bound -> !EpochNo
initBound :: Bound

-- | Compute upper bound given just the epoch number and era parameters
mkUpperBound :: HasCallStack => EraParams -> Bound -> EpochNo -> Bound
slotToEpochBound :: EraParams -> Bound -> SlotNo -> EpochNo

-- | Exclusive upper bound on the era
data EraEnd

-- | Bounded era
EraEnd :: !Bound -> EraEnd

-- | Unbounded era
--   
--   This arises from the use of <a>UnsafeIndefiniteSafeZone</a>.
EraUnbounded :: EraEnd

-- | Information about a specific era
--   
--   The <a>eraEnd</a> of the final era in the summary will be determined
--   by the safe zone considerations discussed above.
--   
--   Let the start of the summary be <tt>(t, s, e)</tt> (time, slot epoch),
--   and the end of the summary be <tt>(t', s', e')</tt>. We have one
--   invariant relating epochs and slots:
--   
--   <pre>
--   INV-1a  e' == e + ((s' - s) / epochSize)
--   INV-1b: s' == s + ((e' - e) * epochSize)
--   </pre>
--   
--   And another invariant relating time and slots:
--   
--   <pre>
--   INV-2a: s' == s + ((t' - t) / slotLen)
--   INV-2b: t' == t + ((s' - s) * slotLen)
--   </pre>
--   
--   Note that these aren't really two sets of independent invariants.
--   <tt>INV-1a</tt> follows from <tt>INV-1b</tt>:
--   
--   <pre>
--         s'                   == s + ((e' - e) * epochSize)
--         s' - s               ==     ((e' - e) * epochSize)
--        (s' - s) / epochSize  ==       e' - e
--   e + ((s' - s) / epochSize) ==       e'
--   </pre>
--   
--   Similarly, <tt>INV-2a</tt> follows from <tt>INV-2b</tt>:
--   
--   <pre>
--         t'                 == t + ((s' - s) * slotLen)
--         t' - t             ==     ((s' - s) * slotLen)
--        (t' - t) / slotLen  ==       s' - s
--   s + ((t' - t) / slotLen) ==       s'
--   </pre>
data EraSummary
EraSummary :: !Bound -> !EraEnd -> !EraParams -> EraSummary

-- | Inclusive lower bound
[eraStart] :: EraSummary -> !Bound

-- | Exclusive upper bound
[eraEnd] :: EraSummary -> !EraEnd

-- | Active parameters
[eraParams] :: EraSummary -> !EraParams

-- | Version of <a>mkUpperBound</a> when the upper bound may not be known
--   
--   If passed <a>Nothing</a>, assumes <a>EraUnbounded</a>. This is
--   <i>NOT</i> suitable for eras where the transition is simply unknown.
mkEraEnd :: EraParams -> Bound -> Maybe EpochNo -> EraEnd

-- | Summary of the <i>confirmed</i> part of the ledger
--   
--   The summary zips <a>Shape</a> with <tt>Forks</tt>, and provides
--   detailed information about the start and end of each era.
--   
--   We have at most one summary for each era, and at least one
newtype Summary xs
Summary :: NonEmpty xs EraSummary -> Summary xs
[getSummary] :: Summary xs -> NonEmpty xs EraSummary

-- | <a>Summary</a> for a ledger that never forks
neverForksSummary :: EpochSize -> SlotLength -> Summary '[x]

-- | Construct <a>Summary</a> with an exact number of <a>EraSummary</a>
--   
--   Primarily useful for tests.
summaryWithExactly :: Exactly (x ': xs) EraSummary -> Summary (x ': xs)

-- | The shape of the chain (old to new)
--   
--   The shape determines how many hard forks we expect as well as the
--   parameters for each era. The type argument is a type-level list
--   containing one entry per era, emphasizing that this information is
--   statically known.
--   
--   The entry indices themselves are not used here, but the idea is that
--   they look something like <tt>'[ByronBlock, ShelleyBlock,
--   GoguenBlock]</tt> and do affect the hard fork combinator. So far this
--   is a list of block types, since most of consensus is indexed by block
--   types.
newtype Shape xs
Shape :: Exactly xs EraParams -> Shape xs
[getShape] :: Shape xs -> Exactly xs EraParams

-- | The exact point of each confirmed hard fork transition (old to new)
--   
--   Unlike the <a>Shape</a> of the chain, which is statically known, the
--   <a>Transitions</a> are derived from the state of the ledger (hard fork
--   transition points only become known after a voting procedure).
--   
--   Any transition listed here must be "certain". How certainty is
--   established is ledger dependent, but it should imply that this is no
--   longer subject to rollback.
data Transitions :: [Type] -> Type

-- | If the indices are, say, <tt>'[Byron, Shelley, Goguen]</tt>, then we
--   can have have at most two transitions: one to Shelley, and one to
--   Goguen. There cannot be a transition <i>to</i> the initial ledger.
[Transitions] :: AtMost xs EpochNo -> Transitions (x ': xs)

-- | Check <a>Shape</a> invariants
--   
--   The only part of the <a>Shape</a> that must make sense is the
--   <tt>safeBeforeEpoch</tt> values (they must be strictly increasing).
--   
--   NOTE: We assume eras cannot be empty. This will be satisfied by any
--   ledger we are interested in since transitions must be voted on (safe
--   zones will be non-empty).
invariantShape :: Shape xs -> Except String ()

-- | Check <a>Summary</a> invariants
invariantSummary :: Summary xs -> Except String ()

-- | There is only one era
singletonShape :: EraParams -> Shape '[x]

-- | Construct hard fork <a>Summary</a>
--   
--   NOTE (on epoch to slot translation). In order to translate
--   <a>SlotNo</a> to <a>EpochNo</a>, we simply "line up" all slots. For
--   example, suppose we have an initial <a>EpochSize</a> of 10, and then
--   an <a>EpochSize</a> of 20 from <a>EpochNo</a> 3 onwards. We end up
--   with something like
--   
--   <pre>
--   Epoch | 0      | 1        | 2        | 3        | 4        | ..
--   Slot  | 0 .. 9 | 10 .. 19 | 20 .. 29 | 30 .. 49 | 50 .. 69 | ..
--   </pre>
--   
--   We do this translation <i>independent</i> from the
--   <tt>minimumPossibleSlotNo</tt> for a particular ledger. This means
--   that for ledgers where the <tt>minimumPossibleSlotNo</tt> is not zero
--   (e.g., some ledgers might set it to 1), the maximum number of blocks
--   (aka filled slots) in an epoch is just 1 (or more) less than the other
--   epochs.
summarize :: WithOrigin SlotNo -> Shape xs -> Transitions xs -> Summary xs

-- | No known transitions yet
transitionsUnknown :: Transitions (x ': xs)

-- | Outer bounds of the summary
summaryBounds :: Summary xs -> (Bound, EraEnd)

-- | Analogue of <a>init</a> for <a>Summary</a> (i.e., split off the final
--   era)
--   
--   This is primarily useful for tests.
summaryInit :: Summary xs -> (Maybe (Summary xs), EraSummary)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.History.Summary.Bound
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Summary.Bound
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.History.Summary.Bound
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Summary.Bound
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.History.Summary.EraEnd
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Summary.EraEnd
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.History.Summary.EraEnd
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Summary.EraEnd
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.History.Summary.EraSummary
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Summary.EraSummary
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.History.Summary.EraSummary
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Summary.EraSummary
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.History.Summary.Summary xs)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.History.Summary.Summary xs)
instance GHC.Classes.Eq (Ouroboros.Consensus.HardFork.History.Summary.Summary xs)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.History.Summary.Shape xs)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.History.Summary.Shape xs)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.History.Summary.Transitions xs)
instance Data.SOP.Constraint.SListI xs => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.History.Summary.Summary xs)
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.HardFork.History.Summary.EraSummary
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.HardFork.History.Summary.EraEnd
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.HardFork.History.Summary.Bound

module Ouroboros.Consensus.HardFork.History.Qry

-- | Query expressions in PHOAS
data Expr (f :: Type -> Type) :: Type -> Type
[EVar] :: f a -> Expr f a
[ELit] :: Show a => a -> Expr f a
[ELet] :: Expr f a -> (f a -> Expr f b) -> Expr f b
[EPair] :: Expr f a -> Expr f b -> Expr f (a, b)
[EFst] :: Expr f (a, b) -> Expr f a
[ESnd] :: Expr f (a, b) -> Expr f b
[EAbsToRelTime] :: Expr f RelativeTime -> Expr f TimeInEra
[EAbsToRelSlot] :: Expr f SlotNo -> Expr f SlotInEra
[EAbsToRelEpoch] :: Expr f EpochNo -> Expr f EpochInEra
[ERelToAbsTime] :: Expr f TimeInEra -> Expr f RelativeTime
[ERelToAbsSlot] :: Expr f (SlotInEra, TimeInSlot) -> Expr f SlotNo
[ERelToAbsEpoch] :: Expr f (EpochInEra, SlotInEpoch) -> Expr f EpochNo
[ERelTimeToSlot] :: Expr f TimeInEra -> Expr f (SlotInEra, TimeInSlot)
[ERelSlotToTime] :: Expr f SlotInEra -> Expr f TimeInEra
[ERelSlotToEpoch] :: Expr f SlotInEra -> Expr f (EpochInEra, SlotInEpoch)
[ERelEpochToSlot] :: Expr f EpochInEra -> Expr f SlotInEra
[ESlotLength] :: Expr f SlotNo -> Expr f SlotLength
[EEpochSize] :: Expr f EpochNo -> Expr f EpochSize

-- | We tried to convert something that is past the horizon
--   
--   That is, we tried to convert something that is past the point in time
--   beyond which we lack information due to uncertainty about the next
--   hard fork.
data PastHorizonException
PastHorizon :: CallStack -> Some ClosedExpr -> [EraSummary] -> PastHorizonException

-- | Callstack to the call to <a>runQuery</a>
[pastHorizonCallStack] :: PastHorizonException -> CallStack

-- | The <a>Expr</a> we tried to evaluate
[pastHorizonExpression] :: PastHorizonException -> Some ClosedExpr

-- | The <a>EraSummary</a>s that we tried to evaluate the <a>Expr</a>
--   against
[pastHorizonSummary] :: PastHorizonException -> [EraSummary]

-- | Construct a <a>Qry</a> from a closed <a>Expr</a>
qryFromExpr :: (forall f. Expr f a) -> Qry a

-- | Run a query
--   
--   Unlike an <a>Expr</a>, which is evaluated in a single era, a
--   <a>Qry</a> is evaluated against <i>all</i> eras. Only if all
--   <a>Expr</a>s embedded in the <a>Qry</a> can be evaluated in the
--   <i>same</i> era (we don't want to mix properties of different eras in
--   one query) do we return the result. If there is no era in which we can
--   evaluate all <a>Expr</a>s in the <a>Qry</a>, we report a
--   <a>PastHorizonException</a>.
--   
--   NOTE: this means that queries about separate eras have to be run
--   separately, they should not be composed into a single query. How could
--   we know to which era which relative slot/time refers?
runQuery :: forall a xs. HasCallStack => Qry a -> Summary xs -> Either PastHorizonException a
runQueryPure :: HasCallStack => Qry a -> Summary xs -> a
runQueryThrow :: (HasCallStack, MonadThrow m) => Qry a -> Summary xs -> m a

-- | Query
--   
--   <a>Qry</a> adds a monadic interface on top of <a>Expr</a>. Although
--   means that <a>Qry</a> itself is not showable, the
--   <a>PastHorizonException</a> can nonetheless show the offending
--   expression alongside the <a>Summary</a> against which it was
--   evaluated.
data Qry :: Type -> Type
interpretQuery :: HasCallStack => Interpreter xs -> Qry a -> Either PastHorizonException a
mkInterpreter :: Summary xs -> Interpreter xs

-- | UNSAFE: extend the safe zone of the current era of the given
--   <a>Interpreter</a> to be <i>unbounded</i>, ignoring any future hard
--   forks.
--   
--   This only has effect when the <a>Interpreter</a> was obtained in an
--   era that was <i>not the final one</i> (in the final era, this is a
--   no-op). The <a>Interpreter</a> will be made to believe that the
--   current era is the final era, making its horizon unbounded, and thus
--   never returning a <a>PastHorizonException</a>.
--   
--   Use of this function is <i>strongly discouraged</i>, as it will ignore
--   any future hard forks, and the results produced by the
--   <a>Interpreter</a> can thus be incorrect.
unsafeExtendSafeZone :: Interpreter xs -> Interpreter xs

-- | Can be sent across the LocalStateQuery protocol to interpret queries
--   in the wallet.
--   
--   The <a>Summary</a> should be considered internal.
data Interpreter xs
epochToSize :: EpochNo -> Qry EpochSize

-- | Translate <a>EpochNo</a> to the <a>SlotNo</a> of the first slot in
--   that epoch
--   
--   Additionally returns the size of the epoch.
epochToSlot :: EpochNo -> Qry (SlotNo, EpochSize)
epochToSlot' :: EpochNo -> Qry SlotNo

-- | Translate <a>SlotNo</a> to its corresponding <a>EpochNo</a>
--   
--   Additionally returns the relative slot within this epoch and how many
--   slots are left in this slot.
slotToEpoch :: SlotNo -> Qry (EpochNo, Word64, Word64)

-- | Convert <a>SlotNo</a> to <a>EpochNo</a> and the relative slot within
--   the epoch
slotToEpoch' :: SlotNo -> Qry (EpochNo, Word64)

-- | Acquire a slot's length
slotToSlotLength :: SlotNo -> Qry SlotLength

-- | Translate <a>SlotNo</a> to the <tt>UTCTime</tt> at the start of that
--   slot
--   
--   Additionally returns the length of the slot.
slotToWallclock :: SlotNo -> Qry (RelativeTime, SlotLength)

-- | Translate <tt>UTCTime</tt> to <a>SlotNo</a>
--   
--   Additionally returns the time spent and time left in this slot.
wallclockToSlot :: RelativeTime -> Qry (SlotNo, NominalDiffTime, NominalDiffTime)
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Qry.TimeInEra
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Qry.TimeInSlot
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Qry.SlotInEra
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Qry.SlotInEpoch
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.History.Qry.EpochInEra
instance GHC.Classes.Eq (Ouroboros.Consensus.HardFork.History.Qry.Interpreter xs)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.History.Qry.Var a)
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Qry.TimeInEra
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Qry.TimeInSlot
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Qry.SlotInEra
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Qry.SlotInEpoch
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Qry.EpochInEra
instance GHC.Show.Show Ouroboros.Consensus.HardFork.History.Qry.PastHorizonException
instance Data.SOP.Constraint.SListI xs => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.History.Qry.Interpreter xs)
instance GHC.Show.Show (Ouroboros.Consensus.Util.Some Ouroboros.Consensus.HardFork.History.Qry.ClosedExpr)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.History.Qry.ClosedExpr a)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.History.Qry.Interpreter xs)
instance GHC.Exception.Type.Exception Ouroboros.Consensus.HardFork.History.Qry.PastHorizonException
instance GHC.Base.Functor Ouroboros.Consensus.HardFork.History.Qry.Qry
instance GHC.Base.Applicative Ouroboros.Consensus.HardFork.History.Qry.Qry
instance GHC.Base.Monad Ouroboros.Consensus.HardFork.History.Qry.Qry


-- | Derive <a>EpochInfo</a>
module Ouroboros.Consensus.HardFork.History.EpochInfo

-- | A dummy <a>EpochInfo</a> that always throws an <a>error</a>.
--   
--   To be used as a placeholder before a summary is available.
dummyEpochInfo :: EpochInfo (Except PastHorizonException)

-- | Construct an <a>EpochInfo</a> for a <i>snapshot</i> of the ledger
--   state
interpreterToEpochInfo :: forall xs. Interpreter xs -> EpochInfo (Except PastHorizonException)

-- | Construct an <a>EpochInfo</a> for a <i>snapshot</i> of the ledger
--   state
summaryToEpochInfo :: forall xs. Summary xs -> EpochInfo (Except PastHorizonException)

-- | Interpret the <a>PastHorizonException</a> as a _pure exception_ via
--   <a>throw</a>
--   
--   As per usual, this should only be used when the pure exception would
--   indicate a bug.
toPureEpochInfo :: EpochInfo (Except PastHorizonException) -> EpochInfo Identity

module Ouroboros.Consensus.HardFork.History.Caching

-- | Stateful abstraction to execute queries
data RunWithCachedSummary (xs :: [Type]) m
RunWithCachedSummary :: (forall a. Qry a -> STM m (Either PastHorizonException a)) -> RunWithCachedSummary (xs :: [Type]) m

-- | Run the specified query
--   
--   If the query fails with a <a>PastHorizonException</a>, it will update
--   its internal state (compute a new summary) and try again. If that
--   <i>still</i> fails, the <a>PastHorizonException</a> is returned.
[cachedRunQuery] :: RunWithCachedSummary (xs :: [Type]) m -> forall a. Qry a -> STM m (Either PastHorizonException a)

-- | Construct <a>RunWithCachedSummary</a> given action that computes the
--   summary
--   
--   Most use cases will probably construct this action from an action that
--   reads the ledger state and then computes the summary from that.
runWithCachedSummary :: forall m xs. MonadSTM m => STM m (Summary xs) -> m (RunWithCachedSummary xs m)

module Ouroboros.Consensus.HardFork.Combinator.PartialConfig

-- | Partial consensus config
class (ConsensusProtocol p, NoThunks (PartialConsensusConfig p)) => HasPartialConsensusConfig p where {
    type PartialConsensusConfig p :: Type;
    type PartialConsensusConfig p = ConsensusConfig p;
}

-- | Construct <a>ConsensusConfig</a> from <a>PartialConsensusConfig</a>
--   
--   See comments for <a>completeLedgerConfig</a> for some details about
--   the <a>EpochInfo</a>.
completeConsensusConfig :: HasPartialConsensusConfig p => proxy p -> EpochInfo (Except PastHorizonException) -> PartialConsensusConfig p -> ConsensusConfig p

-- | Construct <a>ConsensusConfig</a> from <a>PartialConsensusConfig</a>
--   
--   See comments for <a>completeLedgerConfig</a> for some details about
--   the <a>EpochInfo</a>.
completeConsensusConfig :: (HasPartialConsensusConfig p, PartialConsensusConfig p ~ ConsensusConfig p) => proxy p -> EpochInfo (Except PastHorizonException) -> PartialConsensusConfig p -> ConsensusConfig p

-- | Construct partial consensus config from full consensus config
--   
--   NOTE: This is basically just losing <a>EpochInfo</a>, but that is
--   constant anyway when we are dealing with a single era.
toPartialConsensusConfig :: HasPartialConsensusConfig p => proxy p -> ConsensusConfig p -> PartialConsensusConfig p

-- | Construct partial consensus config from full consensus config
--   
--   NOTE: This is basically just losing <a>EpochInfo</a>, but that is
--   constant anyway when we are dealing with a single era.
toPartialConsensusConfig :: (HasPartialConsensusConfig p, PartialConsensusConfig p ~ ConsensusConfig p) => proxy p -> ConsensusConfig p -> PartialConsensusConfig p

-- | Partial ledger config
class (UpdateLedger blk, NoThunks (PartialLedgerConfig blk)) => HasPartialLedgerConfig blk where {
    type PartialLedgerConfig blk :: Type;
    type PartialLedgerConfig blk = LedgerConfig blk;
}

-- | Construct <a>LedgerConfig</a> from <tt>PartialLedgerCfg</tt>
--   
--   NOTE: The <a>EpochInfo</a> provided will have limited range, any
--   attempt to look past its horizon will result in a pure
--   <a>PastHorizonException</a>. The horizon is determined by the tip of
--   the ledger <i>state</i> (not view) from which the <a>EpochInfo</a> is
--   derived.
completeLedgerConfig :: HasPartialLedgerConfig blk => proxy blk -> EpochInfo (Except PastHorizonException) -> PartialLedgerConfig blk -> LedgerConfig blk

-- | Construct <a>LedgerConfig</a> from <tt>PartialLedgerCfg</tt>
--   
--   NOTE: The <a>EpochInfo</a> provided will have limited range, any
--   attempt to look past its horizon will result in a pure
--   <a>PastHorizonException</a>. The horizon is determined by the tip of
--   the ledger <i>state</i> (not view) from which the <a>EpochInfo</a> is
--   derived.
completeLedgerConfig :: (HasPartialLedgerConfig blk, PartialLedgerConfig blk ~ LedgerConfig blk) => proxy blk -> EpochInfo (Except PastHorizonException) -> PartialLedgerConfig blk -> LedgerConfig blk
newtype WrapPartialConsensusConfig blk
WrapPartialConsensusConfig :: PartialConsensusConfig (BlockProtocol blk) -> WrapPartialConsensusConfig blk
[unwrapPartialConsensusConfig] :: WrapPartialConsensusConfig blk -> PartialConsensusConfig (BlockProtocol blk)
newtype WrapPartialLedgerConfig blk
WrapPartialLedgerConfig :: PartialLedgerConfig blk -> WrapPartialLedgerConfig blk
[unwrapPartialLedgerConfig] :: WrapPartialLedgerConfig blk -> PartialLedgerConfig blk
data () => EpochInfo (m :: Type -> Type)
EpochInfo :: (HasCallStack => EpochNo -> m EpochSize) -> (HasCallStack => EpochNo -> m SlotNo) -> (HasCallStack => SlotNo -> m EpochNo) -> (HasCallStack => SlotNo -> m RelativeTime) -> (HasCallStack => SlotNo -> m SlotLength) -> EpochInfo (m :: Type -> Type)
[epochInfoSize_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => EpochNo -> m EpochSize
[epochInfoFirst_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => EpochNo -> m SlotNo
[epochInfoEpoch_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => SlotNo -> m EpochNo
[epochInfoSlotToRelativeTime_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => SlotNo -> m RelativeTime
[epochInfoSlotLength_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => SlotNo -> m SlotLength

-- | The parameterizable exception monad.
--   
--   Computations are either exceptions or normal values.
--   
--   The <a>return</a> function returns a normal value, while
--   <tt>&gt;&gt;=</tt> exits on the first exception. For a variant that
--   continues after an error and collects all the errors, see
--   <a>Errors</a>.
type Except e = ExceptT e Identity

-- | We tried to convert something that is past the horizon
--   
--   That is, we tried to convert something that is past the point in time
--   beyond which we lack information due to uncertainty about the next
--   hard fork.
data PastHorizonException
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.PartialConfig.PartialLedgerConfig blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.PartialConfig.WrapPartialLedgerConfig blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.PartialConfig.PartialConsensusConfig (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.PartialConfig.WrapPartialConsensusConfig blk)


-- | Intended for qualified import
--   
--   <pre>
--   import qualified Ouroboros.Consensus.HardFork.History as History
--   </pre>
module Ouroboros.Consensus.HardFork.History

module Ouroboros.Consensus.HardFork.Abstract
class HasHardForkHistory blk where {
    
    -- | Type level description of the hard fork shape
    --   
    --   The <tt>Summary</tt> infrastructure does not care what the types in
    --   this list are, it just cares how <i>many</i> eras there are. The hard
    --   fork combinator will instantiate <a>HardForkIndices</a> to the types
    --   of the blocks involved in the hard fork, e.g., we might have something
    --   like
    --   
    --   <pre>
    --   '[ByronBlock, ShelleyBlock, GoguenBlock]
    --   </pre>
    type HardForkIndices blk :: [Type];
}

-- | Summary of the hard fork state
--   
--   NOTE: <a>HasHardForkHistory</a> is the only abstraction that the
--   consensus layer is aware in relation to potential hard forks, and is
--   needed only for time translations (in block production and in the
--   chain DB). It is independent from the hard fork combinator and can be
--   used for blocks that never fork (in which case the <tt>Summary</tt>
--   will be trivial) or indeed for blocks that do support transitions but
--   do not use the hard fork combinator.
--   
--   It is however useful to consider what this function means in the
--   (typical) case that the hard fork combinator <i>is</i> used. The HFC
--   introduces the concept of a partial ledger config, which is
--   essentially the ledger config minus an <tt>EpochInfo</tt>. Whenever
--   the HFC calls functions on the underlying ledger, it maintains enough
--   state to be able to <i>construct</i> an <tt>EpochInfo</tt> on the fly
--   and then combines that with the <tt>PartialLedgerConfig</tt> to get
--   the full <a>LedgerConfig</a>. The config of the HFC <i>itself</i>
--   however does <i>not</i> require an <tt>EpochInfo</tt>, and so the
--   config that we pass here will not contain that <tt>EpochInfo</tt> (if
--   it did, that would be strange: we'd be computing the <tt>Summary</tt>
--   required to construct an <tt>EpochInfo</tt> while we already have
--   one). Critically, the HFC implements <a>hardForkSummary</a> directly
--   and does not call <a>hardForkSummary</a> in the underlying ledgers.
--   
--   When running ledgers that are normally run using the HFC as standalone
--   ledgers, then the <a>LedgerConfig</a> here must indeed already contain
--   timing information, and so this function becomes little more than a
--   projection (indeed, in this case the <a>LedgerState</a> should be
--   irrelevant).
hardForkSummary :: HasHardForkHistory blk => LedgerConfig blk -> LedgerState blk -> Summary (HardForkIndices blk)

-- | Helper function that can be used to define <a>hardForkSummary</a>
--   
--   This is basically a proof of the claim of the documentation of
--   <a>hardForkSummary</a> that <a>hardForkSummary</a> becomes a mere
--   projection of a block's ledger state when there are no hard forks. It
--   is useful to give blocks such as <tt>ShelleyBlock</tt> their own
--   <a>HasHardForkHistory</a> instance so that we can run them as
--   independent ledgers (in addition to being run with the hard fork
--   combinator).
neverForksHardForkSummary :: (LedgerConfig blk -> EraParams) -> LedgerConfig blk -> LedgerState blk -> Summary '[blk]


-- | Support for defining <tt>BlockchainTime</tt> instances
module Ouroboros.Consensus.BlockchainTime.WallClock.Util

-- | Time related tracing
--   
--   The <tt>t</tt> parameter can be instantiated by the time, e.g.,
--   <tt>UTCTime</tt> or <tt>RelativeTime</tt>.
data TraceBlockchainTimeEvent t

-- | The start time of the blockchain time is in the future
--   
--   We have to block (for <a>NominalDiffTime</a>) until that time comes.
TraceStartTimeInTheFuture :: SystemStart -> NominalDiffTime -> TraceBlockchainTimeEvent t

-- | Current slot is not yet known
--   
--   This happens when the tip of our current chain is so far in the past
--   that we cannot translate the current wallclock to a slot number,
--   typically during syncing. Until the current slot number is known, we
--   cannot produce blocks. Seeing this message during syncing therefore is
--   normal and to be expected.
--   
--   We record the current time (the time we tried to translate to a
--   <a>SlotNo</a>) as well as the <a>PastHorizonException</a>, which
--   provides detail on the bounds between which we <i>can</i> do
--   conversions. The distance between the current time and the upper bound
--   should rapidly decrease with consecutive
--   <a>TraceCurrentSlotUnknown</a> messages during syncing.
TraceCurrentSlotUnknown :: t -> PastHorizonException -> TraceBlockchainTimeEvent t

-- | The system clock moved back an acceptable time span, e.g., because of
--   an NTP sync.
--   
--   The system clock moved back such that the new current slot would be
--   smaller than the previous one. If this is within the configured limit,
--   we trace this warning but *do not change the current slot*. The
--   current slot never decreases, but the current slot may stay the same
--   longer than expected.
--   
--   When the system clock moved back more than the configured limit, we
--   shut down with a fatal exception.
TraceSystemClockMovedBack :: t -> t -> TraceBlockchainTimeEvent t
data SystemClockMovedBackException

-- | The system clock got moved back so far that the slot number decreased
--   
--   We record the the slot number before and after the change.
SystemClockMovedBack :: SlotNo -> SlotNo -> SystemClockMovedBackException
instance GHC.Base.Functor Ouroboros.Consensus.BlockchainTime.WallClock.Util.TraceBlockchainTimeEvent
instance GHC.Show.Show t => GHC.Show.Show (Ouroboros.Consensus.BlockchainTime.WallClock.Util.TraceBlockchainTimeEvent t)
instance GHC.Show.Show Ouroboros.Consensus.BlockchainTime.WallClock.Util.SystemClockMovedBackException
instance GHC.Exception.Type.Exception Ouroboros.Consensus.BlockchainTime.WallClock.Util.SystemClockMovedBackException

module Ouroboros.Consensus.Forecast
data Forecast a
Forecast :: WithOrigin SlotNo -> (SlotNo -> Except OutsideForecastRange a) -> Forecast a
[forecastAt] :: Forecast a -> WithOrigin SlotNo
[forecastFor] :: Forecast a -> SlotNo -> Except OutsideForecastRange a
data OutsideForecastRange
OutsideForecastRange :: !WithOrigin SlotNo -> !SlotNo -> !SlotNo -> OutsideForecastRange

-- | The slot for which the forecast was obtained
[outsideForecastAt] :: OutsideForecastRange -> !WithOrigin SlotNo

-- | Exclusive upper bound on the range of the forecast
[outsideForecastMaxFor] :: OutsideForecastRange -> !SlotNo

-- | The slot for which we requested a value
[outsideForecastFor] :: OutsideForecastRange -> !SlotNo

-- | Forecast where the values are never changing
--   
--   This is primarily useful for tests; the forecast range is infinite,
--   but we do still check the precondition, to catch any bugs.
constantForecastOf :: a -> WithOrigin SlotNo -> Forecast a
mapForecast :: (a -> b) -> Forecast a -> Forecast b

-- | Trivial forecast of values of type <tt>()</tt> performed by an
--   instance of <a>GetTip</a>.
--   
--   Specialization of <tt>constantForecast</tt>.
trivialForecast :: GetTip b => b -> Forecast ()

-- | Compute the upper bound for a range for a forecast across eras.
--   
--   We have to be very careful here in how we compute the maximum
--   lookahead. As long as we are in a single era, things look like this:
--   
--   <pre>
--                                            /-------------------\
--                                            |                   |
--   chain     ... - block - block - block [block]                |
--                                     |                          v
--   ledger                           TIP                  VIEW
--   </pre>
--   
--   where <tt>TIP</tt> is the current ledger tip and <tt>VIEW</tt> is the
--   last ledger view we can forecast, because the next block
--   <tt>[block]</tt> to arrive will take effect in the next leger state
--   after <tt>VIEW</tt>. Note that if the maximum lookahead is zero, this
--   looks like
--   
--   <pre>
--   chain     ... - block - block - block [block]
--                                     |      |
--   ledger                           TIP
--   </pre>
--   
--   where <tt>[block]</tt> can have immediate changes on the ledger, and
--   so we can't look ahead at all (of course, we always know the
--   <i>current</i> <tt>TIP</tt>).
--   
--   Note that blocks arriving <i>after</i> <tt>[block]</tt> can only take
--   effect <i>later</i> than <tt>[block]</tt>, and so they are not
--   relevant for computing the maximum slot number we can compute a ledger
--   view for.
--   
--   Now, if we are near an era transition, this picture gets a bit more
--   complicated. <i>If</i> the next block is still in this era (that is,
--   unless we are <i>right</i> at the edge), then that imposes <i>one</i>
--   constraint, as before. However, the first block in the <i>next</i> era
--   imposes an <i>additional</i> constraint:
--   
--   <pre>
--                        ~
--                        ~    /------------------\
--                        ~    |                  |
--            /---------- ~ ---|----------\       |
--            |           ~    |          |       |
--   block [block]        ~ [block']      |       |
--     |                  ~               v       v
--    TIP                 ~         VIEW
--                        ~
--   </pre>
--   
--   There are no restrictions on the relative values of these two maximum
--   lookahead values. This means that it's quite possible for the next era
--   to have a <i>smaller</i> lookahead (to re-iterate, since that era has
--   not yet begun, the first block in that era is at the transition, and
--   so the maximum lookahead applies from the transition point):
--   
--   <pre>
--                        ~
--                        ~    /----------\
--                        ~    |          |
--            /---------- ~ ---|----------|-------\
--            |           ~    |          |       |
--   block [block]        ~ [block']      |       |
--     |                  ~               v       v
--    TIP                 ~         VIEW
--                        ~
--   </pre>
--   
--   Indeed, if the next era has zero lookahead, when the first block of
--   the next era comes it, it can make changes immediately, and so we
--   can't even know what the view at the transition point is.
--   
--   Note that if there can be no more blocks in this era, the maximum
--   lookahead of the current era is irrelevant:
--   
--   <pre>
--         ~
--         ~    /----------\
--         ~    |          |
--         ~    |          |
--         ~    |          |
--   block ~ [block']      |
--     |   ~               v
--    TIP  ~         VIEW
--         ~
--   </pre>
--   
--   We can therefore compute the earliest <a>SlotNo</a> the next block in
--   this era (if any) can make changes to the ledger state, as well as the
--   earliest <a>SlotNo</a> the first block in the next era can; their
--   <tt>minimum</tt> will serve as an exclusive upper bound for the
--   forecast range.
crossEraForecastBound :: WithOrigin SlotNo -> SlotNo -> Word64 -> Word64 -> SlotNo
instance GHC.Classes.Eq Ouroboros.Consensus.Forecast.OutsideForecastRange
instance GHC.Show.Show Ouroboros.Consensus.Forecast.OutsideForecastRange
instance GHC.Exception.Type.Exception Ouroboros.Consensus.Forecast.OutsideForecastRange

module Ouroboros.Consensus.Ledger.SupportsProtocol

-- | Link protocol to ledger
class (BlockSupportsProtocol blk, UpdateLedger blk, ValidateEnvelope blk) => LedgerSupportsProtocol blk

-- | Extract the ledger view from the given ticked ledger state
--   
--   See <a>ledgerViewForecastAt</a> for a discussion and precise
--   definition of the relation between this and forecasting.
protocolLedgerView :: LedgerSupportsProtocol blk => LedgerConfig blk -> Ticked (LedgerState blk) -> LedgerView (BlockProtocol blk)

-- | Get a forecast at the given ledger state.
--   
--   This forecast can be used to validate headers of blocks within the
--   range of the forecast. These blocks need to live on a chain that fits
--   on the last applied block of the given ledger.
--   
--   The range of the forecast should allow to validate a sufficient number
--   of headers to validate an alternative chain longer than ours, so that
--   chain selection can decide whether or not we prefer the alternative
--   chain to our current chain. In addition, it would be helpful, though
--   not essential, if we can look further ahead than that, as this would
--   improve sync performance.
--   
--   NOTE (difference between <a>ledgerViewForecastAt</a> and
--   <a>applyChainTick</a>): Both <a>ledgerViewForecastAt</a> and
--   <a>applyChainTick</a> can be used to obtain a protocol ledger view for
--   a future slot. The difference between the two is that
--   <a>applyChainTick</a> assumes no blocks are present between the
--   current ledger tip and the specified <a>SlotNo</a>, whereas
--   <a>ledgerViewForecastAt</a> cannot make such an assumption. Thus,
--   <a>applyChainTick</a> cannot fail, whereas the forecast returned by
--   <a>ledgerViewForecastAt</a> might report an
--   <a>OutsideForecastRange</a> for the same <a>SlotNo</a>. We expect the
--   two functions to produce the same view whenever the <a>SlotNo</a>
--   <i>is</i> in range, however. More precisely:
--   
--   If
--   
--   <pre>
--      forecastFor (ledgerViewForecastAt cfg st) for
--   == Just view
--   </pre>
--   
--   then
--   
--   <pre>
--      protocolLedgerView cfg (applyChainTick cfg for st)
--   == view
--   </pre>
--   
--   See <tt>lemma_ledgerViewForecastAt_applyChainTick</tt>.
ledgerViewForecastAt :: (LedgerSupportsProtocol blk, HasCallStack) => LedgerConfig blk -> LedgerState blk -> Forecast (LedgerView (BlockProtocol blk))

module Ouroboros.Consensus.Ledger.Extended

-- | " Ledger " configuration for the extended ledger
--   
--   Since the extended ledger also does the consensus protocol validation,
--   we also need the consensus config.
newtype ExtLedgerCfg blk
ExtLedgerCfg :: TopLevelConfig blk -> ExtLedgerCfg blk
[getExtLedgerCfg] :: ExtLedgerCfg blk -> TopLevelConfig blk

-- | Extended ledger state
--   
--   This is the combination of the header state and the ledger state
--   proper.
data ExtLedgerState blk
ExtLedgerState :: !LedgerState blk -> !HeaderState blk -> ExtLedgerState blk
[ledgerState] :: ExtLedgerState blk -> !LedgerState blk
[headerState] :: ExtLedgerState blk -> !HeaderState blk
data ExtValidationError blk
ExtValidationErrorLedger :: !LedgerError blk -> ExtValidationError blk
ExtValidationErrorHeader :: !HeaderError blk -> ExtValidationError blk
decodeExtLedgerState :: (forall s. Decoder s (LedgerState blk)) -> (forall s. Decoder s (ChainDepState (BlockProtocol blk))) -> (forall s. Decoder s (AnnTip blk)) -> forall s. Decoder s (ExtLedgerState blk)
encodeExtLedgerState :: (LedgerState blk -> Encoding) -> (ChainDepState (BlockProtocol blk) -> Encoding) -> (AnnTip blk -> Encoding) -> ExtLedgerState blk -> Encoding
castExtLedgerState :: (Coercible (LedgerState blk) (LedgerState blk'), Coercible (ChainDepState (BlockProtocol blk)) (ChainDepState (BlockProtocol blk')), TipInfo blk ~ TipInfo blk') => ExtLedgerState blk -> ExtLedgerState blk'

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type
instance GHC.Generics.Generic (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Ledger.Extended.ExtValidationError blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Ledger.Extended.ExtLedgerCfg blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Show.Show (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Show.Show (Ouroboros.Consensus.Ledger.Extended.ExtValidationError blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Extended.ExtValidationError blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk)
instance (Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.BlockConfig blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.CodecConfig blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Basics.LedgerConfig blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.StorageConfig blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Extended.ExtLedgerCfg blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => Ouroboros.Consensus.Ledger.Basics.IsLedger (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => Ouroboros.Consensus.Ledger.Abstract.ApplyBlock (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk) blk
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Extended.ExtValidationError blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk)
instance Ouroboros.Consensus.Ledger.Basics.IsLedger (Ouroboros.Consensus.Ledger.Basics.LedgerState blk) => Ouroboros.Consensus.Ledger.Basics.GetTip (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk)
instance Ouroboros.Consensus.Ledger.Basics.IsLedger (Ouroboros.Consensus.Ledger.Basics.LedgerState blk) => Ouroboros.Consensus.Ledger.Basics.GetTip (Ouroboros.Consensus.Ticked.Ticked (Ouroboros.Consensus.Ledger.Extended.ExtLedgerState blk))

module Ouroboros.Consensus.Storage.LedgerDB.LedgerDB

-- | Internal newtype wrapper around a ledger state <tt>l</tt> so that we
--   can define a non-blanket <a>Anchorable</a> instance.
newtype Checkpoint l
Checkpoint :: l -> Checkpoint l
[unCheckpoint] :: Checkpoint l -> l

-- | Internal state of the ledger DB
--   
--   The ledger DB looks like
--   
--   <pre>
--   anchor |&gt; snapshots &lt;| current
--   </pre>
--   
--   where <tt>anchor</tt> records the oldest known snapshot and
--   <tt>current</tt> the most recent. The anchor is the oldest point we
--   can roll back to.
--   
--   We take a snapshot after each block is applied and keep in memory a
--   window of the last <tt>k</tt> snapshots. We have verified empirically
--   (#1936) that the overhead of keeping <tt>k</tt> snapshots in memory is
--   small, i.e., about 5% compared to keeping a snapshot every 100 blocks.
--   This is thanks to sharing between consecutive snapshots.
--   
--   As an example, suppose we have <tt>k = 6</tt>. The ledger DB grows as
--   illustrated below, where we indicate the anchor number of blocks, the
--   stored snapshots, and the current ledger.
--   
--   <pre>
--   anchor |&gt; #   [ snapshots ]                   &lt;| tip
--   ---------------------------------------------------------------------------
--   G      |&gt; (0) [ ]                             &lt;| G
--   G      |&gt; (1) [ L1]                           &lt;| L1
--   G      |&gt; (2) [ L1,  L2]                      &lt;| L2
--   G      |&gt; (3) [ L1,  L2,  L3]                 &lt;| L3
--   G      |&gt; (4) [ L1,  L2,  L3,  L4]            &lt;| L4
--   G      |&gt; (5) [ L1,  L2,  L3,  L4,  L5]       &lt;| L5
--   G      |&gt; (6) [ L1,  L2,  L3,  L4,  L5,  L6]  &lt;| L6
--   L1     |&gt; (6) [ L2,  L3,  L4,  L5,  L6,  L7]  &lt;| L7
--   L2     |&gt; (6) [ L3,  L4,  L5,  L6,  L7,  L8]  &lt;| L8
--   L3     |&gt; (6) [ L4,  L5,  L6,  L7,  L8,  L9]  &lt;| L9   (*)
--   L4     |&gt; (6) [ L5,  L6,  L7,  L8,  L9,  L10] &lt;| L10
--   L5     |&gt; (6) [*L6,  L7,  L8,  L9,  L10, L11] &lt;| L11
--   L6     |&gt; (6) [ L7,  L8,  L9,  L10, L11, L12] &lt;| L12
--   L7     |&gt; (6) [ L8,  L9,  L10, L12, L12, L13] &lt;| L13
--   L8     |&gt; (6) [ L9,  L10, L12, L12, L13, L14] &lt;| L14
--   </pre>
--   
--   The ledger DB must guarantee that at all times we are able to roll
--   back <tt>k</tt> blocks. For example, if we are on line (*), and roll
--   back 6 blocks, we get
--   
--   <pre>
--   L3 |&gt; []
--   </pre>
newtype LedgerDB l
LedgerDB :: AnchoredSeq (WithOrigin SlotNo) (Checkpoint l) (Checkpoint l) -> LedgerDB l

-- | Ledger states
[ledgerDbCheckpoints] :: LedgerDB l -> AnchoredSeq (WithOrigin SlotNo) (Checkpoint l) (Checkpoint l)
type LedgerDB' blk = LedgerDB (ExtLedgerState blk)
data LedgerDbCfg l
LedgerDbCfg :: !SecurityParam -> !LedgerCfg l -> LedgerDbCfg l
[ledgerDbCfgSecParam] :: LedgerDbCfg l -> !SecurityParam
[ledgerDbCfg] :: LedgerDbCfg l -> !LedgerCfg l
configLedgerDb :: ConsensusProtocol (BlockProtocol blk) => TopLevelConfig blk -> LedgerDbCfg (ExtLedgerState blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.Checkpoint l)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.LedgerDB l)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.LedgerDbCfg l)
instance GHC.Show.Show l => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.LedgerDB l)
instance GHC.Classes.Eq l => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.LedgerDB l)
instance NoThunks.Class.NoThunks l => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.LedgerDB l)
instance GHC.Show.Show l => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.Checkpoint l)
instance GHC.Classes.Eq l => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.Checkpoint l)
instance NoThunks.Class.NoThunks l => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.Checkpoint l)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Basics.LedgerCfg l) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.LedgerDbCfg l)
instance Ouroboros.Consensus.Ledger.Basics.IsLedger l => Ouroboros.Consensus.Ledger.Basics.GetTip (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.LedgerDB l)
instance Ouroboros.Consensus.Ledger.Basics.GetTip l => Ouroboros.Network.AnchoredSeq.Anchorable (Cardano.Slotting.Slot.WithOrigin Cardano.Slotting.Slot.SlotNo) (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.Checkpoint l) (Ouroboros.Consensus.Storage.LedgerDB.LedgerDB.Checkpoint l)

module Ouroboros.Consensus.Storage.LedgerDB.Query

-- | Information about the state of the ledger at the anchor
ledgerDbAnchor :: LedgerDB l -> l

-- | The ledger state at the tip of the chain
ledgerDbCurrent :: GetTip l => LedgerDB l -> l

-- | Have we seen at least <tt>k</tt> blocks?
ledgerDbIsSaturated :: GetTip l => SecurityParam -> LedgerDB l -> Bool

-- | How many blocks can we currently roll back?
ledgerDbMaxRollback :: GetTip l => LedgerDB l -> Word64

-- | Get a past ledger state
--   
--   &lt;math&gt;
--   
--   When no ledger state (or anchor) has the given <a>Point</a>,
--   <a>Nothing</a> is returned.
ledgerDbPast :: (HasHeader blk, IsLedger l, HeaderHash l ~ HeaderHash blk) => Point blk -> LedgerDB l -> Maybe l

-- | All snapshots currently stored by the ledger DB (new to old)
--   
--   This also includes the snapshot at the anchor. For each snapshot we
--   also return the distance from the tip.
ledgerDbSnapshots :: LedgerDB l -> [(Word64, l)]

-- | Reference to the block at the tip of the chain
ledgerDbTip :: GetTip l => LedgerDB l -> Point l


-- | Accessors for the LedgerDB and management
--   
--   This module defines the operations that can be done on a LedgerDB, as
--   well as the procedures to apply a block to a LedgerDB and pushing the
--   resulting LedgerState into the DB.
module Ouroboros.Consensus.Storage.LedgerDB.Update

-- | Ledger DB starting at the specified ledger state
ledgerDbWithAnchor :: GetTip l => l -> LedgerDB l

-- | Annotated ledger errors
data AnnLedgerError l blk
AnnLedgerError :: LedgerDB l -> RealPoint blk -> LedgerErr l -> AnnLedgerError l blk

-- | The ledger DB just <i>before</i> this block was applied
[annLedgerState] :: AnnLedgerError l blk -> LedgerDB l

-- | Reference to the block that had the error
[annLedgerErrRef] :: AnnLedgerError l blk -> RealPoint blk

-- | The ledger error itself
[annLedgerErr] :: AnnLedgerError l blk -> LedgerErr l
type AnnLedgerError' blk = AnnLedgerError (ExtLedgerState blk) blk

-- | <a>Ap</a> is used to pass information about blocks to ledger DB
--   updates
--   
--   The constructors serve two purposes:
--   
--   <ul>
--   <li>Specify the various parameters a. Are we passing the block by
--   value or by reference? b. Are we applying or reapplying the
--   block?</li>
--   <li>Compute the constraint <tt>c</tt> on the monad <tt>m</tt> in order
--   to run the query: a. If we are passing a block by reference, we must
--   be able to resolve it. b. If we are applying rather than reapplying,
--   we might have ledger errors.</li>
--   </ul>
data Ap m l blk c
[ReapplyVal] :: blk -> Ap m l blk ()
[ApplyVal] :: blk -> Ap m l blk (ThrowsLedgerError m l blk)
[ReapplyRef] :: RealPoint blk -> Ap m l blk (ResolvesBlocks m blk)
[ApplyRef] :: RealPoint blk -> Ap m l blk (ResolvesBlocks m blk, ThrowsLedgerError m l blk)

-- | <a>Weaken</a> increases the constraint on the monad <tt>m</tt>.
--   
--   This is primarily useful when combining multiple <a>Ap</a>s in a
--   single homogeneous structure.
[Weaken] :: (c' => c) => Ap m l blk c -> Ap m l blk c'

-- | Exceeded maximum rollback supported by the current ledger DB state
--   
--   Under normal circumstances this will not arise. It can really only
--   happen in the presence of data corruption (or when switching to a
--   shorter fork, but that is disallowed by all currently known Ouroboros
--   protocols).
--   
--   Records both the supported and the requested rollback.
data ExceededRollback
ExceededRollback :: Word64 -> Word64 -> ExceededRollback
[rollbackMaximum] :: ExceededRollback -> Word64
[rollbackRequested] :: ExceededRollback -> Word64
class Monad m => ThrowsLedgerError m l blk
throwLedgerError :: ThrowsLedgerError m l blk => LedgerDB l -> RealPoint blk -> LedgerErr l -> m a
defaultThrowLedgerErrors :: ExceptT (AnnLedgerError l blk) m a -> m (Either (AnnLedgerError l blk) a)

-- | Resolve a block
--   
--   Resolving a block reference to the actual block lives in <tt>m</tt>
--   because it might need to read the block from disk (and can therefore
--   not be done inside an STM transaction).
--   
--   NOTE: The ledger DB will only ask the <tt>ChainDB</tt> for blocks it
--   knows must exist. If the <tt>ChainDB</tt> is unable to fulfill the
--   request, data corruption must have happened and the <tt>ChainDB</tt>
--   should trigger validation mode.
type ResolveBlock m blk = RealPoint blk -> m blk

-- | Monads in which we can resolve blocks
--   
--   To guide type inference, we insist that we must be able to infer the
--   type of the block we are resolving from the type of the monad.
class Monad m => ResolvesBlocks m blk | m -> blk
doResolveBlock :: ResolvesBlocks m blk => ResolveBlock m blk
defaultResolveBlocks :: ResolveBlock m blk -> ReaderT (ResolveBlock m blk) m a -> m a
defaultResolveWithErrors :: ResolveBlock m blk -> ExceptT (AnnLedgerError l blk) (ReaderT (ResolveBlock m blk) m) a -> m (Either (AnnLedgerError l blk) a)

-- | Transform the underlying <a>AnchoredSeq</a> using the given functions.
ledgerDbBimap :: Anchorable (WithOrigin SlotNo) a b => (l -> a) -> (l -> b) -> LedgerDB l -> AnchoredSeq (WithOrigin SlotNo) a b

-- | Prune snapshots until at we have at most <tt>k</tt> snapshots in the
--   LedgerDB, excluding the snapshots stored at the anchor.
ledgerDbPrune :: GetTip l => SecurityParam -> LedgerDB l -> LedgerDB l
ledgerDbPush :: forall m c l blk. (ApplyBlock l blk, Monad m, c) => LedgerDbCfg l -> Ap m l blk c -> LedgerDB l -> m (LedgerDB l)

-- | Switch to a fork
ledgerDbSwitch :: (ApplyBlock l blk, Monad m, c) => LedgerDbCfg l -> Word64 -> (UpdateLedgerDbTraceEvent blk -> m ()) -> [Ap m l blk c] -> LedgerDB l -> m (Either ExceededRollback (LedgerDB l))
ledgerDbPush' :: ApplyBlock l blk => LedgerDbCfg l -> blk -> LedgerDB l -> LedgerDB l
ledgerDbPushMany' :: ApplyBlock l blk => LedgerDbCfg l -> [blk] -> LedgerDB l -> LedgerDB l
ledgerDbSwitch' :: forall l blk. ApplyBlock l blk => LedgerDbCfg l -> Word64 -> [blk] -> LedgerDB l -> Maybe (LedgerDB l)
newtype PushGoal blk
PushGoal :: RealPoint blk -> PushGoal blk
[unPushGoal] :: PushGoal blk -> RealPoint blk
newtype PushStart blk
PushStart :: RealPoint blk -> PushStart blk
[unPushStart] :: PushStart blk -> RealPoint blk
newtype Pushing blk
Pushing :: RealPoint blk -> Pushing blk
[unPushing] :: Pushing blk -> RealPoint blk
data UpdateLedgerDbTraceEvent blk

-- | Event fired when we are about to push a block to the LedgerDB
StartedPushingBlockToTheLedgerDb :: !PushStart blk -> PushGoal blk -> !Pushing blk -> UpdateLedgerDbTraceEvent blk
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Update.PushStart blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Update.PushStart blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Update.PushGoal blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Update.PushGoal blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Update.Pushing blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Update.Pushing blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.Update.UpdateLedgerDbTraceEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Update.UpdateLedgerDbTraceEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Update.UpdateLedgerDbTraceEvent blk)
instance GHC.Base.Monad m => Ouroboros.Consensus.Storage.LedgerDB.Update.ThrowsLedgerError (Control.Monad.Trans.Except.ExceptT (Ouroboros.Consensus.Storage.LedgerDB.Update.AnnLedgerError l blk) m) l blk
instance GHC.Base.Monad m => Ouroboros.Consensus.Storage.LedgerDB.Update.ResolvesBlocks (Control.Monad.Trans.Reader.ReaderT (Ouroboros.Consensus.Storage.LedgerDB.Update.ResolveBlock m blk) m) blk
instance GHC.Base.Monad m => Ouroboros.Consensus.Storage.LedgerDB.Update.ResolvesBlocks (Control.Monad.Trans.Except.ExceptT e (Control.Monad.Trans.Reader.ReaderT (Ouroboros.Consensus.Storage.LedgerDB.Update.ResolveBlock m blk) m)) blk

module Ouroboros.Consensus.Node.ProtocolInfo
newtype NumCoreNodes
NumCoreNodes :: Word64 -> NumCoreNodes

-- | Data required by clients of a node running the specified protocol.
data ProtocolClientInfo b
ProtocolClientInfo :: CodecConfig b -> ProtocolClientInfo b
[pClientInfoCodecConfig] :: ProtocolClientInfo b -> CodecConfig b

-- | Data required to run the specified protocol.
data ProtocolInfo b
ProtocolInfo :: TopLevelConfig b -> ExtLedgerState b -> ProtocolInfo b
[pInfoConfig] :: ProtocolInfo b -> TopLevelConfig b

-- | At genesis
[pInfoInitLedger] :: ProtocolInfo b -> ExtLedgerState b
enumCoreNodes :: NumCoreNodes -> [CoreNodeId]
data family ProtocolParams blk :: Type
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Node.ProtocolInfo.NumCoreNodes
instance GHC.Show.Show Ouroboros.Consensus.Node.ProtocolInfo.NumCoreNodes

module Ouroboros.Consensus.Protocol.BFT

-- | Basic BFT
--   
--   Basic BFT is very simple:
--   
--   <ul>
--   <li>No support for delegation (and hence has no need for a ledger
--   view)</li>
--   <li>Requires round-robin block signing throughout (and so has no need
--   for any chain state or cryptographic leader proofs).</li>
--   <li>Does not use any stateful crypto (and so has no need for node
--   state)</li>
--   </ul>
data Bft c
data BftFields c toSign
BftFields :: !SignedDSIGN (BftDSIGN c) toSign -> BftFields c toSign
[bftSignature] :: BftFields c toSign -> !SignedDSIGN (BftDSIGN c) toSign

-- | Protocol parameters
data BftParams
BftParams :: !SecurityParam -> !NumCoreNodes -> BftParams

-- | Security parameter
--   
--   Although the protocol proper does not have such a security parameter,
--   we insist on it.
[bftSecurityParam] :: BftParams -> !SecurityParam

-- | Number of core nodes
[bftNumNodes] :: BftParams -> !NumCoreNodes
data BftValidationErr
BftInvalidSignature :: String -> BftValidationErr
forgeBftFields :: (BftCrypto c, Signable (BftDSIGN c) toSign) => ConsensusConfig (Bft c) -> toSign -> BftFields c toSign

-- | Crypto primitives required by BFT
class (Typeable c, DSIGNAlgorithm (BftDSIGN c), Condense (SigDSIGN (BftDSIGN c)), NoThunks (SigDSIGN (BftDSIGN c)), ContextDSIGN (BftDSIGN c) ~ ()) => BftCrypto c where {
    type BftDSIGN c :: Type;
}
data BftMockCrypto
data BftStandardCrypto
data BftValidateView c
BftValidateView :: BftFields c signed -> signed -> BftValidateView c

-- | Convenience constructor for <a>BftValidateView</a>
bftValidateView :: (SignedHeader hdr, Signable (BftDSIGN c) (Signed hdr)) => (hdr -> BftFields c (Signed hdr)) -> hdr -> BftValidateView c

-- | Static configuration required to run the consensus protocol
--   
--   Every method in the <a>ConsensusProtocol</a> class takes the consensus
--   configuration as a parameter, so having this as a data family rather
--   than a type family resolves most ambiguity.
--   
--   Defined out of the class so that protocols can define this type
--   without having to define the entire protocol at the same time (or
--   indeed in the same module).
data family ConsensusConfig p :: Type
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Protocol.BFT.BftParams
instance GHC.Generics.Generic Ouroboros.Consensus.Protocol.BFT.BftParams
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Protocol.BFT.BftValidationErr
instance GHC.Generics.Generic Ouroboros.Consensus.Protocol.BFT.BftValidationErr
instance GHC.Classes.Eq Ouroboros.Consensus.Protocol.BFT.BftValidationErr
instance GHC.Show.Show Ouroboros.Consensus.Protocol.BFT.BftValidationErr
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.Protocol.BFT.Bft c))
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.BFT.BftFields c toSign)
instance Ouroboros.Consensus.Protocol.BFT.BftCrypto c => GHC.Show.Show (Ouroboros.Consensus.Protocol.BFT.BftFields c toSign)
instance Ouroboros.Consensus.Protocol.BFT.BftCrypto c => GHC.Classes.Eq (Ouroboros.Consensus.Protocol.BFT.BftFields c toSign)
instance Ouroboros.Consensus.Protocol.BFT.BftCrypto Ouroboros.Consensus.Protocol.BFT.BftMockCrypto
instance Ouroboros.Consensus.Protocol.BFT.BftCrypto Ouroboros.Consensus.Protocol.BFT.BftStandardCrypto
instance Ouroboros.Consensus.Protocol.BFT.BftCrypto c => Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol (Ouroboros.Consensus.Protocol.BFT.Bft c)
instance (Ouroboros.Consensus.Protocol.BFT.BftCrypto c, Data.Typeable.Internal.Typeable toSign) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.BFT.BftFields c toSign)
instance Ouroboros.Consensus.Protocol.BFT.BftCrypto c => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Protocol.BFT.BftFields c toSign)
instance Ouroboros.Consensus.Protocol.BFT.BftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.Protocol.BFT.Bft c))


-- | HeaderState history
--   
--   Intended for qualified import
--   
--   <pre>
--   import           Ouroboros.Consensus.HeaderStateHistory (HeaderStateHistory)
--   import qualified Ouroboros.Consensus.HeaderStateHistory as HeaderStateHistory
--   </pre>
module Ouroboros.Consensus.HeaderStateHistory

-- | Maintain a history of <a>HeaderState</a>s.
newtype HeaderStateHistory blk
HeaderStateHistory :: AnchoredSeq (WithOrigin SlotNo) (HeaderState blk) (HeaderState blk) -> HeaderStateHistory blk
[unHeaderStateHistory] :: HeaderStateHistory blk -> AnchoredSeq (WithOrigin SlotNo) (HeaderState blk) (HeaderState blk)
cast :: (Coercible (ChainDepState (BlockProtocol blk)) (ChainDepState (BlockProtocol blk')), TipInfo blk ~ TipInfo blk') => HeaderStateHistory blk -> HeaderStateHistory blk'
current :: HeaderStateHistory blk -> HeaderState blk

-- | &lt;math&gt; ). Rewind the header state history
--   
--   NOTE: we don't distinguish headers of regular blocks from headers of
--   EBBs. Whenever we use "header" it can be either. In practice, EBB
--   headers do not affect the <a>ChainDepState</a>, but they <i>do</i>
--   affect the <a>AnnTip</a>.
--   
--   PRECONDITION: the point to rewind to must correspond to a header (or
--   <a>GenesisPoint</a>) that was previously applied to the header state
--   history.
--   
--   Rewinding the header state history is intended to be used when
--   switching to a fork, longer or equally long to the chain to which the
--   current header state corresponds. So each rewinding should be followed
--   by rolling forward (using <tt>headerStateHistoryPush</tt>) at least as
--   many blocks that we have rewound.
--   
--   Note that repeatedly rewinding a header state history does not make it
--   possible to rewind it all the way to genesis (this would mean that the
--   whole historical header state is accumulated or derivable from the
--   current header state history). For example, rewinding a header state
--   by <tt>i</tt> blocks and then rewinding that header state again by
--   <tt>j</tt> where <tt>i + j &gt; k</tt> is not possible and will yield
--   <a>Nothing</a>.
rewind :: forall blk. HasAnnTip blk => Point blk -> HeaderStateHistory blk -> Maybe (HeaderStateHistory blk)

-- | Trim the <a>HeaderStateHistory</a> to the given size, dropping the
--   oldest snapshots. The anchor will be shifted accordingly.
--   
--   Note that we do not include the anchor in the size. For example,
--   trimming to 0 results in no snapshots but still an anchor. Trimming to
--   1 results in 1 snapshot and an anchor.
trim :: Int -> HeaderStateHistory blk -> HeaderStateHistory blk

-- | Variation on <a>validateHeader</a> that maintains a
--   <a>HeaderStateHistory</a>.
--   
--   This is used only in the chain sync client for header-only validation.
--   
--   Note: this function does not trim the <a>HeaderStateHistory</a>.
validateHeader :: forall blk. (BlockSupportsProtocol blk, ValidateEnvelope blk) => TopLevelConfig blk -> LedgerView (BlockProtocol blk) -> Header blk -> HeaderStateHistory blk -> Except (HeaderError blk) (HeaderStateHistory blk)

-- | Create a <a>HeaderStateHistory</a> corresponding to the blocks in the
--   given <a>Chain</a>.
--   
--   PRECONDITION: the blocks in the chain are valid.
fromChain :: ApplyBlock (ExtLedgerState blk) blk => TopLevelConfig blk -> ExtLedgerState blk -> Chain blk -> HeaderStateHistory blk
instance GHC.Generics.Generic (Ouroboros.Consensus.HeaderStateHistory.HeaderStateHistory blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.HasAnnTip blk) => GHC.Classes.Eq (Ouroboros.Consensus.HeaderStateHistory.HeaderStateHistory blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.HasAnnTip blk) => GHC.Show.Show (Ouroboros.Consensus.HeaderStateHistory.HeaderStateHistory blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, Ouroboros.Consensus.HeaderValidation.HasAnnTip blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HeaderStateHistory.HeaderStateHistory blk)

module Ouroboros.Consensus.HardFork.Combinator.State.Types

-- | Information about the current era
data Current f blk
Current :: !Bound -> !f blk -> Current f blk
[currentStart] :: Current f blk -> !Bound
[currentState] :: Current f blk -> !f blk

-- | Generic hard fork state
--   
--   This is used both for the consensus state and the ledger state.
--   
--   By using a telescope with <tt>f ~ LedgerState</tt>, we will keep track
--   of <a>Past</a> information for eras before the current one:
--   
--   <pre>
--   TZ currentByronState
--   TZ pastByronState $ TZ currentShelleyState
--   TZ pastByronState $ TS pastShelleyState $ TZ currentAllegraState
--   ...
--   </pre>
--   
--   These are some intuitions on how the Telescope operations behave for
--   this type:
--   
--   <h1><tt>extend</tt></h1>
--   
--   Suppose we have a telescope containing the ledger state. The "how to
--   extend" argument would take, say, the final Byron state to the initial
--   Shelley state; and "where to extend from" argument would indicate when
--   we want to extend: when the current slot number has gone past the end
--   of the Byron era.
--   
--   <h1><tt>retract</tt></h1>
--   
--   Suppose we have a telescope containing the consensus state. When we
--   rewind the consensus state, we might cross a hard fork transition
--   point. So we first <i>retract</i> the telescope <i>to</i> the era
--   containing the slot number that we want to rewind to, and only then
--   call <tt>rewindChainDepState</tt> on that era. Of course, retraction
--   may fail (we might not <i>have</i> past consensus state to rewind to
--   anymore); this failure would require a choice for a particular monad
--   <tt>m</tt>.
--   
--   <h1><tt>align</tt></h1>
--   
--   Suppose we have one telescope containing the already-ticked ledger
--   state, and another telescope containing the consensus state. Since the
--   ledger state has already been ticked, it might have been advanced to
--   the next era. If this happens, we should then align the consensus
--   state with the ledger state, moving <i>it</i> also to the next era,
--   before we can do the consensus header validation check. Note that in
--   this particular example, the ledger state will always be ahead of the
--   consensus state, never behind; <tt>alignExtend</tt> can be used in
--   this case.
newtype HardForkState f xs
HardForkState :: Telescope (K Past) (Current f) xs -> HardForkState f xs
[getHardForkState] :: HardForkState f xs -> Telescope (K Past) (Current f) xs

-- | Information about a past era
data Past
Past :: !Bound -> !Bound -> Past
[pastStart] :: Past -> !Bound
[pastEnd] :: Past -> !Bound

-- | Thin wrapper around <a>sequence</a>
sequenceHardForkState :: forall m f xs. (All Top xs, Functor m) => HardForkState (m :.: f) xs -> m (HardForkState f xs)

-- | Forecast a <tt>view y</tt> from a <tt>state x</tt> across an era
--   transition.
--   
--   In addition to the <a>Bound</a> of the transition, this is also told
--   the <a>SlotNo</a> we're constructing a forecast for. This enables the
--   translation function to take into account any scheduled changes that
--   the final ledger view in the preceding era might have.
newtype CrossEraForecaster state view x y
CrossEraForecaster :: (Bound -> SlotNo -> state x -> Except OutsideForecastRange (view y)) -> CrossEraForecaster state view x y
[crossEraForecastWith] :: CrossEraForecaster state view x y -> Bound -> SlotNo -> state x -> Except OutsideForecastRange (view y)

-- | Knowledge in a particular era of the transition to the next era
data TransitionInfo

-- | No transition is yet known for this era We instead record the ledger
--   tip (which must be in <i>this</i> era)
--   
--   NOTE: If we are forecasting, this will be set to the slot number of
--   the (past) ledger state in which the forecast was created. This means
--   that when we construct an <tt>EpochInfo</tt> using a
--   <tt>HardForkLedgerView</tt>, the range of that <tt>EpochInfo</tt> will
--   extend a safe zone from that <i>past</i> ledger state.
TransitionUnknown :: !WithOrigin SlotNo -> TransitionInfo

-- | Transition to the next era is known to happen at this <a>EpochNo</a>
TransitionKnown :: !EpochNo -> TransitionInfo

-- | The transition is impossible
--   
--   This can be due to one of two reasons:
--   
--   <ul>
--   <li>We are in the final era</li>
--   <li>This era has not actually begun yet (we are forecasting). In this
--   case, we cannot look past the safe zone of this era and hence, by
--   definition, the transition to the <i>next</i> era cannot happen.</li>
--   </ul>
TransitionImpossible :: TransitionInfo

-- | Translate <tt>f x</tt> to <tt>f y</tt> across an era transition
--   
--   Typically <tt>f</tt> will be <tt>LedgerState</tt> or
--   <tt>WrapChainDepState</tt>.
newtype Translate f x y
Translate :: (EpochNo -> f x -> f y) -> Translate f x y
[translateWith] :: Translate f x y -> EpochNo -> f x -> f y
instance GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.State.Types.Current f blk)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.Combinator.State.Types.Past
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.Combinator.State.Types.Past
instance GHC.Show.Show Ouroboros.Consensus.HardFork.Combinator.State.Types.Past
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.Combinator.State.Types.Past
instance NoThunks.Class.NoThunks Ouroboros.Consensus.HardFork.Combinator.State.Types.TransitionInfo
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.Combinator.State.Types.TransitionInfo
instance GHC.Show.Show Ouroboros.Consensus.HardFork.Combinator.State.Types.TransitionInfo

module Ouroboros.Consensus.HardFork.Combinator.Translation
data EraTranslation xs
EraTranslation :: InPairs (RequiringBoth WrapLedgerConfig (Translate LedgerState)) xs -> InPairs (RequiringBoth WrapConsensusConfig (Translate WrapChainDepState)) xs -> InPairs (RequiringBoth WrapLedgerConfig (CrossEraForecaster LedgerState WrapLedgerView)) xs -> EraTranslation xs
[translateLedgerState] :: EraTranslation xs -> InPairs (RequiringBoth WrapLedgerConfig (Translate LedgerState)) xs
[translateChainDepState] :: EraTranslation xs -> InPairs (RequiringBoth WrapConsensusConfig (Translate WrapChainDepState)) xs
[crossEraForecast] :: EraTranslation xs -> InPairs (RequiringBoth WrapLedgerConfig (CrossEraForecaster LedgerState WrapLedgerView)) xs
trivialEraTranslation :: EraTranslation '[blk]
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Translation.EraTranslation xs)


-- | Lifting functions for the various types used in <a>HardForkState</a>
--   
--   NOTE: These are internal and not exported in the toplevel
--   <tt>.State</tt> module.
module Ouroboros.Consensus.HardFork.Combinator.State.Lift
lift :: (f blk -> f' blk) -> Current f blk -> Current f' blk
liftM :: Functor m => (f blk -> m (f' blk)) -> Current f blk -> m (Current f' blk)


-- | Injecting a transaction from one block type to another
module Ouroboros.Consensus.HardFork.Combinator.InjectTxs

-- | <tt>tx</tt> is either <a>GenTx</a> or <a>WrapValidatedGenTx</a>
--   
--   See <a>InjectTx</a> and <a>InjectValidatedTx</a>, respectively.
data InjectPolyTx tx blk blk'
InjectPolyTx :: (tx blk -> Maybe (tx blk')) -> InjectPolyTx tx blk blk'
[injectTxWith] :: InjectPolyTx tx blk blk' -> tx blk -> Maybe (tx blk')

-- | The injection that always fails
cannotInjectPolyTx :: InjectPolyTx tx blk blk'
matchPolyTx :: SListI xs => InPairs (InjectPolyTx tx) xs -> NS tx xs -> HardForkState f xs -> Either (Mismatch tx (Current f) xs) (HardForkState (Product tx f) xs)

-- | Match a list of transactions with an <a>NS</a>, attempting to inject
--   where possible
matchPolyTxsNS :: forall tx f xs. SListI xs => InPairs (InjectPolyTx tx) xs -> NS f xs -> [NS tx xs] -> ([Mismatch tx f xs], NS (Product f ([] :.: tx)) xs)
type InjectTx = InjectPolyTx GenTx

-- | <a>cannotInjectPolyTx</a> at type <a>InjectTx</a>
cannotInjectTx :: InjectTx blk blk'

-- | <a>matchPolyTx</a> at type <a>InjectTx</a>
matchTx :: SListI xs => InPairs InjectTx xs -> NS GenTx xs -> HardForkState f xs -> Either (Mismatch GenTx (Current f) xs) (HardForkState (Product GenTx f) xs)

-- | <a>InjectPolyTx</a> at type <a>InjectTx</a>
pattern InjectTx :: (GenTx blk -> Maybe (GenTx blk')) -> InjectTx blk blk'
type InjectValidatedTx = InjectPolyTx WrapValidatedGenTx

-- | <a>cannotInjectPolyTx</a> at type <a>InjectValidatedTx</a>
cannotInjectValidatedTx :: InjectValidatedTx blk blk'

-- | <a>matchPolyTx</a> at type <a>InjectValidatedTx</a>
matchValidatedTx :: SListI xs => InPairs InjectValidatedTx xs -> NS WrapValidatedGenTx xs -> HardForkState f xs -> Either (Mismatch WrapValidatedGenTx (Current f) xs) (HardForkState (Product WrapValidatedGenTx f) xs)

-- | <a>matchPolyTxsNS</a> at type <a>InjectValidatedTx</a>
matchValidatedTxsNS :: forall f xs. SListI xs => InPairs InjectValidatedTx xs -> NS f xs -> [NS WrapValidatedGenTx xs] -> ([Mismatch WrapValidatedGenTx f xs], NS (Product f ([] :.: WrapValidatedGenTx)) xs)

-- | <a>InjectPolyTx</a> at type <a>InjectValidatedTx</a>
pattern InjectValidatedTx :: (WrapValidatedGenTx blk -> Maybe (WrapValidatedGenTx blk')) -> InjectValidatedTx blk blk'

module Ouroboros.Consensus.Util.ResourceRegistry

-- | Attempt to allocate a resource in a registry which is closed
--   
--   When calling <a>closeRegistry</a> (typically, leaving the scope of
--   <a>withRegistry</a>), all resources in the registry must be released.
--   If a concurrent thread is still allocating resources, we end up with a
--   race between the thread trying to allocate new resources and the
--   registry trying to free them all. To avoid this, before releasing
--   anything, the registry will record itself as closed. Any attempt by a
--   concurrent thread to allocate a new resource will then result in a
--   <a>RegistryClosedException</a>.
--   
--   It is probably not particularly useful for threads to try and catch
--   this exception (apart from in a generic handler that does local
--   resource cleanup). The thread will anyway soon receive a
--   <tt>ThreadKilled</tt> exception.
data RegistryClosedException
RegistryClosedException :: !Context m -> !PrettyCallStack -> !Context m -> RegistryClosedException

-- | The context in which the registry was created
[registryClosedRegistryContext] :: RegistryClosedException -> !Context m

-- | Callstack to the call to <a>close</a>
--   
--   Note that <a>close</a> can only be called from the same thread that
--   created the registry.
[registryClosedCloseCallStack] :: RegistryClosedException -> !PrettyCallStack

-- | Context of the call resulting in the exception
[registryClosedAllocContext] :: RegistryClosedException -> !Context m

-- | Registry used from untracked threads
--   
--   If this exception is raised, it indicates a bug in the caller.
data ResourceRegistryThreadException

-- | Create a new private registry for use by a bracketed resource
--   
--   Use this combinator as a more specific and easier-to-maintain
--   alternative to the following.
--   
--   <pre>
--   'withRegistry' $ \rr -&gt;
--     'bracket' (newFoo rr) closeFoo $ \foo -&gt;
--       (... rr does not occur in this scope ...)
--   </pre>
--   
--   NB The scoped body can use <a>withRegistry</a> if it also needs its
--   own, separate registry.
--   
--   Use this combinator to emphasize that the registry is private to (ie
--   only used by and/or via) the bracketed resource and that it thus has
--   nearly the same lifetime. This combinator ensures the following
--   specific invariants regarding lifetimes and order of releases.
--   
--   o The registry itself is older than the bracketed resource.
--   
--   o The only registered resources older than the bracketed resource were
--   allocated in the registry by the function that allocated the bracketed
--   resource.
--   
--   o Because of the older resources, the bracketed resource is itself
--   also registered in the registry; that's the only way we can be sure to
--   release all resources in the right order.
--   
--   NB Because the registry is private to the resource, the <tt>a</tt>
--   type could save the handle to <tt>registry</tt> and safely close the
--   registry if the scoped body calls <tt>closeA</tt> before the bracket
--   ends. Though we have not used the type system to guarantee that the
--   interface of the <tt>a</tt> type cannot leak the registry to the body,
--   this combinator does its part to keep the registry private to the
--   bracketed resource.
--   
--   See documentation of <a>ResourceRegistry</a> for a more general
--   discussion.
bracketWithPrivateRegistry :: (IOLike m, HasCallStack) => (ResourceRegistry m -> m a) -> (a -> m ()) -> (a -> m r) -> m r

-- | The thread that created the registry
registryThread :: ResourceRegistry m -> ThreadId m

-- | Create a new registry
--   
--   See documentation of <a>ResourceRegistry</a> for a detailed
--   discussion.
withRegistry :: (IOLike m, HasCallStack) => (ResourceRegistry m -> m a) -> m a

-- | Resource key
--   
--   Resource keys are tied to a particular registry.
data ResourceKey m

-- | Allocate new resource
--   
--   The allocation function will be run with asynchronous exceptions
--   masked. This means that the resource allocation must either be fast or
--   else interruptible; see "Dealing with Asynchronous Exceptions during
--   Resource Acquisition" <a>http://www.well-typed.com/blog/97/</a> for
--   details.
allocate :: forall m a. (IOLike m, HasCallStack) => ResourceRegistry m -> (ResourceId -> m a) -> (a -> m ()) -> m (ResourceKey m, a)

-- | Generalization of <a>allocate</a> for allocation functions that may
--   fail
allocateEither :: forall m e a. (IOLike m, HasCallStack) => ResourceRegistry m -> (ResourceId -> m (Either e a)) -> (a -> m Bool) -> m (Either e (ResourceKey m, a))

-- | Release resource
--   
--   This deallocates the resource and removes it from the registry. It
--   will be the responsibility of the caller to make sure that the
--   resource is no longer used in any thread.
--   
--   The deallocation function is run with exceptions masked, so that we
--   are guaranteed not to remove the resource from the registry without
--   releasing it.
--   
--   Releasing an already released resource is a no-op.
--   
--   When the resource has not been released before, its context is
--   returned.
release :: (IOLike m, HasCallStack) => ResourceKey m -> m (Maybe (Context m))

-- | Release all resources in the <a>ResourceRegistry</a> without closing.
--   
--   See <a>closeRegistry</a> for more details.
releaseAll :: (IOLike m, HasCallStack) => ResourceRegistry m -> m ()

-- | Unsafe version of <a>release</a>
--   
--   The only difference between <a>release</a> and <a>unsafeRelease</a> is
--   that the latter does not insist that it is called from a thread that
--   is known to the registry. This is dangerous, because it implies that
--   there is a thread with access to a resource which may be deallocated
--   before that thread is terminated. Of course, we can't detect all such
--   situations (when the thread merely uses a resource but does not
--   allocate or release we can't tell), but normally when we <i>do</i>
--   detect this we throw an exception.
--   
--   This function should only be used if the above situation can be ruled
--   out or handled by other means.
unsafeRelease :: IOLike m => ResourceKey m -> m (Maybe (Context m))

-- | This is to <a>releaseAll</a> what <a>unsafeRelease</a> is to
--   <a>release</a>: we do not insist that this funciton is called from a
--   thread that is known to the registry. See <a>unsafeRelease</a> for why
--   this is dangerous.
unsafeReleaseAll :: (IOLike m, HasCallStack) => ResourceRegistry m -> m ()

-- | Cancel a thread
--   
--   This is a synchronous operation: the thread will have terminated when
--   this function returns.
--   
--   Uses <a>uninterruptibleCancel</a> because that's what <a>withAsync</a>
--   does.
cancelThread :: IOLike m => Thread m a -> m ()

-- | Fork a thread and link to it to the registry.
--   
--   This function is just a convenience.
forkLinkedThread :: (IOLike m, HasCallStack) => ResourceRegistry m -> String -> m a -> m (Thread m a)

-- | Fork a new thread
forkThread :: forall m a. (IOLike m, HasCallStack) => ResourceRegistry m -> String -> m a -> m (Thread m a)

-- | Link specified <a>Thread</a> to the (thread that created) the registry
linkToRegistry :: IOLike m => Thread m a -> m ()
threadId :: Thread m a -> ThreadId m

-- | Lift <a>waitAny</a> to <a>Thread</a>
waitAnyThread :: forall m a. IOLike m => [Thread m a] -> m a

-- | Wait for thread to terminate and return its result.
--   
--   If the thread throws an exception, this will rethrow that exception.
--   
--   NOTE: If A waits on B, and B is linked to the registry, and B throws
--   an exception, then A might <i>either</i> receive the exception thrown
--   by B <i>or</i> the <tt>ThreadKilled</tt> exception thrown by the
--   registry.
waitThread :: IOLike m => Thread m a -> m a

-- | Bracketed version of <a>forkThread</a>
--   
--   The analogue of <a>withAsync</a> for the registry.
--   
--   Scoping thread lifetime using <a>withThread</a> is important when a
--   parent thread wants to link to a child thread /and handle any
--   exceptions arising from the link/:
--   
--   <pre>
--   let handleLinkException :: ExceptionInLinkedThread -&gt; m ()
--       handleLinkException = ..
--   in handle handleLinkException $
--        withThread registry codeInChild $ \child -&gt;
--          ..
--   </pre>
--   
--   instead of
--   
--   <pre>
--   handle handleLinkException $ do  -- PROBABLY NOT CORRECT!
--     child &lt;- forkThread registry codeInChild
--     ..
--   </pre>
--   
--   where the parent may exit the scope of the exception handler before
--   the child terminates. If the lifetime of the child cannot be limited
--   to the lifetime of the parent, the child should probably be linked to
--   the registry instead and the thread that spawned the registry should
--   handle any exceptions.
--   
--   Note that in <i>principle</i> there is no problem in using
--   <tt>withAync</tt> alongside a registry. After all, in a pattern like
--   
--   <pre>
--   withRegistry $ \registry -&gt;
--     ..
--     withAsync (.. registry ..) $ \async -&gt;
--       ..
--   </pre>
--   
--   the async will be cancelled when leaving the scope of <a>withAsync</a>
--   and so that reference to the registry, or indeed any of the resources
--   inside the registry, is safe. However, the registry implements a
--   sanity check that the registry is only used from known threads. This
--   is useful: when a thread that is not known to the registry (in other
--   words, whose lifetime is not tied to the lifetime of the registry)
--   spawns a resource in that registry, that resource may well be
--   deallocated before the thread terminates, leading to undefined and
--   hard to debug behaviour (indeed, whether or not this results in
--   problems may well depend on precise timing); an exception that is
--   thrown when <i>allocating</i> the resource is (more) deterministic and
--   easier to debug. Unfortunately, it means that the above pattern is not
--   applicable, as the thread spawned by <a>withAsync</a> is not known to
--   the registry, and so if it were to try to use the registry, the
--   registry would throw an error (even though this pattern is actually
--   safe). This situation is not ideal, but for now we merely provide an
--   alternative to <a>withAsync</a> that <i>does</i> register the thread
--   with the registry.
--   
--   NOTE: Threads that are spawned out of the user's control but that must
--   still make use of the registry can use the unsafe API. This should be
--   used with caution, however.
withThread :: IOLike m => ResourceRegistry m -> String -> m a -> (Thread m a -> m b) -> m b

-- | Thread
--   
--   The internals of this type are not exported.
data Thread m a

-- | When <a>runWithTempRegistry</a> exits successfully while there are
--   still resources remaining in the temporary registry that haven't been
--   transferred to the final state.
data TempRegistryException
TempRegistryRemainingResource :: !Context m -> !Context m -> TempRegistryException

-- | The context in which the temporary registry was created.
[tempRegistryContext] :: TempRegistryException -> !Context m

-- | The context in which the resource was allocated that was not
--   transferred to the final state.
[tempRegistryResource] :: TempRegistryException -> !Context m

-- | Allocate a resource in a temporary registry until it has been
--   transferred to the final state <tt>st</tt>. See
--   <a>runWithTempRegistry</a> for more details.
allocateTemp :: (IOLike m, HasCallStack) => m a -> (a -> m Bool) -> (st -> a -> Bool) -> WithTempRegistry st m a

-- | Higher level API on top of <a>runWithTempRegistry</a>: modify the
--   given <tt>st</tt>, allocating resources in the process that will be
--   transferred to the returned <tt>st</tt>.
modifyWithTempRegistry :: forall m st a. IOLike m => m st -> (st -> ExitCase st -> m ()) -> StateT st (WithTempRegistry st m) a -> m a

-- | Embed a self-contained <a>WithTempRegistry</a> computation into a
--   larger one.
--   
--   The internal <a>WithTempRegistry</a> is effectively passed to
--   <a>runWithTempRegistry</a>. It therefore must have no dangling
--   resources, for example. This is the meaning of <i>self-contained</i>
--   above.
--   
--   The key difference beyond <a>runWithTempRegistry</a> is that the
--   resulting composite resource is also guaranteed to be registered in
--   the outer <a>WithTempRegistry</a> computation's registry once the
--   inner registry is closed. Combined with the following assumption, this
--   establishes the invariant that all resources are (transitively) in a
--   temporary registry.
--   
--   As the resource might require some implementation details to be
--   closed, the function to close it will also be provided by the inner
--   computation.
--   
--   ASSUMPTION: closing <tt>res</tt> closes every resource contained in
--   <tt>innerSt</tt>
--   
--   NOTE: In the current implementation, there will be a brief moment
--   where the inner registry still contains the inner computation's
--   resources and also the outer registry simultaneously contains the new
--   composite resource. If an async exception is received at that time,
--   then the inner resources will be closed and then the composite
--   resource will be closed. This means there's a risk of <i>double
--   freeing</i>, which can be harmless if anticipated.
runInnerWithTempRegistry :: forall innerSt st m res a. IOLike m => WithTempRegistry innerSt m (a, innerSt, res) -> (res -> m Bool) -> (st -> res -> Bool) -> WithTempRegistry st m a

-- | Run an action with a temporary resource registry.
--   
--   When allocating resources that are meant to end up in some final
--   state, e.g., stored in a <tt>TVar</tt>, after which they are
--   guaranteed to be released correctly, it is possible that an exception
--   is thrown after allocating such a resource, but before it was stored
--   in the final state. In that case, the resource would be leaked.
--   <a>runWithTempRegistry</a> solves that problem.
--   
--   When no exception is thrown before the end of
--   <a>runWithTempRegistry</a>, the user must have transferred all the
--   resources it allocated to their final state. This means that these
--   resources don't have to be released by the temporary registry anymore,
--   the final state is now in charge of releasing them.
--   
--   In case an exception is thrown before the end of
--   <a>runWithTempRegistry</a>, <i>all</i> resources allocated in the
--   temporary registry will be released.
--   
--   Resources must be allocated using <a>allocateTemp</a>.
--   
--   To make sure that the user doesn't forget to transfer a resource to
--   the final state <tt>st</tt>, the user must pass a function to
--   <a>allocateTemp</a> that checks whether a given <tt>st</tt> contains
--   the resource, i.e., whether the resource was successfully transferred
--   to its final destination.
--   
--   When no exception is thrown before the end of
--   <a>runWithTempRegistry</a>, we check whether all allocated resources
--   have been transferred to the final state <tt>st</tt>. If there's a
--   resource that hasn't been transferred to the final state <i>and</i>
--   that hasn't be released or closed before (see the release function
--   passed to <a>allocateTemp</a>), a <a>TempRegistryRemainingResource</a>
--   exception will be thrown.
--   
--   For that reason, <a>WithTempRegistry</a> is parameterised over the
--   final state type <tt>st</tt> and the given <a>WithTempRegistry</a>
--   action must return the final state.
--   
--   NOTE: we explicitly don't let <a>runWithTempRegistry</a> return the
--   final state, because the state <i>must</i> have been stored somewhere
--   safely, transferring the resources, before the temporary registry is
--   closed.
runWithTempRegistry :: (IOLike m, HasCallStack) => WithTempRegistry st m (a, st) -> m a

-- | An action with a temporary registry in scope, see
--   <a>runWithTempRegistry</a> for more details.
--   
--   The most important function to run in this monad is
--   <a>allocateTemp</a>.
data WithTempRegistry st m a

-- | Close the registry
--   
--   This can only be called from the same thread that created the
--   registry. This is a no-op if the registry is already closed.
--   
--   This entire function runs with exceptions masked, so that we are not
--   interrupted while we release all resources.
--   
--   Resources will be allocated from young to old, so that resources
--   allocated later can safely refer to resources created earlier.
--   
--   The release functions are run in the scope of an exception handler, so
--   that if releasing one resource throws an exception, we still attempt
--   to release the other resources. Should we catch an exception whilst we
--   close the registry, we will rethrow it after having attempted to
--   release all resources. If there is more than one, we will pick a
--   random one to rethrow, though we will prioritize asynchronous
--   exceptions over other exceptions. This may be important for exception
--   handlers that catch all-except-asynchronous exceptions.
closeRegistry :: (IOLike m, HasCallStack) => ResourceRegistry m -> m ()

-- | Number of currently allocated resources
--   
--   Primarily for the benefit of testing.
countResources :: IOLike m => ResourceRegistry m -> m Int

-- | Create a new registry
--   
--   You are strongly encouraged to use <a>withRegistry</a> instead.
--   Exported primarily for the benefit of tests.
unsafeNewRegistry :: (IOLike m, HasCallStack) => m (ResourceRegistry m)

-- | Resource registry
--   
--   Note on terminology: when thread A forks thread B, we will say that
--   thread A is the " parent " and thread B is the " child ". No further
--   relationship between the two threads is implied by this terminology.
--   In particular, note that the child may outlive the parent. We will use
--   "fork" and "spawn" interchangeably.
--   
--   <h1>Motivation</h1>
--   
--   Whenever we allocate resources, we must keep track of them so that we
--   can deallocate them when they are no longer required. The most
--   important tool we have to achieve this is <a>bracket</a>:
--   
--   <pre>
--   bracket allocateResource releaseResource $ \r -&gt;
--     .. use r ..
--   </pre>
--   
--   Often <a>bracket</a> comes in the guise of a with-style combinator
--   
--   <pre>
--   withResource $ \r -&gt;
--     .. use r ..
--   </pre>
--   
--   Where this pattern is applicable, it should be used and there is no
--   need to use the <a>ResourceRegistry</a>. However, <a>bracket</a>
--   introduces strict lexical scoping: the resource is available inside
--   the scope of the bracket, and will be deallocated once we leave that
--   scope. That pattern is sometimes hard to use.
--   
--   For example, suppose we have this interface to an SQL server
--   
--   <pre>
--   query :: Query -&gt; IO QueryHandle
--   close :: QueryHandle -&gt; IO ()
--   next  :: QueryHandle -&gt; IO Row
--   </pre>
--   
--   and suppose furthermore that we are writing a simple webserver that
--   allows a client to send multiple SQL queries, get rows from any open
--   query, and close queries when no longer required:
--   
--   <pre>
--   server :: IO ()
--   server = go Map.empty
--     where
--       go :: Map QueryId QueryHandle -&gt; IO ()
--       go handles = getRequest &gt;&gt;= \case
--           New q -&gt; do
--             h   &lt;- query q                        -- allocate
--             qId &lt;- generateQueryId
--             sendResponse qId
--             go $ Map.insert qId h handles
--           Close qId -&gt; do
--             close (handles ! qId)                 -- release
--             go $ Map.delete qId handles
--           Next qId -&gt; do
--             sendResponse =&lt;&lt; next (handles ! qId)
--             go handles
--   </pre>
--   
--   The server opens and closes query handles in response to client
--   requests. Restructuring this code to use <a>bracket</a> would be
--   awkward, but as it stands this code does not ensure that resources get
--   deallocated; for example, if the server thread is killed
--   (<a>killThread</a>), resources will be leaked.
--   
--   Another, perhaps simpler, example is spawning threads. Threads too
--   should be considered to be resources that we should keep track of and
--   deallocate when they are no longer required, primarily because when we
--   deallocate (terminate) those threads they too will have a chance to
--   deallocate <i>their</i> resources. As for other resources, we have a
--   with-style combinator for this
--   
--   <pre>
--   withAsync $ \thread -&gt; ..
--   </pre>
--   
--   Lexical scoping of threads is often inconvenient, however, more so
--   than for regular resources. The temptation is therefore to simply fork
--   a thread and forget about it, but if we are serious about resource
--   deallocation this is not an acceptable solution.
--   
--   <h1>The resource registry</h1>
--   
--   The resource registry is essentially a piece of state tracking which
--   resources have been allocated. The registry itself is allocated with a
--   with-style combinator <a>withRegistry</a>, and when we leave that
--   scope any resources not yet deallocated will be released at that
--   point. Typically the registry is only used as a fall-back, ensuring
--   that resources will deallocated even in the presence of exceptions.
--   For example, here's how we might rewrite the above server example
--   using a registry:
--   
--   <pre>
--   server' :: IO ()
--   server' =
--       withRegistry $ \registry -&gt; go registry Map.empty
--     where
--       go :: ResourceRegistry IO
--          -&gt; Map QueryId (ResourceKey, QueryHandle)
--          -&gt; IO ()
--       go registry handles = getRequest &gt;&gt;= \case
--           New q -&gt; do
--             (key, h) &lt;- allocate registry (query q) close  -- allocate
--             qId      &lt;- generateQueryId
--             sendResponse qId
--             go registry $ Map.insert qId (key, h) handles
--           Close qId -&gt; do
--             release registry (fst (handles ! qId))         -- release
--             go registry $ Map.delete qId handles
--           Next qId -&gt; do
--             sendResponse =&lt;&lt; next (snd (handles ! qId))
--             go registry handles
--   </pre>
--   
--   We allocate the query with the help of the registry, providing the
--   registry with the means to deallocate the query should that be
--   required. We can /and should/ still manually release resources also:
--   in this particular example, the (lexical) scope of the registry is the
--   entire server thread, so delaying releasing queries until we exit that
--   scope will probably mean we hold on to resources for too long. The
--   registry is only there as a fall-back.
--   
--   <h1>Spawning threads</h1>
--   
--   We already observed in the introduction that insisting on lexical
--   scoping for threads is often inconvenient, and that simply using
--   <tt>fork</tt> is no solution as it means we might leak resources.
--   There is however another problem with <tt>fork</tt>. Consider this
--   snippet:
--   
--   <pre>
--   withRegistry $ \registry -&gt;
--     r &lt;- allocate registry allocateResource releaseResource
--     fork $ .. use r ..
--   </pre>
--   
--   It is easy to see that this code is problematic: we allocate a
--   resource <tt>r</tt>, then spawn a thread that uses <tt>r</tt>, and
--   finally leave the scope of <a>withRegistry</a>, thereby deallocating
--   <tt>r</tt> -- leaving the thread to run with a now deallocated
--   resource.
--   
--   It is <i>only</i> safe for threads to use a given registry, and/or its
--   registered resources, if the lifetime of those threads is tied to the
--   lifetime of the registry. There would be no problem with the example
--   above if the thread would be terminated when we exit the scope of
--   <a>withRegistry</a>.
--   
--   The <a>forkThread</a> combinator provided by the registry therefore
--   does two things: it allocates the thread as a resource in the
--   registry, so that it can kill the thread when releasing all resources
--   in the registry. It also records the thread ID in a set of known
--   threads. Whenever the registry is accessed from a thread <i>not</i> in
--   this set, the registry throws a runtime exception, since such a thread
--   might outlive the registry and hence its contents. The intention is
--   that this guards against dangerous patterns like the one above.
--   
--   <h1>Linking</h1>
--   
--   When thread A spawns thread B using <a>withAsync</a>, the lifetime of
--   B is tied to the lifetime of A:
--   
--   <pre>
--   withAsync .. $ \threadB -&gt; ..
--   </pre>
--   
--   After all, when A exits the scope of the <a>withAsync</a>, thread B
--   will be killed. The reverse is however not true: thread B can
--   terminate before thread A. It is often useful for thread A to be able
--   to declare a dependency on thread B: if B somehow fails, that is,
--   terminates with an exception, we want that exception to be rethrown in
--   thread A as well. A can achieve this by <i>linking</i> to B:
--   
--   <pre>
--   withAsync .. $ \threadB -&gt; do
--     link threadB
--     ..
--   </pre>
--   
--   Linking a parent to a child is however of limited value if the
--   lifetime of the child is not limited by the lifetime of the parent.
--   For example, if A does
--   
--   <pre>
--   threadB &lt;- async $ ..
--   link threadB
--   </pre>
--   
--   and A terminates before B does, any exception thrown by B might be
--   send to a thread that no longer exists. This is particularly
--   problematic when we start chaining threads: if A spawns-and-links-to B
--   which spawns-and-links-to C, and C throws an exception, perhaps the
--   intention is that this gets rethrown to B, and then rethrown to A,
--   terminating all three threads; however, if B has terminated before the
--   exception is thrown, C will throw the exception to a non-existent
--   thread and A is never notified.
--   
--   For this reason, the registry's <a>linkToRegistry</a> combinator does
--   not link the specified thread to the thread calling
--   <a>linkToRegistry</a>, but rather to the thread that created the
--   registry. After all, the lifetime of threads spawned with
--   <a>forkThread</a> can certainly exceed the lifetime of their parent
--   threads, but the lifetime of <i>all</i> threads spawned using the
--   registry will be limited by the scope of that registry, and hence the
--   lifetime of the thread that created it. So, when we call
--   <a>linkToRegistry</a>, the exception will be thrown the thread that
--   created the registry, which (if not caught) will cause that that to
--   exit the scope of <a>withRegistry</a>, thereby terminating all threads
--   in that registry.
--   
--   <h1>Combining the registry and with-style allocation</h1>
--   
--   It is perfectly possible (indeed, advisable) to use <a>bracket</a> and
--   bracket-like allocation functions alongside the registry, but note
--   that the usual caveats with <a>bracket</a> and forking threads still
--   applies. In particular, spawning threads inside the <a>bracket</a>
--   that make use of the bracketed resource is problematic; this is of
--   course true whether or not a registry is used.
--   
--   In principle this also includes <a>withAsync</a>; however, since
--   <a>withAsync</a> results in a thread that is not known to the
--   registry, such a thread will not be able to use the registry (the
--   registry would throw an unknown thread exception, as described above).
--   For this purpose we provide <a>withThread</a>; <a>withThread</a> (as
--   opposed to <a>forkThread</a>) should be used when a parent thread
--   wants to handle exceptions in the child thread; see <a>withThread</a>
--   for detailed discussion.
--   
--   It is <i>also</i> fine to includes nested calls to
--   <a>withRegistry</a>. Since the lifetime of such a registry (and all
--   resources within) is tied to the thread calling <a>withRegistry</a>,
--   which itself is tied to the "parent registry" in which it was created,
--   this creates a hierarchy of registries. It is of course essential for
--   compositionality that we should be able to create local registries,
--   but even if we do have easy access to a parent regisry, creating a
--   local one where possibly is useful as it limits the scope of the
--   resources created within, and hence their maximum lifetimes.
data ResourceRegistry m
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Util.ResourceRegistry.Age
instance GHC.Classes.Ord Ouroboros.Consensus.Util.ResourceRegistry.Age
instance GHC.Classes.Eq Ouroboros.Consensus.Util.ResourceRegistry.Age
instance GHC.Show.Show Ouroboros.Consensus.Util.ResourceRegistry.Age
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.KnownThreads m)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Util.ResourceRegistry.RegistryStatus
instance GHC.Generics.Generic Ouroboros.Consensus.Util.ResourceRegistry.RegistryStatus
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Util.ResourceRegistry.ResourceId
instance GHC.Enum.Enum Ouroboros.Consensus.Util.ResourceRegistry.ResourceId
instance GHC.Classes.Ord Ouroboros.Consensus.Util.ResourceRegistry.ResourceId
instance GHC.Classes.Eq Ouroboros.Consensus.Util.ResourceRegistry.ResourceId
instance GHC.Show.Show Ouroboros.Consensus.Util.ResourceRegistry.ResourceId
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.Release m)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.TransferredTo st)
instance GHC.Base.Monoid (Ouroboros.Consensus.Util.ResourceRegistry.TransferredTo st)
instance GHC.Base.Semigroup (Ouroboros.Consensus.Util.ResourceRegistry.TransferredTo st)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.Resource m)
instance GHC.Generics.Generic (Ouroboros.Consensus.Util.ResourceRegistry.Resource m)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.RegistryState m)
instance GHC.Generics.Generic (Ouroboros.Consensus.Util.ResourceRegistry.RegistryState m)
instance GHC.Generics.Generic (Ouroboros.Consensus.Util.ResourceRegistry.ResourceRegistry m)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.Thread m a)
instance Control.Monad.Class.MonadThrow.MonadMask m => Control.Monad.Class.MonadThrow.MonadMask (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st m)
instance Control.Monad.Class.MonadThrow.MonadCatch m => Control.Monad.Class.MonadThrow.MonadCatch (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st m)
instance Control.Monad.Class.MonadThrow.MonadThrow m => Control.Monad.Class.MonadThrow.MonadThrow (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st m)
instance GHC.Base.Monad m => GHC.Base.Monad (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st m)
instance GHC.Base.Applicative m => GHC.Base.Applicative (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st m)
instance GHC.Base.Functor m => GHC.Base.Functor (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st m)
instance Ouroboros.Consensus.Util.IOLike.IOLike m => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.ResourceKey m)
instance GHC.Generics.Generic (Ouroboros.Consensus.Util.ResourceRegistry.ResourceKey m)
instance Ouroboros.Consensus.Util.IOLike.IOLike m => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.ResourceRegistry m)
instance GHC.Show.Show Ouroboros.Consensus.Util.ResourceRegistry.RegistryClosedException
instance GHC.Show.Show Ouroboros.Consensus.Util.ResourceRegistry.TempRegistryException
instance GHC.Show.Show Ouroboros.Consensus.Util.ResourceRegistry.ResourceRegistryThreadException
instance GHC.Show.Show (Ouroboros.Consensus.Util.ResourceRegistry.Context m)
instance Control.Monad.Trans.Class.MonadTrans (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st)
instance Control.Monad.State.Class.MonadState s m => Control.Monad.State.Class.MonadState s (Ouroboros.Consensus.Util.ResourceRegistry.WithTempRegistry st m)
instance GHC.Classes.Eq (Ouroboros.Consensus.Util.ResourceRegistry.Thread m a)
instance GHC.Exception.Type.Exception Ouroboros.Consensus.Util.ResourceRegistry.RegistryClosedException
instance GHC.Exception.Type.Exception Ouroboros.Consensus.Util.ResourceRegistry.TempRegistryException
instance GHC.Exception.Type.Exception Ouroboros.Consensus.Util.ResourceRegistry.ResourceRegistryThreadException
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.ResourceRegistry.Context m)
instance GHC.Show.Show (Ouroboros.Consensus.Util.ResourceRegistry.Release m)

module Ouroboros.Consensus.Storage.VolatileDB.Impl.State
data TraceEvent blk
DBAlreadyClosed :: TraceEvent blk
BlockAlreadyHere :: HeaderHash blk -> TraceEvent blk
Truncate :: ParseError blk -> FsPath -> BlockOffset -> TraceEvent blk
InvalidFileNames :: [FsPath] -> TraceEvent blk

-- | The offset at which a block is stored in a file.
newtype BlockOffset
BlockOffset :: Word64 -> BlockOffset
[unBlockOffset] :: BlockOffset -> Word64
newtype BlockSize
BlockSize :: Word32 -> BlockSize
[unBlockSize] :: BlockSize -> Word32

-- | The <a>FileId</a> is the unique identifier of each file found in the
--   db. For example, the file <tt>blocks-42.dat</tt> has <a>FileId</a>
--   <tt>42</tt>.
type FileId = Int
data InternalState blk h
DbClosed :: InternalState blk h
DbOpen :: !OpenState blk h -> InternalState blk h

-- | Internal state when the database is open.
data OpenState blk h
OpenState :: !Handle h -> !FsPath -> !FileId -> !Word64 -> !Index blk -> !ReverseIndex blk -> !SuccessorsIndex blk -> !MaxSlotNo -> OpenState blk h

-- | The only open file we append blocks to.
[currentWriteHandle] :: OpenState blk h -> !Handle h

-- | The path of the file above.
[currentWritePath] :: OpenState blk h -> !FsPath

-- | The <a>FileId</a> of the same file.
[currentWriteId] :: OpenState blk h -> !FileId

-- | The offset of the same file.
[currentWriteOffset] :: OpenState blk h -> !Word64

-- | The contents of each file.
[currentMap] :: OpenState blk h -> !Index blk

-- | Where to find each block based on its slot number.
[currentRevMap] :: OpenState blk h -> !ReverseIndex blk

-- | The successors for each block.
[currentSuccMap] :: OpenState blk h -> !SuccessorsIndex blk

-- | Highest stored SlotNo.
--   
--   INVARIANT: this is the cached value of: &gt; FileInfo.maxSlotNoInFiles
--   (Index.elems (currentMap st))
[currentMaxSlotNo] :: OpenState blk h -> !MaxSlotNo

-- | We map the header hash of each block to the <a>InternalBlockInfo</a>.
type ReverseIndex blk = Map (HeaderHash blk) (InternalBlockInfo blk)

-- | For each block, we store the set of all blocks which have this block
--   as a predecessor (set of successors).
type SuccessorsIndex blk = Map (ChainHash blk) (Set (HeaderHash blk))
data VolatileDBEnv m blk
VolatileDBEnv :: !HasFS m h -> !RAWLock m (InternalState blk h) -> !BlocksPerFile -> !blk -> Bool -> !CodecConfig blk -> !Tracer m (TraceEvent blk) -> VolatileDBEnv m blk
[hasFS] :: VolatileDBEnv m blk -> !HasFS m h
[varInternalState] :: VolatileDBEnv m blk -> !RAWLock m (InternalState blk h)
[maxBlocksPerFile] :: VolatileDBEnv m blk -> !BlocksPerFile
[checkIntegrity] :: VolatileDBEnv m blk -> !blk -> Bool
[codecConfig] :: VolatileDBEnv m blk -> !CodecConfig blk
[tracer] :: VolatileDBEnv m blk -> !Tracer m (TraceEvent blk)
dbIsOpen :: InternalState blk h -> Bool

-- | Shorthand
type ModifyOpenState m blk h = StateT (OpenState blk h) (WithTempRegistry (OpenState blk h) m)

-- | Append to the open state. Reads can happen concurrently with this
--   operation.
--   
--   NOTE: This is safe in terms of throwing FsErrors.
appendOpenState :: forall blk m a. (IOLike m, Typeable blk, StandardHash blk) => VolatileDBEnv m blk -> (forall h. Eq h => HasFS m h -> ModifyOpenState m blk h a) -> m a

-- | Close the handles in the <a>OpenState</a>.
--   
--   Idempotent, as closing a handle is idempotent.
--   
--   NOTE: does not wrap <a>FsError</a>s and must be called within
--   <a>wrapFsError</a> or <a>tryVolatileDB</a>.
closeOpenHandles :: HasFS m h -> OpenState blk h -> m ()
mkOpenState :: forall m blk h. (HasCallStack, IOLike m, GetPrevHash blk, HasBinaryBlockInfo blk, HasNestedContent Header blk, DecodeDisk blk (ByteString -> blk), Eq h) => CodecConfig blk -> HasFS m h -> (blk -> Bool) -> BlockValidationPolicy -> Tracer m (TraceEvent blk) -> BlocksPerFile -> WithTempRegistry (OpenState blk h) m (OpenState blk h)

-- | Perform an action that accesses the internal state of an open
--   database.
--   
--   In case the database is closed, a <a>ClosedDBError</a> is thrown.
--   
--   In case an <a>VolatileDBError</a> is thrown while the action is being
--   run, the database is closed to prevent further appending to a database
--   in a potentially inconsistent state. All other exceptions will leave
--   the database open.
withOpenState :: forall blk m r. (IOLike m, StandardHash blk, Typeable blk) => VolatileDBEnv m blk -> (forall h. HasFS m h -> OpenState blk h -> m r) -> m r

-- | Write to the open state. No reads or appends can concurrently with
--   this operation.
--   
--   NOTE: This is safe in terms of throwing FsErrors.
writeOpenState :: forall blk m a. (IOLike m, Typeable blk, StandardHash blk) => VolatileDBEnv m blk -> (forall h. Eq h => HasFS m h -> ModifyOpenState m blk h a) -> m a
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.VolatileDB.Impl.State.OpenState blk h)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.VolatileDB.Impl.State.OpenState blk h)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.VolatileDB.Impl.State.InternalState blk h)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.VolatileDB.Impl.State.InternalState blk h)


-- | Volatile on-disk database of blocks
--   
--   <h1>Logic</h1>
--   
--   The VolatileDB is a key-value store of blocks indexed by their hashes.
--   It is parameterised by the block type <tt>blk</tt>.
--   
--   The "volatile" in the name refers to the fact that the blocks stored
--   in it make up the <i>volatile</i> part of the chain, i.e., the last
--   <tt>k</tt> blocks of the chain, which can still be rolled back. Not
--   only the last <tt>k</tt> blocks of the current chain are stored in
--   this database, but also blocks of forks which we have switched from or
--   will switch to.
--   
--   The VolatileDB appends new blocks sequentially to a file. When
--   <a>volMaxBlocksPerFile</a> are stored in the current file, a new file
--   is started.
--   
--   The VolatileDB provides four main operations:
--   
--   <ol>
--   <li>Adding blocks with <a>putBlock</a></li>
--   <li>Get blocks or information about them with
--   <a>getBlockComponent</a></li>
--   <li>Accessing the in-memory indices using <a>getBlockInfo</a> and
--   <a>filterByPredecessor</a></li>
--   <li>Garbage collecting blocks older than a given slot using
--   <a>garbageCollect</a></li>
--   </ol>
--   
--   Garbage collection will only delete a file from the VolatileDB when
--   all blocks in it have a slot older than the one passed to
--   <a>garbageCollect</a>.
--   
--   <h1>Errors</h1>
--   
--   When an exception occurs while modifying the VolatileDB, we close the
--   database as a safety measure, e.g., in case a file could not be
--   written to disk, as we can no longer make sure the in-memory indices
--   match what's stored on the file system. When reopening, we validate
--   the blocks stored in the file system and reconstruct the in-memory
--   indices.
--   
--   NOTE: this means that when a thread modifying the VolatileDB is
--   killed, the database will be closed. This is an intentional choice to
--   simplify things.
--   
--   The in-memory indices can always be reconstructed from the file
--   system. This is important, as we must be resilient against unexpected
--   shutdowns, power losses, etc.
--   
--   We achieve this by only performing basic operations on the VolatileDB:
--   * <a>putBlock</a> only appends a new block to a file. Losing an update
--   means we only lose a block, which is not a problem, it can be
--   redownloaded. * <a>garbageCollect</a> only deletes entire files. *
--   There is no operation that modifies a file in-place. This means we do
--   not have to keep any rollback journals to make sure we are safe in
--   case of unexpected shutdowns.
--   
--   We only throw <a>VolatileDBError</a>. File-system errors are caught,
--   wrapped in a <a>VolatileDBError</a>, and rethrown. We make sure that
--   all calls to <a>HasFS</a> functions are properly wrapped. This
--   wrapping is automatically done when inside the scope of
--   <tt>modifyOpenState</tt> and <a>withOpenState</a>. Otherwise, we use
--   <a>wrapFsError</a>.
--   
--   <h1>Concurrency</h1>
--   
--   A single folder should only be used by a single VolatileDB. Naturally,
--   a VolatileDB can be accessed concurrently by multiple threads.
--   
--   <h1>File-system layout:</h1>
--   
--   The on-disk representation is as follows:
--   
--   <pre>
--   dbFolder/
--     blocks-0.dat
--     blocks-1.dat
--     ...
--   </pre>
--   
--   Files not fitting the naming scheme are ignored. The numbering of
--   these files does not correlate to the blocks stored in them.
--   
--   Each file stores a fixed number of blocks, specified by
--   <a>volMaxBlocksPerFile</a>. When opening the VolatileDB, it will start
--   appending to the file with the highest number that is not yet full. If
--   all are full or none exist, a new file will be created.
--   
--   There is an implicit ordering of block files, which is NOT
--   alpharithmetic. For example, <tt>blocks-20.dat</tt> &lt;
--   <tt>blocks-100.dat</tt>.
--   
--   <h1>Recovery</h1>
--   
--   The VolatileDB will always try to recover to a consistent state even
--   if this means deleting all of its contents. In order to achieve this,
--   it truncates the files containing blocks if some blocks fail to parse,
--   are invalid, or are duplicated.
module Ouroboros.Consensus.Storage.VolatileDB.Impl
data VolatileDbArgs f m blk
VolatileDbArgs :: HKD f (blk -> Bool) -> HKD f (CodecConfig blk) -> SomeHasFS m -> BlocksPerFile -> Tracer m (TraceEvent blk) -> BlockValidationPolicy -> VolatileDbArgs f m blk
[volCheckIntegrity] :: VolatileDbArgs f m blk -> HKD f (blk -> Bool)
[volCodecConfig] :: VolatileDbArgs f m blk -> HKD f (CodecConfig blk)
[volHasFS] :: VolatileDbArgs f m blk -> SomeHasFS m
[volMaxBlocksPerFile] :: VolatileDbArgs f m blk -> BlocksPerFile
[volTracer] :: VolatileDbArgs f m blk -> Tracer m (TraceEvent blk)
[volValidationPolicy] :: VolatileDbArgs f m blk -> BlockValidationPolicy

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   VolatileDB.
type VolatileDbSerialiseConstraints blk = (EncodeDisk blk blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, HasNestedContent Header blk, HasBinaryBlockInfo blk)

-- | Default arguments
defaultArgs :: Applicative m => SomeHasFS m -> VolatileDbArgs Defaults m blk
openDB :: forall m blk ans. (HasCallStack, IOLike m, GetPrevHash blk, VolatileDbSerialiseConstraints blk) => VolatileDbArgs Identity m blk -> (forall st. WithTempRegistry st m (VolatileDB m blk, st) -> ans) -> ans

-- | When block validation is enabled, the parser checks for each block a
--   number of properties and stops parsing if it finds any invalid blocks.
data BlockValidationPolicy
NoValidation :: BlockValidationPolicy
ValidateAll :: BlockValidationPolicy

-- | The maximum number of blocks to store per file.
data BlocksPerFile

-- | Note that we recover from the error, and thus never throw it as an
--   <tt>Exception</tt>.
--   
--   Defined here instead of in the <tt>Parser</tt> module because
--   <a>TraceEvent</a> depends on it.
data ParseError blk

-- | A block could not be parsed.
BlockReadErr :: ReadIncrementalErr -> ParseError blk

-- | A block was corrupted, e.g., checking its signature and/or hash
--   failed.
BlockCorruptedErr :: HeaderHash blk -> ParseError blk

-- | A block with the same hash occurred twice in the VolatileDB files.
--   
--   We include the file in which it occurred first and the file in which
--   it occured the second time. The two files can be the same.
DuplicatedBlock :: HeaderHash blk -> FsPath -> FsPath -> ParseError blk
data TraceEvent blk
DBAlreadyClosed :: TraceEvent blk
BlockAlreadyHere :: HeaderHash blk -> TraceEvent blk
Truncate :: ParseError blk -> FsPath -> BlockOffset -> TraceEvent blk
InvalidFileNames :: [FsPath] -> TraceEvent blk
extractBlockInfo :: (GetPrevHash blk, HasBinaryBlockInfo blk) => blk -> BlockInfo blk

-- | Create a <a>BlocksPerFile</a>.
--   
--   PRECONDITION: the given number must be greater than 0, if not, this
--   function will throw an <a>error</a>.
mkBlocksPerFile :: Word32 -> BlocksPerFile

module Ouroboros.Consensus.Storage.VolatileDB

module Ouroboros.Consensus.Storage.ImmutableDB.API

-- | API for the <a>ImmutableDB</a>.
--   
--   The <a>ImmutableDB</a> stores blocks in <a>SlotNo</a>s. Nevertheless,
--   lookups use <a>RealPoint</a>, primarily because Epoch Boundary Blocks
--   (EBBs) have the same <a>SlotNo</a> as the regular block after them
--   (unless that slot is empty), so that we have to use the hash of the
--   block to distinguish the two (hence <a>RealPoint</a>). But also to
--   avoid reading the wrong block, i.e., when we expect a block with a
--   different hash.
--   
--   The database is append-only, so you cannot append a block to a slot in
--   the past. You can, however, skip slots, e.g., append to slot 0 and
--   then to slot 5, but afterwards, you can no longer append to slots 1-4.
--   You can only store at most one block in each slot, except for EBBs,
--   which are stored separately, at the start of each epoch/chunk.
--   
--   The block stored in a slot can be queried with
--   <a>getBlockComponent</a>. Block components can also be streamed using
--   <a>Iterator</a>s, see <a>stream</a>.
--   
--   The <a>Tip</a> of the database can be queried with <a>getTip</a>. This
--   tip will always point to a filled slot or an EBB that is present.
--   
--   The database can be explicitly closed, but can also be automatically
--   closed in case of an <a>ImmutableDBError</a>.
data ImmutableDB m blk
ImmutableDB :: (HasCallStack => m ()) -> (HasCallStack => STM m (WithOrigin (Tip blk))) -> (forall b. HasCallStack => BlockComponent blk b -> RealPoint blk -> m (Either (MissingBlock blk) b)) -> (HasCallStack => blk -> m ()) -> (forall b. HasCallStack => ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (MissingBlock blk) (Iterator m blk b))) -> ImmutableDB m blk

-- | Close the database.
--   
--   Idempotent.
--   
--   <b>Note</b>: Use <a>withDB</a> instead of this function.
[closeDB_] :: ImmutableDB m blk -> HasCallStack => m ()

-- | Return the tip of the database.
--   
--   The tip of the database will never point to an unfilled slot or
--   missing EBB.
--   
--   Throws a <a>ClosedDBError</a> if the database is closed.
[getTip_] :: ImmutableDB m blk -> HasCallStack => STM m (WithOrigin (Tip blk))

-- | Get the block component of the block with the given <a>Point</a>.
--   
--   The hash of the point is used to distinguish a potential EBB from the
--   regular block in the same slot.
--   
--   Returns a <a>MissingBlockError</a> if no block was stored with the
--   given <a>Point</a>, either because the slot was empty or because the
--   block stored with that slot had a different hash.
--   
--   Throws a <a>ClosedDBError</a> if the database is closed.
[getBlockComponent_] :: ImmutableDB m blk -> forall b. HasCallStack => BlockComponent blk b -> RealPoint blk -> m (Either (MissingBlock blk) b)

-- | Appends a block to the ImmutableDB.
--   
--   Throws an <a>AppendBlockNotNewerThanTipError</a> if the given slot is
--   &lt;= the result of <a>getTip</a>.
--   
--   Throws a <a>ClosedDBError</a> if the database is closed.
[appendBlock_] :: ImmutableDB m blk -> HasCallStack => blk -> m ()

-- | Return an <a>Iterator</a> to efficiently stream blocks from the
--   ImmutableDB.
--   
--   Throws an <a>InvalidIteratorRangeError</a> if the start of the range
--   is greater than the end of the range.
--   
--   NOTE: <a>MissingBlock</a> is returned, but
--   <a>InvalidIteratorRangeError</a> is thrown. This is because the former
--   is expected to occur during normal operation: a node serving blocks
--   might get requests to stream blocks that are not in the database. The
--   latter exception indicates incorrect usage and should not happen
--   during normal operation.
--   
--   Throws a <a>ClosedDBError</a> if the database is closed.
--   
--   The iterator is automatically closed when exhausted, and can be
--   prematurely closed with <a>iteratorClose</a>.
[stream_] :: ImmutableDB m blk -> forall b. HasCallStack => ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (MissingBlock blk) (Iterator m blk b))

-- | An <a>Iterator</a> is a handle which can be used to efficiently stream
--   block components from the ImmutableDB.
data Iterator m blk b
Iterator :: (HasCallStack => m (IteratorResult b)) -> (HasCallStack => STM m (Maybe (RealPoint blk))) -> (HasCallStack => m ()) -> Iterator m blk b

-- | Steps an <a>Iterator</a> yielding an <a>IteratorResult</a>.
--   
--   After returning the block component as an <a>IteratorResult</a>, the
--   iterator is advanced to the next non-empty slot or non-empty EBB.
--   
--   Throws a <a>ClosedDBError</a> if the database is closed.
--   
--   The iterator is automatically closed when exhausted
--   (<a>IteratorExhausted</a>), and can be prematurely closed with
--   <a>iteratorClose</a>.
[iteratorNext] :: Iterator m blk b -> HasCallStack => m (IteratorResult b)

-- | Return the point of the next block to stream, if there is one. Return
--   <a>Nothing</a> if not.
--   
--   This operation is idempotent.
[iteratorHasNext] :: Iterator m blk b -> HasCallStack => STM m (Maybe (RealPoint blk))

-- | Dispose of the <a>Iterator</a> by closing any open handles.
--   
--   Idempotent operation.
[iteratorClose] :: Iterator m blk b -> HasCallStack => m ()

-- | The result of stepping an <a>Iterator</a>.
data IteratorResult b
IteratorExhausted :: IteratorResult b
IteratorResult :: b -> IteratorResult b

-- | Consume an <a>Iterator</a> by stepping until it is exhausted. A list
--   of all the <a>IteratorResult</a>s (excluding the final
--   <a>IteratorExhausted</a>) produced by the <a>Iterator</a> is returned.
iteratorToList :: (HasCallStack, Monad m) => Iterator m blk b -> m [b]

-- | Variant of <a>traverse</a> instantiated to <tt><a>Iterator</a> m blk
--   m</tt> that executes the monadic function when calling
--   <a>iteratorNext</a>.
traverseIterator :: Monad m => (b -> m b') -> Iterator m blk b -> Iterator m blk b'

-- | newtype with an <a>Ord</a> instance that only uses <a>tipSlotNo</a>
--   and <a>tipIsEBB</a> and ignores the other fields.
newtype CompareTip blk
CompareTip :: Tip blk -> CompareTip blk
[getCompareTip] :: CompareTip blk -> Tip blk

-- | Information about the tip of the ImmutableDB.
data Tip blk
Tip :: !SlotNo -> !IsEBB -> !BlockNo -> !HeaderHash blk -> Tip blk
[tipSlotNo] :: Tip blk -> !SlotNo
[tipIsEBB] :: Tip blk -> !IsEBB
[tipBlockNo] :: Tip blk -> !BlockNo
[tipHash] :: Tip blk -> !HeaderHash blk
blockToTip :: (HasHeader blk, GetHeader blk) => blk -> Tip blk
tipToAnchor :: WithOrigin (Tip blk) -> Anchor blk
tipToPoint :: WithOrigin (Tip blk) -> Point blk
tipToRealPoint :: Tip blk -> RealPoint blk
data ApiMisuse blk

-- | When trying to append a new block, it was not newer than the current
--   tip, i.e., the slot was older than or equal to the current tip's slot.
--   
--   The <a>RealPoint</a> corresponds to the new block and the <a>Point</a>
--   to the current tip.
AppendBlockNotNewerThanTipError :: RealPoint blk -> Point blk -> ApiMisuse blk

-- | When the chosen iterator range was invalid, i.e. the <tt>start</tt>
--   (first parameter) came after the <tt>end</tt> (second parameter).
InvalidIteratorRangeError :: StreamFrom blk -> StreamTo blk -> ApiMisuse blk

-- | When performing an operation on a closed DB that is only allowed when
--   the database is open.
ClosedDBError :: ApiMisuse blk

-- | When performing an operation on an open DB that is only allowed when
--   the database is closed.
OpenDBError :: ApiMisuse blk

-- | Errors that might arise when working with this database.
data ImmutableDBError blk

-- | An error thrown because of incorrect usage of the immutable database
--   by the user.
ApiMisuse :: ApiMisuse blk -> PrettyCallStack -> ImmutableDBError blk

-- | An unexpected error thrown because something went wrong on a lower
--   layer.
UnexpectedFailure :: UnexpectedFailure blk -> ImmutableDBError blk

-- | This type can be part of an exception, but also returned as part of an
--   <a>Either</a>, because it can be expected in some cases.
data MissingBlock blk

-- | There is no block in the slot of the given point.
EmptySlot :: RealPoint blk -> MissingBlock blk

-- | The block and/or EBB in the slot of the given point have a different
--   hash. We return the <a>HeaderHash</a> for each block we found with the
--   corresponding slot number.
WrongHash :: RealPoint blk -> NonEmpty (HeaderHash blk) -> MissingBlock blk

-- | The requested point is in the future, i.e., its slot is greater than
--   that of the tip. We record the tip as the second argument.
NewerThanTip :: RealPoint blk -> Point blk -> MissingBlock blk
data UnexpectedFailure blk

-- | An IO operation on the file-system threw an error.
FileSystemError :: FsError -> UnexpectedFailure blk

-- | When loading an epoch or index file, its contents did not pass
--   validation.
InvalidFileError :: FsPath -> String -> PrettyCallStack -> UnexpectedFailure blk

-- | A missing epoch or index file.
MissingFileError :: FsPath -> PrettyCallStack -> UnexpectedFailure blk

-- | There was a checksum mismatch when reading the block with the given
--   point. The first <a>CRC</a> is the expected one, the second one the
--   actual one.
ChecksumMismatchError :: RealPoint blk -> CRC -> CRC -> FsPath -> PrettyCallStack -> UnexpectedFailure blk

-- | A block failed to parse
ParseError :: FsPath -> RealPoint blk -> DeserialiseFailure -> UnexpectedFailure blk

-- | When parsing a block we got some trailing data
TrailingDataError :: FsPath -> RealPoint blk -> ByteString -> UnexpectedFailure blk

-- | Block missing
--   
--   This exception gets thrown when a block that we <i>know</i> it should
--   be in the ImmutableDB, nonetheless was not found.
MissingBlockError :: MissingBlock blk -> UnexpectedFailure blk

-- | A (parsed) block did not pass the integrity check.
--   
--   This exception gets thrown when a block doesn't pass the integrity
--   check done for <a>GetVerifiedBlock</a>.
--   
--   NOTE: we do not check the integrity of a block when it is added to the
--   ImmutableDB. While this exception typically means the block has been
--   corrupted, it could also mean the block didn't pass the check at the
--   time it was added.
CorruptBlockError :: RealPoint blk -> UnexpectedFailure blk

-- | Return the <a>RealPoint</a> of the block that was missing.
missingBlockPoint :: MissingBlock blk -> RealPoint blk
throwApiMisuse :: (MonadThrow m, HasCallStack, StandardHash blk, Typeable blk) => ApiMisuse blk -> m a
throwUnexpectedFailure :: (StandardHash blk, Typeable blk, MonadThrow m) => UnexpectedFailure blk -> m a
appendBlock :: HasCallStack => ImmutableDB m blk -> blk -> m ()
closeDB :: HasCallStack => ImmutableDB m blk -> m ()
getBlockComponent :: HasCallStack => ImmutableDB m blk -> BlockComponent blk b -> RealPoint blk -> m (Either (MissingBlock blk) b)
getTip :: HasCallStack => ImmutableDB m blk -> STM m (WithOrigin (Tip blk))
stream :: HasCallStack => ImmutableDB m blk -> ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (MissingBlock blk) (Iterator m blk b))
getKnownBlockComponent :: (MonadThrow m, HasHeader blk) => ImmutableDB m blk -> BlockComponent blk b -> RealPoint blk -> m b
getTipAnchor :: (MonadSTM m, HasCallStack) => ImmutableDB m blk -> STM m (Anchor blk)
getTipPoint :: (MonadSTM m, HasCallStack) => ImmutableDB m blk -> STM m (Point blk)
getTipSlot :: (MonadSTM m, HasCallStack) => ImmutableDB m blk -> STM m (WithOrigin SlotNo)
hasBlock :: (MonadSTM m, HasCallStack) => ImmutableDB m blk -> RealPoint blk -> m Bool

-- | Variant of <a>streamAfterPoint</a> that throws a
--   <a>MissingBlockError</a> when the point is not in the ImmutableDB (or
--   genesis).
streamAfterKnownPoint :: (MonadSTM m, MonadThrow m, HasHeader blk, HasCallStack) => ImmutableDB m blk -> ResourceRegistry m -> BlockComponent blk b -> Point blk -> m (Iterator m blk b)

-- | Open an iterator with the given point as lower exclusive bound and the
--   current tip as the inclusive upper bound.
--   
--   Returns a <a>MissingBlock</a> when the point is not in the
--   ImmutableDB.
streamAfterPoint :: (MonadSTM m, HasHeader blk, HasCallStack) => ImmutableDB m blk -> ResourceRegistry m -> BlockComponent blk b -> Point blk -> m (Either (MissingBlock blk) (Iterator m blk b))
streamAll :: (MonadSTM m, MonadThrow m, HasHeader blk, HasCallStack) => ImmutableDB m blk -> ResourceRegistry m -> BlockComponent blk b -> m (Iterator m blk b)

-- | Open the database using the given function, perform the given action
--   using the database, and closes the database using its <a>closeDB</a>
--   function, in case of success or when an exception was raised.
withDB :: (HasCallStack, MonadThrow m) => m (ImmutableDB m blk) -> (ImmutableDB m blk -> m a) -> m a
instance Data.Traversable.Traversable Ouroboros.Consensus.Storage.ImmutableDB.API.IteratorResult
instance Data.Foldable.Foldable Ouroboros.Consensus.Storage.ImmutableDB.API.IteratorResult
instance GHC.Base.Functor Ouroboros.Consensus.Storage.ImmutableDB.API.IteratorResult
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.API.IteratorResult b)
instance GHC.Classes.Eq b => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.API.IteratorResult b)
instance GHC.Show.Show b => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.API.IteratorResult b)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.API.Iterator m blk b)
instance GHC.Base.Functor m => GHC.Base.Functor (Ouroboros.Consensus.Storage.ImmutableDB.API.Iterator m blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.API.Tip blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.API.MissingBlock blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.API.MissingBlock blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.API.MissingBlock blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.API.ImmutableDBError blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.API.ImmutableDBError blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.API.ImmutableDB m blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.API.Tip blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.API.Tip blk)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.API.Tip blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.API.ApiMisuse blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.API.UnexpectedFailure blk)
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => GHC.Exception.Type.Exception (Ouroboros.Consensus.Storage.ImmutableDB.API.ImmutableDBError blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.API.CompareTip blk)
instance GHC.Classes.Ord (Ouroboros.Consensus.Storage.ImmutableDB.API.CompareTip blk)

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Util

-- | Useful when you have exactly two values of some type and want to
--   <a>traverse</a> over both of them (which is not possible with a
--   tuple).
data Two a
Two :: a -> a -> Two a

-- | Check whether the given checksums match. If not, throw a
--   <a>ChecksumMismatchError</a>.
checkChecksum :: (HasCallStack, HasHeader blk, MonadThrow m) => FsPath -> RealPoint blk -> CRC -> CRC -> m ()

-- | Go through all files, making three sets: the set of chunk files,
--   primary index files, and secondary index files, discarding all others.
dbFilesOnDisk :: Set String -> (Set ChunkNo, Set ChunkNo, Set ChunkNo)
fsPathChunkFile :: ChunkNo -> FsPath
fsPathPrimaryIndexFile :: ChunkNo -> FsPath
fsPathSecondaryIndexFile :: ChunkNo -> FsPath

-- | Parse the prefix and chunk number from the filename of an index or
--   chunk file.
--   
--   <pre>
--   parseDBFile "00001.chunk"
--   </pre>
--   
--   Just ("chunk", 1) &gt; parseDBFile "00012.primary" Just ("primary",
--   12)
parseDBFile :: String -> Maybe (String, ChunkNo)

-- | Remove all chunk and index starting from the given chunk (included).
removeFilesStartingFrom :: (HasCallStack, Monad m) => HasFS m h -> ChunkNo -> m ()

-- | Opposite of <a>parseDBFile</a>.
renderFile :: Text -> ChunkNo -> FsPath

-- | Wrapper around <a>runGetOrFail</a> that throws an
--   <a>InvalidFileError</a> when it failed or when there was unconsumed
--   input.
runGet :: forall blk a m. (HasCallStack, MonadThrow m, StandardHash blk, Typeable blk) => Proxy blk -> FsPath -> Get a -> ByteString -> m a

-- | Same as <a>runGet</a>, but allows unconsumed input and returns it.
runGetWithUnconsumed :: forall blk a m. (HasCallStack, MonadThrow m, StandardHash blk, Typeable blk) => Proxy blk -> FsPath -> Get a -> ByteString -> m (ByteString, a)

-- | Execute an action and catch the <a>ImmutableDBError</a> and
--   <a>FsError</a> that can be thrown by it, and wrap the <a>FsError</a>
--   in an <a>ImmutableDBError</a> using the <a>FileSystemError</a>
--   constructor.
--   
--   This should be used whenever you want to run an action on the
--   ImmutableDB and catch the <a>ImmutableDBError</a> and the
--   <a>FsError</a> (wrapped in the former) it may thrown.
tryImmutableDB :: forall m blk a. (MonadCatch m, StandardHash blk, Typeable blk) => Proxy blk -> m a -> m (Either (ImmutableDBError blk) a)

-- | Rewrap <a>FsError</a> in a <a>ImmutableDBError</a>.
wrapFsError :: forall blk m a. (MonadCatch m, StandardHash blk, Typeable blk) => Proxy blk -> m a -> m a
instance Data.Traversable.Traversable Ouroboros.Consensus.Storage.ImmutableDB.Impl.Util.Two
instance Data.Foldable.Foldable Ouroboros.Consensus.Storage.ImmutableDB.Impl.Util.Two
instance GHC.Base.Functor Ouroboros.Consensus.Storage.ImmutableDB.Impl.Util.Two

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types
data BlockOrEBB
Block :: !SlotNo -> BlockOrEBB
EBB :: !EpochNo -> BlockOrEBB
data WithBlockSize a
WithBlockSize :: !Word32 -> !a -> WithBlockSize a
[blockSize] :: WithBlockSize a -> !Word32
[withoutBlockSize] :: WithBlockSize a -> !a
isBlockOrEBB :: BlockOrEBB -> IsEBB

-- | The validation policy used when opening an <a>ImmutableDB</a>.
--   
--   The validation policy is used by <a>openDB</a>: the initial opening of
--   the database, either an empty database or a database that was
--   previously closed.
--   
--   The recovery policy dictates which on-disk files should be validated.
data ValidationPolicy

-- | The chunk and index files of the most recent chunk stored on disk will
--   be validated.
--   
--   Prior chunk and index files are ignored, even their presence will not
--   be checked.
--   
--   A <tt>MissingFileError</tt> or an <tt>InvalidFileError</tt> will be
--   thrown in case of a missing or invalid chunk file, or an invalid index
--   file.
--   
--   Because not all files are validated, subsequent operations on the
--   database after opening may result in unexpected errors.
ValidateMostRecentChunk :: ValidationPolicy

-- | The chunk and index files of all chunks starting from the first one up
--   to the last chunk stored on disk will be validated.
--   
--   A <tt>MissingFileError</tt> or an <tt>InvalidFileError</tt> will be
--   thrown in case of a missing or invalid chunk file, or an invalid index
--   file.
ValidateAllChunks :: ValidationPolicy

-- | Defined here instead of in the <tt>Parser</tt> module because
--   <a>TraceEvent</a> depends on it.
data ChunkFileError blk

-- | A block could not be decoded
ChunkErrRead :: ReadIncrementalErr -> ChunkFileError blk

-- | The previous hash of a block did not match the hash of the previous
--   block.
ChunkErrHashMismatch :: HeaderHash blk -> ChainHash blk -> ChunkFileError blk

-- | The integrity verification of the block with the given point returned
--   <a>False</a>, indicating that the block got corrupted.
ChunkErrCorrupt :: Point blk -> ChunkFileError blk

-- | The argument with type <a>Word32</a> is the number of past chunk
--   currently in the cache.
data TraceCacheEvent
TraceCurrentChunkHit :: ChunkNo -> Word32 -> TraceCacheEvent
TracePastChunkHit :: ChunkNo -> Word32 -> TraceCacheEvent
TracePastChunkMiss :: ChunkNo -> Word32 -> TraceCacheEvent

-- | The least recently used past chunk was evicted because the cache was
--   full.
TracePastChunkEvict :: ChunkNo -> Word32 -> TraceCacheEvent

-- | Past chunks were expired from the cache because they haven't been used
--   for a while.
TracePastChunksExpired :: [ChunkNo] -> Word32 -> TraceCacheEvent
data TraceChunkValidation blk validateTo
StartedValidatingChunk :: ChunkNo -> validateTo -> TraceChunkValidation blk validateTo
ValidatedChunk :: ChunkNo -> validateTo -> TraceChunkValidation blk validateTo
MissingChunkFile :: ChunkNo -> TraceChunkValidation blk validateTo
InvalidChunkFile :: ChunkNo -> ChunkFileError blk -> TraceChunkValidation blk validateTo
MissingPrimaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
MissingSecondaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
InvalidPrimaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
InvalidSecondaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
RewritePrimaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
RewriteSecondaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
data TraceEvent blk
NoValidLastLocation :: TraceEvent blk
ValidatedLastLocation :: ChunkNo -> Tip blk -> TraceEvent blk
ChunkValidationEvent :: TraceChunkValidation blk ChunkNo -> TraceEvent blk

-- | The hash of the last block in the previous epoch doesn't match the
--   previous hash of the first block in the current epoch
ChunkFileDoesntFit :: ChainHash blk -> ChainHash blk -> TraceEvent blk

-- | Performing a migration of the on-disk files
Migrating :: Text -> TraceEvent blk
DeletingAfter :: WithOrigin (Tip blk) -> TraceEvent blk
DBAlreadyClosed :: TraceEvent blk
DBClosed :: TraceEvent blk
TraceCacheEvent :: !TraceCacheEvent -> TraceEvent blk
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.BlockOrEBB
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.BlockOrEBB
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.BlockOrEBB
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.BlockOrEBB
instance Data.Traversable.Traversable Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.WithBlockSize
instance Data.Foldable.Foldable Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.WithBlockSize
instance GHC.Base.Functor Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.WithBlockSize
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.WithBlockSize a)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.WithBlockSize a)
instance GHC.Show.Show a => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.WithBlockSize a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.WithBlockSize a)
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.ValidationPolicy
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.ValidationPolicy
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.ValidationPolicy
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.ChunkFileError blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.ChunkFileError blk)
instance GHC.Base.Functor (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceChunkValidation blk)
instance (Ouroboros.Network.Block.StandardHash blk, GHC.Show.Show validateTo) => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceChunkValidation blk validateTo)
instance (Ouroboros.Network.Block.StandardHash blk, GHC.Classes.Eq validateTo) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceChunkValidation blk validateTo)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceChunkValidation blk validateTo)
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceCacheEvent
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceCacheEvent
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceCacheEvent
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Types.TraceEvent blk)


-- | Layout of individual chunks on disk
--   
--   This module is not re-exported from the public Chunks API, since it's
--   only relevant internally in the immutable DB. This module makes the
--   layout decisions.
module Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Layout

-- | Result of <a>nextRelativeSlot</a>
data NextRelativeSlot

-- | There is a next negative slot
NextRelativeSlot :: RelativeSlot -> NextRelativeSlot

-- | We reached the end of the chunk
NoMoreRelativeSlots :: NextRelativeSlot

-- | The first relative slot
--   
--   NOTE: This refers to an EBB only if the <a>ChunkSize</a> supports it.
firstBlockOrEBB :: ChunkInfo -> ChunkNo -> RelativeSlot

-- | The last relative slot within a chunk of the given size
maxRelativeSlot :: ChunkInfo -> ChunkNo -> RelativeSlot

-- | Next relative slot
nextRelativeSlot :: HasCallStack => RelativeSlot -> NextRelativeSlot

-- | The <tt>n</tt>'th relative slot for an arbitrary block
--   
--   NOTE: Offset <tt>0</tt> refers to an EBB only if the <a>ChunkSize</a>
--   supports it.
nthBlockOrEBB :: (HasCallStack, Integral a) => ChunkInfo -> ChunkNo -> a -> RelativeSlot

-- | Is this relative slot reserved for an EBB?
relativeSlotIsEBB :: RelativeSlot -> IsEBB

-- | Variation on <a>nextRelativeSlot</a> where the caller <i>knows</i>
--   that there must be a next slot
--   
--   Throws an assertion failure (if assertions are enabled) if there is no
--   next slot.
unsafeNextRelativeSlot :: HasCallStack => RelativeSlot -> RelativeSlot

-- | A <i>relative</i> slot within a chunk
data RelativeSlot
chunkIndexOfSlot :: ChunkInfo -> SlotNo -> ChunkNo

-- | Uniquely identify a block within the immutable DB
--   
--   Constructor marked as <tt>Unsafe</tt>; construction should normally
--   happen inside this module only (though see the <a>ChunkSlot</a>
--   pattern synonym).
data ChunkSlot
UnsafeChunkSlot :: !ChunkNo -> !RelativeSlot -> ChunkSlot
[chunkIndex] :: ChunkSlot -> !ChunkNo
[chunkRelative] :: ChunkSlot -> !RelativeSlot
pattern ChunkSlot :: ChunkNo -> RelativeSlot -> ChunkSlot

-- | Chunk slot for <a>BlockOrEBB</a>
chunkSlotForBlockOrEBB :: ChunkInfo -> BlockOrEBB -> ChunkSlot

-- | Chunk slot for EBB
chunkSlotForBoundaryBlock :: HasCallStack => ChunkInfo -> EpochNo -> ChunkSlot

-- | Chunk slot for a regular block (i.e., not an EBB)
chunkSlotForRegularBlock :: ChunkInfo -> SlotNo -> ChunkSlot
chunkSlotForRelativeSlot :: ChunkNo -> RelativeSlot -> ChunkSlot

-- | Chunk slot for <a>Tip</a>
chunkSlotForTip :: ChunkInfo -> Tip blk -> ChunkSlot

-- | Chunk slot for an unknown block
--   
--   This returns <i>two</i> <a>ChunkSlot</a>s: one in case the block could
--   be an EBB, and one in case the block is a regular block. In addition,
--   it also returns the <a>ChunkNo</a> that both of these
--   <a>ChunkSlot</a>s must necessarily share.
chunkSlotForUnknownBlock :: HasCallStack => ChunkInfo -> SlotNo -> (ChunkNo, Maybe ChunkSlot, ChunkSlot)
chunkSlotToBlockOrEBB :: ChunkInfo -> ChunkSlot -> BlockOrEBB

-- | From relative to absolute slot
--   
--   This can be used for EBBs and regular blocks, since they don't share a
--   relative slot.
chunkSlotToSlot :: ChunkInfo -> ChunkSlot -> SlotNo
slotMightBeEBB :: ChunkInfo -> SlotNo -> Maybe EpochNo
slotNoOfBlockOrEBB :: ChunkInfo -> BlockOrEBB -> SlotNo
slotNoOfEBB :: HasCallStack => ChunkInfo -> EpochNo -> SlotNo
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Layout.ChunkSlot
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Layout.ChunkSlot
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Layout.ChunkSlot
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Layout.ChunkSlot
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Chunks.Layout.ChunkSlot

module Ouroboros.Consensus.Storage.ImmutableDB.Chunks

-- | Size of the chunks of the immutable DB
--   
--   This is the key data structure that drives all layout functions.
--   
--   TODO: Add support for non-uniform <a>ChunkInfo</a>
--   <a>https://github.com/IntersectMBO/ouroboros-network/issues/1754</a>
data ChunkInfo

-- | A single, uniform, chunk size
--   
--   If EBBs are present, the chunk size must line up precisely with the
--   epoch size (that is, the number of regular blocks in the chunk must
--   equal the number of regular blocks in an epoch).
UniformChunkSize :: !ChunkSize -> ChunkInfo

-- | Chunk number
data ChunkNo

-- | Size of a chunk
--   
--   The total number of slots available in a chunk is equal to
--   <a>numRegularBlocks</a> if <tt>not</tt> <a>chunkCanContainEBB</a>, and
--   <a>numRegularBlocks</a> <tt>+ 1</tt> otherwise.
data ChunkSize
ChunkSize :: !Bool -> !Word64 -> ChunkSize

-- | Does this chunk also accomodate an EBB?
[chunkCanContainEBB] :: ChunkSize -> !Bool

-- | The number of regular blocks in this chunk
[numRegularBlocks] :: ChunkSize -> !Word64

-- | A <i>relative</i> slot within a chunk
data RelativeSlot

-- | Result of <a>nextRelativeSlot</a>
data NextRelativeSlot

-- | There is a next negative slot
NextRelativeSlot :: RelativeSlot -> NextRelativeSlot

-- | We reached the end of the chunk
NoMoreRelativeSlots :: NextRelativeSlot

-- | Uniquely identify a block within the immutable DB
--   
--   Constructor marked as <tt>Unsafe</tt>; construction should normally
--   happen inside this module only (though see the <a>ChunkSlot</a>
--   pattern synonym).
data ChunkSlot
UnsafeChunkSlot :: !ChunkNo -> !RelativeSlot -> ChunkSlot
[chunkIndex] :: ChunkSlot -> !ChunkNo
[chunkRelative] :: ChunkSlot -> !RelativeSlot
pattern ChunkSlot :: ChunkNo -> RelativeSlot -> ChunkSlot

-- | Can we store EBBs in the chunks described by this <a>ChunkInfo</a>?
--   
--   This is only used for tests. This API will need to change (and the
--   tests will become more complicated) once we support non-uniform
--   <a>ChunkInfo</a>.
chunkInfoSupportsEBBs :: ChunkInfo -> Bool

-- | Enumerate all chunks
--   
--   <pre>
--   chunksBetween x              x  == [x]
--   chunksBetween x (nextChunkNo x) == [x, nextChunkNo x]
--   </pre>
chunksBetween :: ChunkNo -> ChunkNo -> [ChunkNo]

-- | <a>RelativeSlot</a> is partially ordered, not totally ordered
--   
--   It makes no sense to compare <tt>RelativeSlots</tt> from different
--   chunks. Doing so will result in an assertion failure.
compareRelativeSlot :: HasCallStack => RelativeSlot -> RelativeSlot -> Ordering

-- | Count number of chunks between two indices
--   
--   <pre>
--   countChunks x              x  == 0
--   countChunks x (nextChunkNo x) == 1
--   </pre>
countChunks :: ChunkNo -> ChunkNo -> Word64

-- | First chunk
firstChunkNo :: ChunkNo
getChunkSize :: ChunkInfo -> ChunkNo -> ChunkSize

-- | Smart constructor for <a>RelativeSlot</a>
mkRelativeSlot :: HasCallStack => ChunkInfo -> ChunkNo -> Word64 -> RelativeSlot
nextChunkNo :: ChunkNo -> ChunkNo
prevChunkNo :: ChunkNo -> Maybe ChunkNo

-- | Simple chunk config with a single chunk size
--   
--   This intentionally takes <a>EpochSize</a> (number of slots) rather
--   than <a>ChunkSize</a>: the translation from <a>EpochSize</a> to
--   <a>ChunkSize</a> (number of available entries in a chunk) should not
--   be done by client code.
simpleChunkInfo :: EpochSize -> ChunkInfo

-- | <a>ChunkInfo</a> for a single <a>ChunkSize</a>
--   
--   See also <a>simpleChunkInfo</a>.
singleChunkInfo :: ChunkSize -> ChunkInfo

-- | The first relative slot
--   
--   NOTE: This refers to an EBB only if the <a>ChunkSize</a> supports it.
firstBlockOrEBB :: ChunkInfo -> ChunkNo -> RelativeSlot

-- | The last relative slot within a chunk of the given size
maxRelativeSlot :: ChunkInfo -> ChunkNo -> RelativeSlot

-- | Next relative slot
nextRelativeSlot :: HasCallStack => RelativeSlot -> NextRelativeSlot

-- | The <tt>n</tt>'th relative slot for an arbitrary block
--   
--   NOTE: Offset <tt>0</tt> refers to an EBB only if the <a>ChunkSize</a>
--   supports it.
nthBlockOrEBB :: (HasCallStack, Integral a) => ChunkInfo -> ChunkNo -> a -> RelativeSlot

-- | Is this relative slot reserved for an EBB?
relativeSlotIsEBB :: RelativeSlot -> IsEBB

-- | Variation on <a>nextRelativeSlot</a> where the caller <i>knows</i>
--   that there must be a next slot
--   
--   Throws an assertion failure (if assertions are enabled) if there is no
--   next slot.
unsafeNextRelativeSlot :: HasCallStack => RelativeSlot -> RelativeSlot
chunkIndexOfSlot :: ChunkInfo -> SlotNo -> ChunkNo

-- | Chunk slot for <a>BlockOrEBB</a>
chunkSlotForBlockOrEBB :: ChunkInfo -> BlockOrEBB -> ChunkSlot

-- | Chunk slot for EBB
chunkSlotForBoundaryBlock :: HasCallStack => ChunkInfo -> EpochNo -> ChunkSlot

-- | Chunk slot for a regular block (i.e., not an EBB)
chunkSlotForRegularBlock :: ChunkInfo -> SlotNo -> ChunkSlot
chunkSlotForRelativeSlot :: ChunkNo -> RelativeSlot -> ChunkSlot

-- | Chunk slot for <a>Tip</a>
chunkSlotForTip :: ChunkInfo -> Tip blk -> ChunkSlot

-- | Chunk slot for an unknown block
--   
--   This returns <i>two</i> <a>ChunkSlot</a>s: one in case the block could
--   be an EBB, and one in case the block is a regular block. In addition,
--   it also returns the <a>ChunkNo</a> that both of these
--   <a>ChunkSlot</a>s must necessarily share.
chunkSlotForUnknownBlock :: HasCallStack => ChunkInfo -> SlotNo -> (ChunkNo, Maybe ChunkSlot, ChunkSlot)
chunkSlotToBlockOrEBB :: ChunkInfo -> ChunkSlot -> BlockOrEBB

-- | From relative to absolute slot
--   
--   This can be used for EBBs and regular blocks, since they don't share a
--   relative slot.
chunkSlotToSlot :: ChunkInfo -> ChunkSlot -> SlotNo
slotMightBeEBB :: ChunkInfo -> SlotNo -> Maybe EpochNo
slotNoOfBlockOrEBB :: ChunkInfo -> BlockOrEBB -> SlotNo
slotNoOfEBB :: HasCallStack => ChunkInfo -> EpochNo -> SlotNo


-- | Primary Index
--   
--   Intended for qualified import &gt; import qualified
--   Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Primary as
--   PrimaryIndex
module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Primary

-- | An offset in the secondary index file.
--   
--   We need 4 bytes (<a>Word32</a>) because the secondary index file can
--   grow to +1MiB.
type SecondaryOffset = Word32

-- | In-memory representation of the primary index file.
--   
--   The primary index maps relative slots to offsets in the secondary
--   index file. The first offset is always 0, as the first entry in the
--   secondary index file will always start at offset 0. The second offset
--   will be equal to the size of a secondary index entry, unless the slot
--   is empty, in which case it will be 0. In general, an offset will
--   either be a repetition of the offset before it, to indicate the slot
--   is empty, or the offset before it + the fixed size of a secondary
--   index entry, in case the slot is filled.
--   
--   The size of a secondary index entry can be computed by subtracting the
--   offset corresponding to the respective slot from the offset
--   corresponding to the slot after it.
--   
--   For example, if slots 0, 1 and 4 are filled, we'd have the following
--   offsets in the primary index file:
--   
--   <pre>
--   slot:       0   1   2   3   4
--           ┌───┬───┬───┬───┬───┬───┐
--   offset: │ 0 │ x │ y │ y │ y │ z │
--           └───┴───┴───┴───┴───┴───┘
--   </pre>
--   
--   We use <tt>x, y, z</tt> in the example above, but in practice these
--   will be multiples of the (fixed) size of an entry in secondary index.
--   
--   TODO As all entries have the same size, we could use a bitvector
--   instead, see #1234.
--   
--   The serialisation of a primary index file starts with
--   <tt>currentVersionNumber</tt> followed by all its offset.
data PrimaryIndex
MkPrimaryIndex :: !ChunkNo -> !Vector SecondaryOffset -> PrimaryIndex

-- | The <a>ChunkNo</a> of the chunk this index is associated with
[primaryIndexChunkNo] :: PrimaryIndex -> !ChunkNo

-- | The entries in the index proper
[primaryIndexOffsets] :: PrimaryIndex -> !Vector SecondaryOffset

-- | Append the given <a>SecondaryOffset</a> to the end of the file (passed
--   as a handle).
appendOffsets :: (Monad m, Foldable f, HasCallStack) => HasFS m h -> Handle h -> f SecondaryOffset -> m ()

-- | Return the slots to backfill the primary index file with.
--   
--   A situation may arise in which we "skip" some relative slots, and we
--   write into the DB, for example, every other relative slot. In this
--   case, we need to backfill the primary index file with offsets for the
--   skipped relative slots. Similarly, before we start a new chunk, we
--   must backfill the primary index file of the current chunk to indicate
--   that the remaining slots in the chunk are empty.
--   
--   For example, say we have written to relative slots 0 and 1. We have
--   the following primary index file:
--   
--   <pre>
--   slot:       0   1
--           ┌───┬───┬───┐
--   offset: │ 0 │ x │ y │
--           └───┴───┴───┘
--   </pre>
--   
--   Now we want to write to relative slot 4, skipping 2 and 3. We first
--   have to backfill the primary index by repeating the last offset for
--   the two missing slots:
--   
--   <pre>
--   slot:       0   1   2   3
--           ┌───┬───┬───┬───┬───┐
--   offset: │ 0 │ x │ y │ y │ y │
--           └───┴───┴───┴───┴───┘
--   </pre>
--   
--   After backfilling (writing the offset <tt>y</tt> twice), we can write
--   the next offset:
--   
--   <pre>
--   slot:       0   1   2   3   4
--           ┌───┬───┬───┬───┬───┬───┐
--   offset: │ 0 │ x │ y │ y │ y │ z │
--           └───┴───┴───┴───┴───┴───┘
--   </pre>
--   
--   For the example above, the output of this function would thus be:
--   <tt>[y, y]</tt>.
--   
--   We use <tt>x, y, z</tt> in the examples above, but in practice these
--   will be multiples of the (fixed) size of an entry in secondary index.
backfill :: RelativeSlot -> RelativeSlot -> SecondaryOffset -> [SecondaryOffset]

-- | Return the slots to backfill the primary index file with when padding
--   it to the chunk size.
--   
--   See <a>backfill</a> for more details.
backfillChunk :: ChunkInfo -> ChunkNo -> NextRelativeSlot -> SecondaryOffset -> [SecondaryOffset]

-- | Check whether the given slot is within the primary index.
containsSlot :: PrimaryIndex -> RelativeSlot -> Bool

-- | Version number of the index format
currentVersionNumber :: Word8

-- | Return a list of all the filled (length &gt; zero) slots in the
--   primary index.
filledSlots :: ChunkInfo -> PrimaryIndex -> [RelativeSlot]

-- | Find the first filled (length &gt; zero) slot in the primary index. If
--   there is none, return <a>Nothing</a>.
--   
--   Example: given the primary index below:
--   
--   <pre>
--   slot:       0   1
--           ┌───┬───┬───┐
--   offset: │ 0 │ 0 │ x │
--           └───┴───┴───┘
--   </pre>
--   
--   Return slot 1.
firstFilledSlot :: ChunkInfo -> PrimaryIndex -> Maybe RelativeSlot

-- | Return the last slot of the primary index (empty or not).
--   
--   Returns <a>Nothing</a> if the index is empty.
getLastSlot :: ChunkInfo -> PrimaryIndex -> Maybe RelativeSlot

-- | Return <a>True</a> when the given slot is filled.
--   
--   Precondition: the given slot must be within the primary index
--   (<a>containsSlot</a>).
isFilledSlot :: HasCallStack => PrimaryIndex -> RelativeSlot -> Bool

-- | Return the last filled slot in the primary index.
lastFilledSlot :: HasCallStack => ChunkInfo -> PrimaryIndex -> Maybe RelativeSlot

-- | Return the last <a>SecondaryOffset</a> in the primary index file.
lastOffset :: PrimaryIndex -> SecondaryOffset

-- | Load a primary index file in memory.
load :: forall blk m h. (HasCallStack, MonadThrow m, StandardHash blk, Typeable blk) => Proxy blk -> HasFS m h -> ChunkNo -> m PrimaryIndex

-- | Find the next filled (length &gt; zero) slot after the given slot in
--   the primary index. If there is none, return <a>Nothing</a>.
--   
--   Precondition: the given slot must be within the primary index
--   (<a>containsSlot</a>).
--   
--   Example: given the primary index below and slot 1:
--   
--   <pre>
--   slot:       0   1   2   3   4
--           ┌───┬───┬───┬───┬───┬───┐
--   offset: │ 0 │ x │ y │ y │ y │ z │
--           └───┴───┴───┴───┴───┴───┘
--   </pre>
--   
--   Return slot 4.
nextFilledSlot :: ChunkInfo -> PrimaryIndex -> RelativeSlot -> Maybe RelativeSlot

-- | Return the offset for the given slot.
--   
--   Precondition: the given slot must be within the primary index
--   (<a>containsSlot</a>).
offsetOfSlot :: HasCallStack => PrimaryIndex -> RelativeSlot -> SecondaryOffset

-- | Open a primary index file for the given chunk and return a handle to
--   it.
--   
--   The file is opened with the given <a>AllowExisting</a> value. When
--   given <a>MustBeNew</a>, the version number is written to the file.
open :: (HasCallStack, MonadCatch m) => HasFS m h -> ChunkNo -> AllowExisting -> m (Handle h)

-- | Return the first filled slot in the primary index file, or
--   <a>Nothing</a> in case there are no filled slots.
--   
--   PRECONDITION: the index file must exist and contain at least the
--   version number and offset 0.
--   
--   May throw <tt>InvalidPrimaryIndexException</tt>.
readFirstFilledSlot :: forall blk m h. (HasCallStack, MonadThrow m, StandardHash blk, Typeable blk) => Proxy blk -> HasFS m h -> ChunkInfo -> ChunkNo -> m (Maybe RelativeSlot)

-- | Read the <a>SecondaryOffset</a> corresponding to the given relative
--   slot in the primary index. Return <a>Nothing</a> when the slot is
--   empty.
readOffset :: forall blk m h. (HasCallStack, MonadThrow m, StandardHash blk, Typeable blk) => Proxy blk -> HasFS m h -> ChunkNo -> RelativeSlot -> m (Maybe SecondaryOffset)

-- | Same as <a>readOffset</a>, but for multiple offsets.
--   
--   NOTE: only use this for a few offsets, as we will seek
--   (<tt>pread</tt>) for each offset. Use <a>load</a> if you want to read
--   the whole primary index.
readOffsets :: forall blk m h t. (HasCallStack, MonadThrow m, Traversable t, StandardHash blk, Typeable blk) => Proxy blk -> HasFS m h -> ChunkNo -> t RelativeSlot -> m (t (Maybe SecondaryOffset))

-- | The size of each entry in the primary index file, i.e., the size of a
--   <a>SecondaryOffset</a>.
secondaryOffsetSize :: Word64

-- | Return the size of the given slot according to the primary index.
--   
--   Precondition: the given slot must be within the primary index
--   (<a>containsSlot</a>).
sizeOfSlot :: HasCallStack => PrimaryIndex -> RelativeSlot -> Word32

-- | Count the number of (filled or unfilled) slots currently in the index
slots :: PrimaryIndex -> Word64

-- | Truncate the primary index so that the given <a>RelativeSlot</a> will
--   be the last slot (filled or not) in the primary index, unless the
--   primary index didn't contain the <a>RelativeSlot</a> in the first
--   place.
truncateToSlot :: ChunkInfo -> RelativeSlot -> PrimaryIndex -> PrimaryIndex

-- | On-disk variant of <a>truncateToSlot</a>. The truncation is done
--   without reading the primary index from disk.
truncateToSlotFS :: (HasCallStack, MonadThrow m) => HasFS m h -> ChunkNo -> RelativeSlot -> m ()

-- | Remove all trailing empty slots that were added during the
--   finalisation/backfilling of the primary index.
--   
--   POSTCONDITION: the last slot of the primary index file will be filled,
--   unless the index itself is empty.
unfinalise :: (HasCallStack, MonadThrow m, StandardHash blk, Typeable blk) => Proxy blk -> HasFS m h -> ChunkInfo -> ChunkNo -> m ()

-- | Write a primary index to a file.
--   
--   Property: for <tt>hasFS</tt>, <tt>err</tt>, <tt>chunk</tt>
--   
--   <pre>
--   'write' hasFS chunk primaryIndex
--   primaryIndex' &lt;- 'load' hasFS err chunk
--   </pre>
--   
--   Then it must be that:
--   
--   <pre>
--   primaryIndex === primaryIndex'
--   </pre>
write :: (HasCallStack, MonadThrow m) => HasFS m h -> ChunkNo -> PrimaryIndex -> m ()

-- | Smart constructor: checks that the offsets are non-decreasing, there
--   is at least one offset, and that the first offset is 0.
mk :: ChunkNo -> [SecondaryOffset] -> Maybe PrimaryIndex

-- | Return the <a>SecondaryOffset</a>s in the <a>PrimaryIndex</a>.
toSecondaryOffsets :: PrimaryIndex -> [SecondaryOffset]
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Primary.PrimaryIndex
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Primary.PrimaryIndex
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Primary.PrimaryIndex
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Primary.PrimaryIndex

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary
newtype BlockOffset
BlockOffset :: Word64 -> BlockOffset
[unBlockOffset] :: BlockOffset -> Word64
data BlockSize
BlockSize :: Word32 -> BlockSize

-- | In case of the last entry, we don't have any entry and thus block
--   offset after it that we can use to calculate the size of the block.
LastEntry :: BlockSize
data Entry blk
Entry :: !BlockOffset -> !HeaderOffset -> !HeaderSize -> !CRC -> !HeaderHash blk -> !BlockOrEBB -> Entry blk
[blockOffset] :: Entry blk -> !BlockOffset
[headerOffset] :: Entry blk -> !HeaderOffset
[headerSize] :: Entry blk -> !HeaderSize
[checksum] :: Entry blk -> !CRC
[headerHash] :: Entry blk -> !HeaderHash blk
[blockOrEBB] :: Entry blk -> !BlockOrEBB
newtype HeaderOffset
HeaderOffset :: Word16 -> HeaderOffset
[unHeaderOffset] :: HeaderOffset -> Word16
newtype HeaderSize
HeaderSize :: Word16 -> HeaderSize
[unHeaderSize] :: HeaderSize -> Word16
appendEntry :: forall m blk h. (HasCallStack, ConvertRawHash blk, MonadThrow m) => HasFS m h -> Handle h -> Entry blk -> m Word64
entrySize :: ConvertRawHash blk => Proxy blk -> Word32

-- | Read all entries in a secondary index file, starting from the given
--   <a>SecondaryOffset</a> until the stop condition is true or until the
--   end of the file is reached. The entry for which the stop condition is
--   true will be the last in the returned list of entries.
readAllEntries :: forall m blk h. (HasCallStack, ConvertRawHash blk, MonadThrow m, StandardHash blk, Typeable blk) => HasFS m h -> SecondaryOffset -> ChunkNo -> (Entry blk -> Bool) -> Word64 -> IsEBB -> m [WithBlockSize (Entry blk)]

-- | Same as <a>readEntry</a>, but for multiple entries.
--   
--   NOTE: only use this for a few entries, as we will seek
--   (<tt>pread</tt>) for each entry. Use <a>readAllEntries</a> if you want
--   to read all entries in the secondary index file.
readEntries :: forall m blk h t. (HasCallStack, ConvertRawHash blk, MonadThrow m, StandardHash blk, Typeable blk, Traversable t) => HasFS m h -> ChunkNo -> t (IsEBB, SecondaryOffset) -> m (t (Entry blk, BlockSize))

-- | Read the entry at the given <a>SecondaryOffset</a>. Interpret it as an
--   EBB depending on the given <a>IsEBB</a>.
readEntry :: forall m blk h. (HasCallStack, ConvertRawHash blk, MonadThrow m, StandardHash blk, Typeable blk) => HasFS m h -> ChunkNo -> IsEBB -> SecondaryOffset -> m (Entry blk, BlockSize)

-- | Remove all entries after the entry at the given
--   <a>SecondaryOffset</a>. That entry will now be the last entry in the
--   secondary index file.
truncateToEntry :: forall m blk h. (HasCallStack, ConvertRawHash blk, MonadThrow m) => Proxy blk -> HasFS m h -> ChunkNo -> SecondaryOffset -> m ()
writeAllEntries :: forall m blk h. (HasCallStack, ConvertRawHash blk, MonadThrow m) => HasFS m h -> ChunkNo -> [Entry blk] -> m ()
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance Foreign.Storable.Storable Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance GHC.Num.Num Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance GHC.Real.Integral Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance GHC.Real.Real Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance GHC.Enum.Enum Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderOffset
instance Foreign.Storable.Storable Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderOffset
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderOffset
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderOffset
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderSize
instance Foreign.Storable.Storable Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderSize
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderSize
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderSize
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.Entry blk)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockSize
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockSize
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockSize
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockSize
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.Entry blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.Entry blk)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.Entry blk)
instance Data.Binary.Class.Binary Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderSize
instance Data.Binary.Class.Binary Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.HeaderOffset
instance Data.Binary.Class.Binary Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Secondary.BlockOffset

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Parser

-- | Information about a block returned by the parser.
--   
--   The fields of this record are strict to make sure that by evaluating
--   this record to WHNF, we no longer hold on to the entire block.
--   Otherwise, we might accidentally keep all blocks in a single file in
--   memory during parsing.
data BlockSummary blk
BlockSummary :: !Entry blk -> !BlockNo -> !SlotNo -> BlockSummary blk
[summaryEntry] :: BlockSummary blk -> !Entry blk
[summaryBlockNo] :: BlockSummary blk -> !BlockNo
[summarySlotNo] :: BlockSummary blk -> !SlotNo

-- | Defined here instead of in the <tt>Parser</tt> module because
--   <a>TraceEvent</a> depends on it.
data ChunkFileError blk

-- | A block could not be decoded
ChunkErrRead :: ReadIncrementalErr -> ChunkFileError blk

-- | The previous hash of a block did not match the hash of the previous
--   block.
ChunkErrHashMismatch :: HeaderHash blk -> ChainHash blk -> ChunkFileError blk

-- | The integrity verification of the block with the given point returned
--   <a>False</a>, indicating that the block got corrupted.
ChunkErrCorrupt :: Point blk -> ChunkFileError blk

-- | Parse the contents of a chunk file.
--   
--   <ul>
--   <li>The parser decodes each block in the chunk. When one of them fails
--   to decode, a <a>ChunkErrRead</a> error is returned.</li>
--   <li>Each block's checksum is checked against its given expected
--   checksum (coming from the secondary index). When a checksum doesn't
--   match, a <a>ChunkErrCorrupt</a> error is returned. When the secondary
--   index is missing or corrupt, and there are no or fewer expected
--   checksums, we use the given (more expensive) integrity checking
--   function instead of checksum comparison.</li>
--   <li>We check that each block fits onto the previous one by checking
--   the hashes. If not, we return a <a>ChunkErrHashMismatch</a>
--   error.</li>
--   <li>An error is returned in the form of:</li>
--   </ul>
--   
--   <pre>
--   'Maybe' ('ChunkFileError' blk, 'Word64')
--   </pre>
--   
--   The <a>Word64</a> corresponds to the offset in the file where the last
--   valid entry ends. Truncating to this offset will remove all invalid
--   data from the file and just leave the valid entries before it. Note
--   that we are not using <a>Either</a> because the error might occur
--   after some valid entries have been parsed successfully, in which case
--   we still want these valid entries, but also want to know about the
--   error so we can truncate the file to get rid of the unparseable data.
parseChunkFile :: forall m blk h r. (IOLike m, GetPrevHash blk, HasBinaryBlockInfo blk, DecodeDisk blk (ByteString -> blk)) => CodecConfig blk -> HasFS m h -> (blk -> Bool) -> FsPath -> [CRC] -> (Stream (Of (BlockSummary blk, ChainHash blk)) m (Maybe (ChunkFileError blk, Word64)) -> m r) -> m r

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache
data CacheConfig
CacheConfig :: Word32 -> DiffTime -> CacheConfig

-- | Maximum number of past chunks to cache, excluding the current chunk.
--   
--   NOTE: must be &gt; 0
[$sel:pastChunksToCache:CacheConfig] :: CacheConfig -> Word32

-- | Expire past chunks that haven't been used for
--   <a>$sel:expireUnusedAfter:CacheConfig</a> from the cache, regardless
--   the number of past chunks in the cache.
[$sel:expireUnusedAfter:CacheConfig] :: CacheConfig -> DiffTime

-- | Environment used by functions operating on the cached index.
data CacheEnv m blk h
checkInvariants :: Word32 -> Cached blk -> Maybe String

-- | Creates a new <a>CacheEnv</a> and launches a background thread that
--   expires unused past chunks (<a>expireUnusedChunks</a>).
--   
--   PRECONDITION: <a>$sel:pastChunksToCache:CacheConfig</a> (in
--   <a>CacheConfig</a>) &gt; 0
newEnv :: (HasCallStack, ConvertRawHash blk, IOLike m, StandardHash blk, Typeable blk) => HasFS m h -> ResourceRegistry m -> Tracer m TraceCacheEvent -> CacheConfig -> ChunkInfo -> ChunkNo -> m (CacheEnv m blk h)

-- | Intended to run as a background thread.
--   
--   Will expire past chunks that haven't been used for
--   <a>$sel:expireUnusedAfter:CacheConfig</a> from the cache.
expireUnusedChunks :: (HasCallStack, IOLike m) => CacheEnv m blk h -> m Void

-- | Stops the background expiration thread.
--   
--   This operation is idempotent.
close :: IOLike m => CacheEnv m blk h -> m ()

-- | Restarts the background expiration thread, drops all previously cached
--   information, loads the given chunk.
--   
--   PRECONDITION: the background thread expiring unused past chunks must
--   have been terminated.
restart :: (ConvertRawHash blk, IOLike m, StandardHash blk, Typeable blk) => CacheEnv m blk h -> ChunkNo -> m ()
appendOffsets :: (HasCallStack, Foldable f, IOLike m) => CacheEnv m blk h -> Handle h -> f SecondaryOffset -> m ()

-- | This is called when a new chunk is started, which means we need to
--   update <a>Cached</a> to reflect this.
openPrimaryIndex :: (HasCallStack, ConvertRawHash blk, IOLike m, StandardHash blk, Typeable blk) => CacheEnv m blk h -> ChunkNo -> AllowExisting -> m (Handle h)
readFirstFilledSlot :: (HasCallStack, ConvertRawHash blk, IOLike m, StandardHash blk, Typeable blk) => CacheEnv m blk h -> ChunkNo -> m (Maybe RelativeSlot)
readOffsets :: (HasCallStack, ConvertRawHash blk, IOLike m, StandardHash blk, Typeable blk, Traversable t) => CacheEnv m blk h -> ChunkNo -> t RelativeSlot -> m (t (Maybe SecondaryOffset))
appendEntry :: forall m blk h. (HasCallStack, ConvertRawHash blk, IOLike m) => CacheEnv m blk h -> ChunkNo -> Handle h -> Entry blk -> m Word64
readAllEntries :: forall m blk h. (HasCallStack, ConvertRawHash blk, IOLike m, StandardHash blk, Typeable blk) => CacheEnv m blk h -> SecondaryOffset -> ChunkNo -> (Entry blk -> Bool) -> Word64 -> IsEBB -> m [WithBlockSize (Entry blk)]
readEntries :: forall m blk h t. (HasCallStack, ConvertRawHash blk, IOLike m, StandardHash blk, Typeable blk, Traversable t) => CacheEnv m blk h -> ChunkNo -> t (IsEBB, SecondaryOffset) -> m (t (Entry blk, BlockSize))
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.CacheConfig
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.CacheConfig
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.CurrentChunkInfo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.CurrentChunkInfo blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.CurrentChunkInfo blk)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.PastChunkInfo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.PastChunkInfo blk)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.LastUsed
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.LastUsed
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.LastUsed
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.LastUsed
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.Cached blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Cache.Cached blk)

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index

-- | Bundle the operations on the primary and secondary index that touch
--   the files. This allows us to easily introduce an intermediary caching
--   layer.
data Index m blk h
Index :: (forall t. (HasCallStack, Traversable t) => ChunkNo -> t RelativeSlot -> m (t (Maybe SecondaryOffset))) -> (HasCallStack => ChunkNo -> m (Maybe RelativeSlot)) -> (HasCallStack => ChunkNo -> AllowExisting -> m (Handle h)) -> (forall f. (HasCallStack, Foldable f) => Handle h -> f SecondaryOffset -> m ()) -> (forall t. (HasCallStack, Traversable t) => ChunkNo -> t (IsEBB, SecondaryOffset) -> m (t (Entry blk, BlockSize))) -> (HasCallStack => SecondaryOffset -> ChunkNo -> (Entry blk -> Bool) -> Word64 -> IsEBB -> m [WithBlockSize (Entry blk)]) -> (HasCallStack => ChunkNo -> Handle h -> WithBlockSize (Entry blk) -> m Word64) -> (HasCallStack => m ()) -> (HasCallStack => ChunkNo -> m ()) -> Index m blk h

-- | See <a>readOffsets</a>
[readOffsets] :: Index m blk h -> forall t. (HasCallStack, Traversable t) => ChunkNo -> t RelativeSlot -> m (t (Maybe SecondaryOffset))

-- | See <a>readFirstFilledSlot</a>
[readFirstFilledSlot] :: Index m blk h -> HasCallStack => ChunkNo -> m (Maybe RelativeSlot)

-- | See <a>open</a>
[openPrimaryIndex] :: Index m blk h -> HasCallStack => ChunkNo -> AllowExisting -> m (Handle h)

-- | See <a>appendOffsets</a>
[appendOffsets] :: Index m blk h -> forall f. (HasCallStack, Foldable f) => Handle h -> f SecondaryOffset -> m ()

-- | See <a>readEntries</a>
[readEntries] :: Index m blk h -> forall t. (HasCallStack, Traversable t) => ChunkNo -> t (IsEBB, SecondaryOffset) -> m (t (Entry blk, BlockSize))

-- | See <a>readAllEntries</a>
[readAllEntries] :: Index m blk h -> HasCallStack => SecondaryOffset -> ChunkNo -> (Entry blk -> Bool) -> Word64 -> IsEBB -> m [WithBlockSize (Entry blk)]

-- | See <a>appendEntry</a>
[appendEntry] :: Index m blk h -> HasCallStack => ChunkNo -> Handle h -> WithBlockSize (Entry blk) -> m Word64

-- | Close the index and stop any background threads.
--   
--   Should be called when the ImmutableDB is closed.
[close] :: Index m blk h -> HasCallStack => m ()

-- | Restart a closed index using the given chunk as the current chunk,
--   drop all previously cached information.
--   
--   NOTE: this will only used in the testsuite, when we need to truncate.
[restart] :: Index m blk h -> HasCallStack => ChunkNo -> m ()

-- | See <a>readEntry</a>.
readEntry :: Functor m => Index m blk h -> ChunkNo -> IsEBB -> SecondaryOffset -> m (Entry blk, BlockSize)

-- | See <a>readOffset</a>.
readOffset :: Functor m => Index m blk h -> ChunkNo -> RelativeSlot -> m (Maybe SecondaryOffset)
fileBackedIndex :: forall m blk h. (ConvertRawHash blk, MonadCatch m, StandardHash blk, Typeable blk) => HasFS m h -> ChunkInfo -> Index m blk h
data CacheConfig
CacheConfig :: Word32 -> DiffTime -> CacheConfig

-- | Maximum number of past chunks to cache, excluding the current chunk.
--   
--   NOTE: must be &gt; 0
[$sel:pastChunksToCache:CacheConfig] :: CacheConfig -> Word32

-- | Expire past chunks that haven't been used for
--   <a>$sel:expireUnusedAfter:CacheConfig</a> from the cache, regardless
--   the number of past chunks in the cache.
[$sel:expireUnusedAfter:CacheConfig] :: CacheConfig -> DiffTime

-- | Caches the current chunk's indices as well as a number of past chunk's
--   indices.
--   
--   Spawns a background thread to expire past chunks from the cache that
--   haven't been used for a while.
cachedIndex :: forall m blk h. (IOLike m, ConvertRawHash blk, StandardHash blk, Typeable blk) => HasFS m h -> ResourceRegistry m -> Tracer m TraceCacheEvent -> CacheConfig -> ChunkInfo -> ChunkNo -> m (Index m blk h)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index.Index m blk h)

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.State

-- | The environment used by the immutable database.
data ImmutableDBEnv m blk
ImmutableDBEnv :: !HasFS m h -> !StrictSVar m (InternalState m blk h) -> !blk -> Bool -> !ChunkInfo -> !Tracer m (TraceEvent blk) -> !CacheConfig -> !CodecConfig blk -> ImmutableDBEnv m blk
[hasFS] :: ImmutableDBEnv m blk -> !HasFS m h
[varInternalState] :: ImmutableDBEnv m blk -> !StrictSVar m (InternalState m blk h)
[checkIntegrity] :: ImmutableDBEnv m blk -> !blk -> Bool
[chunkInfo] :: ImmutableDBEnv m blk -> !ChunkInfo
[tracer] :: ImmutableDBEnv m blk -> !Tracer m (TraceEvent blk)
[cacheConfig] :: ImmutableDBEnv m blk -> !CacheConfig
[codecConfig] :: ImmutableDBEnv m blk -> !CodecConfig blk
data InternalState m blk h
DbClosed :: InternalState m blk h
DbOpen :: !OpenState m blk h -> InternalState m blk h

-- | Internal state when the database is open.
data OpenState m blk h
OpenState :: !ChunkNo -> !BlockOffset -> !SecondaryOffset -> !Handle h -> !Handle h -> !Handle h -> !WithOrigin (Tip blk) -> !Index m blk h -> OpenState m blk h

-- | The current <a>ChunkNo</a> the immutable store is writing to.
[currentChunk] :: OpenState m blk h -> !ChunkNo

-- | The offset at which the next block will be written in the current
--   chunk file.
[currentChunkOffset] :: OpenState m blk h -> !BlockOffset

-- | The offset at which the next index entry will be written in the
--   current secondary index.
[currentSecondaryOffset] :: OpenState m blk h -> !SecondaryOffset

-- | The write handle for the current chunk file.
[currentChunkHandle] :: OpenState m blk h -> !Handle h

-- | The write handle for the current primary index file.
[currentPrimaryHandle] :: OpenState m blk h -> !Handle h

-- | The write handle for the current secondary index file.
[currentSecondaryHandle] :: OpenState m blk h -> !Handle h

-- | The current tip of the database.
[currentTip] :: OpenState m blk h -> !WithOrigin (Tip blk)

-- | An abstraction layer on top of the indices to allow for caching.
[currentIndex] :: OpenState m blk h -> !Index m blk h
dbIsOpen :: InternalState m blk h -> Bool

-- | Shorthand
type ModifyOpenState m blk h = StateT (OpenState m blk h) (WithTempRegistry (OpenState m blk h) m)

-- | Clean up the <a>OpenState</a>: <a>closeOpenHandles</a> + close the
--   index (i.e., shut down its background thread)
cleanUp :: Monad m => HasFS m h -> OpenState m blk h -> m ()

-- | Close the handles in the <a>OpenState</a>.
--   
--   Idempotent, as closing a handle is idempotent.
closeOpenHandles :: Monad m => HasFS m h -> OpenState m blk h -> m ()

-- | Get the <a>OpenState</a> of the given database, throw a
--   <a>ClosedDBError</a> in case it is closed.
--   
--   NOTE: Since the <a>OpenState</a> is parameterized over a type
--   parameter <tt>h</tt> of handles, which is not visible from the type of
--   the <tt>ImmutableDBEnv</tt>, we return a <tt>SomePair</tt> here that
--   returns the open state along with a <a>HasFS</a> instance for the
--   <i>same</i> type parameter <tt>h</tt>. Note that it would be
--   impossible to use an existing <a>HasFS</a> instance already in scope
--   otherwise, since the <tt>h</tt> parameters would not be known to
--   match.
getOpenState :: forall m blk. (HasCallStack, IOLike m, StandardHash blk, Typeable blk) => ImmutableDBEnv m blk -> STM m (SomePair (HasFS m) (OpenState m blk))

-- | Create the internal open state for the given chunk.
mkOpenState :: forall m blk h. (HasCallStack, IOLike m, Eq h) => HasFS m h -> Index m blk h -> ChunkNo -> WithOrigin (Tip blk) -> AllowExisting -> WithTempRegistry (OpenState m blk h) m (OpenState m blk h)

-- | Modify the internal state of an open database.
--   
--   In case the database is closed, a <a>ClosedDBError</a> is thrown.
--   
--   In case an <a>ImmutableDBError</a> is thrown, the database is closed
--   to prevent further appending to a database in a potentially
--   inconsistent state.
--   
--   The action is run in the <a>ModifyOpenState</a> monad, which is a
--   <a>StateT</a> transformer (of the <a>OpenState</a>) over the
--   <a>WithTempRegistry</a> monad. This monad can be used to allocate
--   resources in that will be transferred to the returned <a>OpenState</a>
--   that is safely stored in the <a>ImmutableDBEnv</a>. This approach
--   makes sure that no resources are leaked when an exception is thrown
--   while running the action modifying the state.
--   
--   TODO: update this comment <b>Note</b>: This <i>takes</i> the
--   <tt>TMVar</tt>, <i>then</i> runs the action (which might be in
--   <a>IO</a>), and then puts the <tt>TMVar</tt> back, just like
--   <a>modifyMVar</a> does. Consequently, it has the same gotchas that
--   <tt>modifyMVar</tt> does; the effects are observable and it is
--   susceptible to deadlock.
modifyOpenState :: forall m blk a. (HasCallStack, IOLike m, StandardHash blk, Typeable blk) => ImmutableDBEnv m blk -> (forall h. Eq h => HasFS m h -> ModifyOpenState m blk h a) -> m a

-- | Perform an action that accesses the internal state of an open
--   database.
--   
--   In case the database is closed, a <a>ClosedDBError</a> is thrown.
--   
--   In case an <a>ImmutableDBError</a> is thrown while the action is being
--   run, the database is closed to prevent further appending to a database
--   in a potentially inconsistent state.
withOpenState :: forall m blk r. (HasCallStack, IOLike m, StandardHash blk, Typeable blk) => ImmutableDBEnv m blk -> (forall h. HasFS m h -> OpenState m blk h -> m r) -> m r
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.State.OpenState m blk h)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.State.OpenState m blk h)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.State.InternalState m blk h)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.State.InternalState m blk h)

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Validation

-- | Bundle of arguments used most validation functions.
--   
--   Note that we don't use
--   <a>Ouroboros.Consensus.Storage.ImmutableDB.Impl.Index</a> because we
--   are reading and manipulating index files in different ways, e.g.,
--   truncating them.
data ValidateEnv m blk h
ValidateEnv :: !HasFS m h -> !ChunkInfo -> !Tracer m (TraceEvent blk) -> !CacheConfig -> !CodecConfig blk -> !blk -> Bool -> ValidateEnv m blk h
[hasFS] :: ValidateEnv m blk h -> !HasFS m h
[chunkInfo] :: ValidateEnv m blk h -> !ChunkInfo
[tracer] :: ValidateEnv m blk h -> !Tracer m (TraceEvent blk)
[cacheConfig] :: ValidateEnv m blk h -> !CacheConfig
[codecConfig] :: ValidateEnv m blk h -> !CodecConfig blk
[checkIntegrity] :: ValidateEnv m blk h -> !blk -> Bool

-- | Perform validation as per the <a>ValidationPolicy</a> using
--   <a>validate</a> and create an <a>OpenState</a> corresponding to its
--   outcome using <a>mkOpenState</a>.
validateAndReopen :: forall m blk h. (IOLike m, GetPrevHash blk, HasBinaryBlockInfo blk, DecodeDisk blk (ByteString -> blk), ConvertRawHash blk, Eq h, HasCallStack) => ValidateEnv m blk h -> ResourceRegistry m -> ValidationPolicy -> WithTempRegistry (OpenState m blk h) m (OpenState m blk h)

-- | Iff the chunk is the most recent chunk, it should not be finalised.
--   
--   With finalising, we mean: if there are one or more empty slots at the
--   end of the chunk, the primary index should be padded with offsets to
--   indicate that these slots are empty. See <a>backfill</a>.
data ShouldBeFinalised
ShouldBeFinalised :: ShouldBeFinalised
ShouldNotBeFinalised :: ShouldBeFinalised

-- | Reconstruct a <a>PrimaryIndex</a> based on a list of <a>Entry</a>s.
reconstructPrimaryIndex :: forall blk. (ConvertRawHash blk, HasCallStack) => Proxy blk -> ChunkInfo -> ShouldBeFinalised -> ChunkNo -> [BlockOrEBB] -> PrimaryIndex
instance GHC.Show.Show Ouroboros.Consensus.Storage.ImmutableDB.Impl.Validation.ShouldBeFinalised

module Ouroboros.Consensus.Storage.ImmutableDB.Impl.Iterator

-- | Auxiliary data type that combines the <a>currentChunk</a> and
--   <a>currentChunkOffset</a> fields from <a>OpenState</a>. This is used
--   to avoid passing the whole state around, and moreover, it avoids
--   issues with existential <tt>h</tt> type parameter.
data CurrentChunkInfo
CurrentChunkInfo :: !ChunkNo -> !BlockOffset -> CurrentChunkInfo
extractBlockComponent :: forall m blk b h. (HasHeader blk, ReconstructNestedCtxt Header blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, IOLike m) => HasFS m h -> ChunkInfo -> ChunkNo -> CodecConfig blk -> (blk -> Bool) -> Handle h -> WithBlockSize (Entry blk) -> BlockComponent blk b -> m b

-- | Get information about the block or EBB at the given slot with the
--   given hash. If no such block exists, because the slot is empty, it
--   contains a block and/or EBB with a different hash, or it is newer than
--   the current tip, return a <a>MissingBlock</a>.
--   
--   Return the <a>ChunkSlot</a> corresponding to the block or EBB, the
--   corresponding entry (and <a>BlockSize</a>) from the secondary index
--   file, and the <a>SecondaryOffset</a> of that entry.
--   
--   The primary index is read to find out whether the slot is filled and
--   what the <a>SecondaryOffset</a> is for the slot. The secondary index
--   is read to check the hash and to return the <a>Entry</a>.
getSlotInfo :: forall m blk h. (HasCallStack, IOLike m, HasHeader blk) => ChunkInfo -> Index m blk h -> WithOrigin (Tip blk) -> RealPoint blk -> ExceptT (MissingBlock blk) m (ChunkSlot, (Entry blk, BlockSize), SecondaryOffset)
streamImpl :: forall m blk b. (IOLike m, HasHeader blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, ReconstructNestedCtxt Header blk, HasCallStack) => ImmutableDBEnv m blk -> ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (MissingBlock blk) (Iterator m blk b))
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Iterator.IteratorState m blk h)
instance (Ouroboros.Network.Block.StandardHash hash, Ouroboros.Consensus.Util.IOLike.IOLike m) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Iterator.IteratorStateOrExhausted m hash h)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Iterator.IteratorStateOrExhausted m hash h)
instance (Ouroboros.Network.Block.StandardHash blk, Ouroboros.Consensus.Util.IOLike.IOLike m) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ImmutableDB.Impl.Iterator.IteratorState m blk h)


-- | Immutable on-disk database of binary blobs
--   
--   <h1>Internal format</h1>
--   
--   The API of the ImmutableDB uses <a>SlotNo</a> to indicate a location
--   in the chain/immutable database. To distinguish EBBs from regular
--   blocks, the hash is used (together they form a <a>RealPoint</a>). The
--   contents of the database are not stored in one big file that is
--   appended to in eternity, but a separate file is created for each
--   <a>ChunkNo</a>.
--   
--   Within each <a>ChunkNo</a>, the entries are numbered by
--   <a>RelativeSlot</a>s. Each <a>SlotNo</a> can be converted to a
--   combination of an <a>ChunkNo</a> and a <a>RelativeSlot</a> (=
--   <a>ChunkSlot</a>) and vice versa. This conversion depends on the size
--   of the chunks: <a>ChunkSize</a>. This size may not be the same for
--   each chunk. When opening the database, the user must give a
--   <a>ChunkInfo</a> that will be used to find out the size of each chunk.
--   
--   For example:
--   
--   <pre>
--   Chunks:         &lt;──────── 0 ────────&gt; &lt;────── 1 ──────&gt;
--   chunk size:               4                   3
--                   ┌───┬───┬───┬───┬───┐ ┌───┬───┬───┬───┐
--                   │   │   │   │   │   │ │   │   │   │   │
--                   └───┴───┴───┴───┴───┘ └───┴───┴───┴───┘
--   'RelativeSlot':   0   1   2   3   4     0   1   2   3
--   'SlotNo':        EBB  0   1   2   3    EBB  4   5   6
--   </pre>
--   
--   Not all chunks can contain EBBs; see <a>ChunkInfo</a> for details.
--   
--   <h1>Errors</h1>
--   
--   Whenever an <a>ImmutableDBError</a> is thrown during an operation,
--   e.g., <a>appendBlock</a>, the database will be automatically closed
--   because we can not guarantee a consistent state in the face of file
--   system errors.
--   
--   <h1>Opening the database</h1>
--   
--   The database can be closed and opened again. In case the database was
--   closed because of an unexpected error. When the database is opened
--   again, invalid data will be truncated from the database until a valid
--   prefix is recovered.
--   
--   <h1>Concurrency</h1>
--   
--   The same database should not be opened multiple times concurrently.
--   This is ensured by the file lock of the ChainDB.
--   
--   The database can have multiple readers, but should only have one
--   writer.
--   
--   <h1>Layout on disk</h1>
--   
--   The database is structured on disk as follows:
--   
--   <pre>
--   /
--     00000.chunk
--     00000.primary
--     00000.secondary
--     ..
--     00008.chunk
--     00008.primary
--     00008.secondary
--   </pre>
--   
--   For each chunk, there are three files on disk:
--   
--   <ul>
--   <li>A "chunk file" that stores the actual blocks. But nothing more, so
--   nothing is stored for empty slots.</li>
--   <li>A "secondary index file" that stores information about each block:
--   its hash, the slot number or epoch number in case of an EBB, a
--   checksum of the block, the offset of the block in the chunk file, and
--   more. This index is sparse to save space.</li>
--   <li>A "primary index file" that maps slots to offsets in the secondary
--   index file.</li>
--   </ul>
module Ouroboros.Consensus.Storage.ImmutableDB.Impl
data ImmutableDbArgs f m blk
ImmutableDbArgs :: CacheConfig -> HKD f (blk -> Bool) -> HKD f ChunkInfo -> HKD f (CodecConfig blk) -> SomeHasFS m -> HKD f (ResourceRegistry m) -> Tracer m (TraceEvent blk) -> ValidationPolicy -> ImmutableDbArgs f m blk
[immCacheConfig] :: ImmutableDbArgs f m blk -> CacheConfig
[immCheckIntegrity] :: ImmutableDbArgs f m blk -> HKD f (blk -> Bool)
[immChunkInfo] :: ImmutableDbArgs f m blk -> HKD f ChunkInfo
[immCodecConfig] :: ImmutableDbArgs f m blk -> HKD f (CodecConfig blk)
[immHasFS] :: ImmutableDbArgs f m blk -> SomeHasFS m
[immRegistry] :: ImmutableDbArgs f m blk -> HKD f (ResourceRegistry m)
[immTracer] :: ImmutableDbArgs f m blk -> Tracer m (TraceEvent blk)
[immValidationPolicy] :: ImmutableDbArgs f m blk -> ValidationPolicy

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   ImmutableDB.
type ImmutableDbSerialiseConstraints blk = (EncodeDisk blk blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, ReconstructNestedCtxt Header blk, HasBinaryBlockInfo blk)

-- | Default arguments
defaultArgs :: Applicative m => SomeHasFS m -> ImmutableDbArgs Defaults m blk
openDB :: forall m blk ans. (IOLike m, GetPrevHash blk, ConvertRawHash blk, ImmutableDbSerialiseConstraints blk, HasCallStack) => ImmutableDbArgs Identity m blk -> (forall st. WithTempRegistry st m (ImmutableDB m blk, st) -> ans) -> ans

-- | Defined here instead of in the <tt>Parser</tt> module because
--   <a>TraceEvent</a> depends on it.
data ChunkFileError blk

-- | A block could not be decoded
ChunkErrRead :: ReadIncrementalErr -> ChunkFileError blk

-- | The previous hash of a block did not match the hash of the previous
--   block.
ChunkErrHashMismatch :: HeaderHash blk -> ChainHash blk -> ChunkFileError blk

-- | The integrity verification of the block with the given point returned
--   <a>False</a>, indicating that the block got corrupted.
ChunkErrCorrupt :: Point blk -> ChunkFileError blk
data CacheConfig
CacheConfig :: Word32 -> DiffTime -> CacheConfig

-- | Maximum number of past chunks to cache, excluding the current chunk.
--   
--   NOTE: must be &gt; 0
[$sel:pastChunksToCache:CacheConfig] :: CacheConfig -> Word32

-- | Expire past chunks that haven't been used for
--   <a>$sel:expireUnusedAfter:CacheConfig</a> from the cache, regardless
--   the number of past chunks in the cache.
[$sel:expireUnusedAfter:CacheConfig] :: CacheConfig -> DiffTime
data TraceChunkValidation blk validateTo
StartedValidatingChunk :: ChunkNo -> validateTo -> TraceChunkValidation blk validateTo
ValidatedChunk :: ChunkNo -> validateTo -> TraceChunkValidation blk validateTo
MissingChunkFile :: ChunkNo -> TraceChunkValidation blk validateTo
InvalidChunkFile :: ChunkNo -> ChunkFileError blk -> TraceChunkValidation blk validateTo
MissingPrimaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
MissingSecondaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
InvalidPrimaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
InvalidSecondaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
RewritePrimaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
RewriteSecondaryIndex :: ChunkNo -> TraceChunkValidation blk validateTo
data TraceEvent blk
NoValidLastLocation :: TraceEvent blk
ValidatedLastLocation :: ChunkNo -> Tip blk -> TraceEvent blk
ChunkValidationEvent :: TraceChunkValidation blk ChunkNo -> TraceEvent blk

-- | The hash of the last block in the previous epoch doesn't match the
--   previous hash of the first block in the current epoch
ChunkFileDoesntFit :: ChainHash blk -> ChainHash blk -> TraceEvent blk

-- | Performing a migration of the on-disk files
Migrating :: Text -> TraceEvent blk
DeletingAfter :: WithOrigin (Tip blk) -> TraceEvent blk
DBAlreadyClosed :: TraceEvent blk
DBClosed :: TraceEvent blk
TraceCacheEvent :: !TraceCacheEvent -> TraceEvent blk

-- | The validation policy used when opening an <a>ImmutableDB</a>.
--   
--   The validation policy is used by <a>openDB</a>: the initial opening of
--   the database, either an empty database or a database that was
--   previously closed.
--   
--   The recovery policy dictates which on-disk files should be validated.
data ValidationPolicy

-- | The chunk and index files of the most recent chunk stored on disk will
--   be validated.
--   
--   Prior chunk and index files are ignored, even their presence will not
--   be checked.
--   
--   A <tt>MissingFileError</tt> or an <tt>InvalidFileError</tt> will be
--   thrown in case of a missing or invalid chunk file, or an invalid index
--   file.
--   
--   Because not all files are validated, subsequent operations on the
--   database after opening may result in unexpected errors.
ValidateMostRecentChunk :: ValidationPolicy

-- | The chunk and index files of all chunks starting from the first one up
--   to the last chunk stored on disk will be validated.
--   
--   A <tt>MissingFileError</tt> or an <tt>InvalidFileError</tt> will be
--   thrown in case of a missing or invalid chunk file, or an invalid index
--   file.
ValidateAllChunks :: ValidationPolicy
data Internal m blk
Internal :: (HasCallStack => WithOrigin (Tip blk) -> m ()) -> Internal m blk

-- | Delete everything in the database after the specified tip.
--   
--   PRECONDITION: The tip must correspond to an existing block or genesis.
--   
--   The correctness of open iterators is not guaranteed, they should be
--   closed before calling this operation.
--   
--   Throws a <a>ClosedDBError</a> if the database is closed.
[deleteAfter_] :: Internal m blk -> HasCallStack => WithOrigin (Tip blk) -> m ()

-- | Wrapper around <a>deleteAfter_</a> to ensure <a>HasCallStack</a>
--   constraint
--   
--   See documentation of <a>deleteAfter_</a>.
deleteAfter :: HasCallStack => Internal m blk -> WithOrigin (Tip blk) -> m ()

-- | For testing purposes: exposes internals via <a>Internal</a>
openDBInternal :: forall m blk ans. (IOLike m, GetPrevHash blk, ConvertRawHash blk, ImmutableDbSerialiseConstraints blk, HasCallStack) => ImmutableDbArgs Identity m blk -> (forall h. WithTempRegistry (OpenState m blk h) m ((ImmutableDB m blk, Internal m blk), OpenState m blk h) -> ans) -> ans

module Ouroboros.Consensus.Storage.ImmutableDB

module Ouroboros.Consensus.Util.STM

-- | Specification for a thread that watches a variable, and reports
--   interesting changes.
--   
--   NOTE: STM does not guarantee that <a>wNotify</a> will <i>literally</i>
--   be called on <i>every</i> change: when the system is under heavy load,
--   some updates may be missed.
data Watcher m a fp
Watcher :: (a -> fp) -> Maybe fp -> (a -> m ()) -> STM m a -> Watcher m a fp

-- | Obtain a fingerprint from a value of the monitored variable.
[wFingerprint] :: Watcher m a fp -> a -> fp

-- | The initial fingerprint
--   
--   If <a>Nothing</a>, the action is executed once immediately to obtain
--   the initial fingerprint.
[wInitial] :: Watcher m a fp -> Maybe fp

-- | An action executed each time the fingerprint changes.
[wNotify] :: Watcher m a fp -> a -> m ()

-- | The variable to monitor.
[wReader] :: Watcher m a fp -> STM m a

-- | Spawn a new thread that runs a <a>Watcher</a>
--   
--   The thread will be linked to the registry.
forkLinkedWatcher :: forall m a fp. (IOLike m, Eq fp, HasCallStack) => ResourceRegistry m -> String -> Watcher m a fp -> m (Thread m Void)

-- | Spawn a new thread that runs a <a>Watcher</a>
--   
--   The thread is bracketed via <a>withAsync</a> and <a>link</a>ed.
--   
--   We do not provide the <a>Async</a> handle only because our anticipated
--   use cases don't need it.
withWatcher :: forall m a fp r. (IOLike m, Eq fp, HasCallStack) => String -> Watcher m a fp -> m r -> m r

-- | Simple type that can be used to indicate something in a <tt>TVar</tt>
--   is changed.
newtype Fingerprint
Fingerprint :: Word64 -> Fingerprint

-- | Store a value together with its fingerprint.
data WithFingerprint a
WithFingerprint :: !a -> !Fingerprint -> WithFingerprint a
[forgetFingerprint] :: WithFingerprint a -> !a
[getFingerprint] :: WithFingerprint a -> !Fingerprint
blockUntilAllJust :: MonadSTM m => [STM m (Maybe a)] -> STM m [a]

-- | Wait until the TVar changed
blockUntilChanged :: forall m a b. (MonadSTM m, Eq b) => (a -> b) -> b -> STM m a -> STM m (a, b)
blockUntilJust :: MonadSTM m => STM m (Maybe a) -> STM m a

-- | Spawn a new thread that waits for an STM value to become <a>Just</a>
--   
--   The thread will be linked to the registry.
runWhenJust :: IOLike m => ResourceRegistry m -> String -> STM m (Maybe a) -> (a -> m ()) -> m ()
newtype Sim n m
Sim :: (forall a. n a -> STM m a) -> Sim n m
[runSim] :: Sim n m -> forall a. n a -> STM m a
simId :: Sim (STM m) m
simStateT :: IOLike m => StrictTVar m st -> Sim n m -> Sim (StateT st n) m
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Util.STM.Fingerprint
instance GHC.Enum.Enum Ouroboros.Consensus.Util.STM.Fingerprint
instance GHC.Generics.Generic Ouroboros.Consensus.Util.STM.Fingerprint
instance GHC.Classes.Eq Ouroboros.Consensus.Util.STM.Fingerprint
instance GHC.Show.Show Ouroboros.Consensus.Util.STM.Fingerprint
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.STM.WithFingerprint a)
instance GHC.Generics.Generic (Ouroboros.Consensus.Util.STM.WithFingerprint a)
instance GHC.Base.Functor Ouroboros.Consensus.Util.STM.WithFingerprint
instance GHC.Classes.Eq a => GHC.Classes.Eq (Ouroboros.Consensus.Util.STM.WithFingerprint a)
instance GHC.Show.Show a => GHC.Show.Show (Ouroboros.Consensus.Util.STM.WithFingerprint a)

module Ouroboros.Consensus.BlockchainTime.API

-- | Blockchain time
--   
--   When we run the blockchain, there is a single, global time. We
--   abstract over this here to allow to query this time (in terms of the
--   current slot), and execute an action each time we advance a slot.
data BlockchainTime m
BlockchainTime :: STM m CurrentSlot -> BlockchainTime m

-- | Get current slot
[getCurrentSlot] :: BlockchainTime m -> STM m CurrentSlot
data CurrentSlot

-- | The current slot is known
CurrentSlot :: !SlotNo -> CurrentSlot

-- | The current slot is not yet known
--   
--   This only happens when the tip of the ledger is so far behind that we
--   lack the information necessary to translate the current
--   <tt>UTCTime</tt> into a <a>SlotNo</a>. This should only be the case
--   during syncing.
CurrentSlotUnknown :: CurrentSlot

-- | Watches for changes in the current slot
--   
--   The action will not be called until the current slot becomes known (if
--   the tip of our ledger is too far away from the current wallclock time,
--   we may not know what the current <a>SlotNo</a> is).
knownSlotWatcher :: forall m. IOLike m => BlockchainTime m -> (SlotNo -> m ()) -> Watcher m SlotNo SlotNo
instance NoThunks.Class.NoThunks Ouroboros.Consensus.BlockchainTime.API.CurrentSlot
instance GHC.Show.Show Ouroboros.Consensus.BlockchainTime.API.CurrentSlot
instance GHC.Generics.Generic Ouroboros.Consensus.BlockchainTime.API.CurrentSlot
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.BlockchainTime.API.BlockchainTime m)

module Ouroboros.Consensus.Util.TentativeState

-- | Tentative header state in the context of diffusion pipelining. This is
--   used to check/enforce the monotonicity requirement on invalid
--   tentative block bodies.
--   
--   <ul>
--   <li>During chain selection, we maintain the last invalid tentative
--   header to ensure that the stream of tentative headers we sent
--   downstream whose blocks turned out to be invalid are strictly
--   improving.</li>
--   <li>In the BlockFetch client, we use it to enforce this property for
--   each upstream peer.</li>
--   </ul>
data TentativeState blk
LastInvalidTentative :: !SelectView (BlockProtocol blk) -> TentativeState blk
NoLastInvalidTentative :: TentativeState blk
preferToLastInvalidTentative :: forall blk. LedgerSupportsProtocol blk => BlockConfig blk -> TentativeState blk -> Header blk -> Bool
instance GHC.Generics.Generic (Ouroboros.Consensus.Util.TentativeState.TentativeState blk)
instance GHC.Show.Show (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Show.Show (Ouroboros.Consensus.Util.TentativeState.TentativeState blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => GHC.Classes.Eq (Ouroboros.Consensus.Util.TentativeState.TentativeState blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Util.TentativeState.TentativeState blk)


-- | How to punish the sender of a invalid block
module Ouroboros.Consensus.Storage.ChainDB.API.Types.InvalidBlockPunishment

-- | How to handle a discovered <a>Invalidity</a>
--   
--   This type is opaque because the soundness of the punishment is subtle
--   because of where it is invoked during the chain selection. As a
--   result, arbitrary monadic actions would be foot guns. Instead, this
--   module defines a small DSL for punishment that we judge to be sound.
data InvalidBlockPunishment m
enact :: InvalidBlockPunishment m -> Invalidity -> m ()

-- | Is the added block itself invalid, or is its prefix invalid?
data Invalidity
BlockItself :: Invalidity
BlockPrefix :: Invalidity

-- | Punish according to the <a>Invalidity</a>
branch :: (Invalidity -> InvalidBlockPunishment m) -> InvalidBlockPunishment m

-- | Create a punishment that kills this thread
mkPunishThisThread :: IOLike m => m (InvalidBlockPunishment m)

-- | Allocate a stateful punishment that performs the given punishment
--   unless the given header is better than the previous invocation
mkUnlessImproved :: forall proxy m blk. (IOLike m, NoThunks (SelectView (BlockProtocol blk)), Ord (SelectView (BlockProtocol blk))) => proxy blk -> STM m (SelectView (BlockProtocol blk) -> InvalidBlockPunishment m -> InvalidBlockPunishment m)

-- | A noop punishment
noPunishment :: Applicative m => InvalidBlockPunishment m
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.API.Types.InvalidBlockPunishment.InvalidBlockPunishment m)
instance GHC.Show.Show Ouroboros.Consensus.Storage.ChainDB.API.Types.InvalidBlockPunishment.PeerSentAnInvalidBlockException
instance GHC.Exception.Type.Exception Ouroboros.Consensus.Storage.ChainDB.API.Types.InvalidBlockPunishment.PeerSentAnInvalidBlockException

module Ouroboros.Consensus.Util.Time

-- | Multiply a <a>NominalDiffTime</a> by an integer
--   
--   The right conversions to use are somewhat tricky. The key fact is that
--   <a>fromIntegral</a> interprets its argument as seconds.
multipleNominalDelay :: Integral a => NominalDiffTime -> a -> NominalDiffTime
nominalDelay :: NominalDiffTime -> DiffTime
secondsToNominalDiffTime :: Double -> NominalDiffTime

module Ouroboros.Consensus.BlockchainTime.WallClock.Simple

-- | Real blockchain time
--   
--   WARNING: if the start time is in the future,
--   <a>simpleBlockchainTime</a> will block until the start time has come.
simpleBlockchainTime :: forall m. IOLike m => ResourceRegistry m -> SystemTime m -> SlotLength -> NominalDiffTime -> m (BlockchainTime m)

-- | Get current slot and time spent in that slot
getWallClockSlot :: IOLike m => SystemTime m -> SlotLength -> m (SlotNo, NominalDiffTime)

-- | Wait until the next slot
--   
--   Takes the current slot number to guard against system clock changes.
--   If the clock changes back further than the max clock rewind parameter,
--   a fatal <a>SystemClockMovedBack</a> exception will be thrown. When
--   this exception is thrown, the node will shut down, and should be
--   restarted with (full?) validation enabled: it is conceivable that
--   blocks got moved to the immutable DB that, due to the clock change,
--   should not be considered immutable anymore.
--   
--   If the clock changed back less than the max clock rewind parameter, we
--   stay in the same slot for longer and don't throw an exception.
waitUntilNextSlot :: IOLike m => SystemTime m -> SlotLength -> NominalDiffTime -> SlotNo -> m SlotNo

module Ouroboros.Consensus.BlockchainTime.WallClock.HardFork

-- | A backoff delay
--   
--   If the <tt>horizon</tt> is very far away, the current tip is very far
--   away from the wallclock. However, that probably does not mean we have
--   to wait <tt>now - horizon</tt> time: we are probably just syncing, and
--   so the tip of the ledger will rapidly move forward. So at most <tt>now
--   - horizon</tt> could be used as a heuristic for how long to wait. For
--   now we just trace it.
--   
--   Instead, we just return a fixed delay of <tt>backoffDelay</tt>. There
--   is a trade-off between trying to often, incurring computational
--   overhead, and missing the opportunity to produce a block. For mainnet,
--   we anticipate a 60 second delay will keep both the computational
--   overhead and the number of slots we might miss reasonably small. We
--   anyway can't guarantee the speed of syncing, so delaying it by a
--   further 60 seconds as needed does not change anything fundamentally.
--   
--   (NOTE: We could reduce this delay but Edsko doesn't think it would
--   change very much, and it would increase the frequency of the trace
--   messages and incur computational overhead.)
newtype BackoffDelay
BackoffDelay :: NominalDiffTime -> BackoffDelay
data HardForkBlockchainTimeArgs m blk
HardForkBlockchainTimeArgs :: m BackoffDelay -> STM m (LedgerState blk) -> LedgerConfig blk -> ResourceRegistry m -> SystemTime m -> Tracer m (TraceBlockchainTimeEvent RelativeTime) -> NominalDiffTime -> HardForkBlockchainTimeArgs m blk

-- | See <a>BackoffDelay</a>
[hfbtBackoffDelay] :: HardForkBlockchainTimeArgs m blk -> m BackoffDelay
[hfbtGetLedgerState] :: HardForkBlockchainTimeArgs m blk -> STM m (LedgerState blk)
[hfbtLedgerConfig] :: HardForkBlockchainTimeArgs m blk -> LedgerConfig blk
[hfbtRegistry] :: HardForkBlockchainTimeArgs m blk -> ResourceRegistry m
[hfbtSystemTime] :: HardForkBlockchainTimeArgs m blk -> SystemTime m
[hfbtTracer] :: HardForkBlockchainTimeArgs m blk -> Tracer m (TraceBlockchainTimeEvent RelativeTime)

-- | Maximum time the clock can be rewound without throwing a fatal
--   <a>SystemClockMovedBack</a> exception.
--   
--   When the slot length is short, e.g., Praos' 1s compared to PBFT's 20s,
--   the chances of an NTP sync causing the clock to go back to the
--   previous slot increase.
--   
--   We allow the system clock to rewind up to <a>hfbtMaxClockRewind</a>,
--   tracing a <a>TraceSystemClockMovedBack</a> message in such cases. Note
--   that the current slot *never decreases*, we just wait a bit longer in
--   the same slot.
[hfbtMaxClockRewind] :: HardForkBlockchainTimeArgs m blk -> NominalDiffTime

-- | <a>BlockchainTime</a> instance with support for the hard fork history
hardForkBlockchainTime :: forall m blk. (IOLike m, HasHardForkHistory blk, HasCallStack) => HardForkBlockchainTimeArgs m blk -> m (BlockchainTime m)

module Ouroboros.Consensus.BlockchainTime.WallClock.Default
defaultSystemTime :: (MonadTime m, MonadDelay m) => SystemStart -> Tracer m (TraceBlockchainTimeEvent UTCTime) -> SystemTime m

module Ouroboros.Consensus.BlockchainTime


-- | Intended for qualified import
--   
--   <pre>
--   import Ouroboros.Consensus.Fragment.InFuture (CheckInFuture(..), ClockSkew(..))
--   import qualified Ouroboros.Consensus.Fragment.InFuture as InFuture
--   </pre>
module Ouroboros.Consensus.Fragment.InFuture
data CheckInFuture m blk
CheckInFuture :: (ValidatedFragment (Header blk) (LedgerState blk) -> m (AnchoredFragment (Header blk), [InFuture m blk])) -> CheckInFuture m blk

-- | POSTCONDITION:
--   
--   <pre>
--   checkInFuture vf &gt;&gt;= \(af, fut) -&gt;
--     validatedFragment vf == af &lt;=&gt; null fut
--   </pre>
[checkInFuture] :: CheckInFuture m blk -> ValidatedFragment (Header blk) (LedgerState blk) -> m (AnchoredFragment (Header blk), [InFuture m blk])

-- | Header of block that we found to be in the future
data InFuture m blk
InFuture :: Header blk -> Bool -> InvalidBlockPunishment m -> InFuture m blk

-- | The header itself
[inFutureHeader] :: InFuture m blk -> Header blk

-- | Whether or not this header exceeded the allowed clock skew
--   
--   Headers that do exceed the clock skew should be considered invalid.
[inFutureExceedsClockSkew] :: InFuture m blk -> Bool

-- | <a>blockPunish</a>
[inFuturePunish] :: InFuture m blk -> InvalidBlockPunishment m
reference :: forall m blk. (Monad m, UpdateLedger blk, HasHardForkHistory blk) => LedgerConfig blk -> ClockSkew -> SystemTime m -> CheckInFuture m blk

-- | Specify maximum clock skew in seconds
clockSkewInSeconds :: Double -> ClockSkew

-- | Default maximum permissible clock skew
--   
--   See <a>ClockSkew</a> for details. We allow for 5 seconds skew by
--   default.
defaultClockSkew :: ClockSkew

-- | Maximum permissible clock skew
--   
--   When running NTP, systems clocks will never be perfectly synchronized.
--   The maximum clock skew records how much of a difference we consider
--   acceptable.
--   
--   For example. Suppose
--   
--   <ul>
--   <li>Two nodes A and B</li>
--   <li>A's clock is 0.5 ahead of B's</li>
--   <li>A produces a block and sends it to B</li>
--   <li>When B translates the <a>SlotNo</a> of that block to a time, it
--   may find that it is 0.5 seconds ahead of its current clock (worst
--   case).</li>
--   </ul>
--   
--   The maximum permissible clock skew decides if B will consider this
--   block to be valid (even if it will not yet consider it for chain
--   seleciton) or as invalid (and disconnect from A, since A is sending it
--   invalid blocks).
--   
--   Use <a>defaultClockSkew</a> when unsure.
data ClockSkew
unClockSkew :: ClockSkew -> NominalDiffTime

-- | Trivial <a>InFuture</a> check that doesn't do any check at all
--   
--   This is useful for testing and tools such as the DB converter.
dontCheck :: Monad m => CheckInFuture m blk

-- | If by some miracle we have a function that can always tell us what the
--   correct slot is, implementing <a>CheckInFuture</a> is easy
--   
--   NOTE: Use of <a>miracle</a> in tests means that none of the hard fork
--   infrastructure for converting slots to time is tested.
miracle :: forall m blk. (MonadSTM m, HasHeader (Header blk)) => STM m SlotNo -> Word64 -> CheckInFuture m blk
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Fragment.InFuture.CheckInFuture m blk)
instance GHC.Classes.Ord Ouroboros.Consensus.Fragment.InFuture.ClockSkew
instance GHC.Classes.Eq Ouroboros.Consensus.Fragment.InFuture.ClockSkew
instance GHC.Show.Show Ouroboros.Consensus.Fragment.InFuture.ClockSkew

module Ouroboros.Consensus.MiniProtocol.ChainSync.Client.InFutureCheck

-- | The interface a ChainSync client needs in order to check the arrival
--   time of headers.
--   
--   Instead of alphabetical, the fields are in the order in which the
--   ChainSync client logic will invoke them for each header.
data HeaderInFutureCheck m blk arrival judgment
HeaderInFutureCheck :: Proxy arrival -> (Header blk -> m arrival) -> (LedgerConfig blk -> LedgerState blk -> arrival -> Except PastHorizonException judgment) -> (judgment -> m (Maybe HeaderArrivalException)) -> HeaderInFutureCheck m blk arrival judgment
[proxyArrival] :: HeaderInFutureCheck m blk arrival judgment -> Proxy arrival

-- | This is ideally called _immediately_ upon the header arriving.
[recordHeaderArrival] :: HeaderInFutureCheck m blk arrival judgment -> Header blk -> m arrival

-- | Judge what to do about the header's arrival time.
--   
--   Note that this may be called after a delay, hence <tt>arrival</tt>
--   contains at least the arrival time.
--   
--   In particular, such a delay might be caused by waiting for the
--   intersection with the local selection to change after this function
--   returns <a>HistoryPastHorizon</a>.
[judgeHeaderArrival] :: HeaderInFutureCheck m blk arrival judgment -> LedgerConfig blk -> LedgerState blk -> arrival -> Except PastHorizonException judgment

-- | Enact the judgment.
--   
--   If <tt>Just</tt> is returned, an exception should be raised.
[handleHeaderArrival] :: HeaderInFutureCheck m blk arrival judgment -> judgment -> m (Maybe HeaderArrivalException)
data SomeHeaderInFutureCheck m blk
SomeHeaderInFutureCheck :: HeaderInFutureCheck m blk arrival judgment -> SomeHeaderInFutureCheck m blk
data HeaderArrivalException

-- | The header arrived so early that its issuer either minted it before
--   their clock reached its slot onset or else the difference between
--   their clock and ours is more severe than we're configured to tolerate.
--   
--   INVARIANT: <tt><a>tolerableClockSkew</a> &lt; negate
--   <a>ageUponArrival</a></tt>
FarFutureHeaderException :: !NominalDiffTime -> !RealPoint blk -> !RelativeTime -> !NominalDiffTime -> HeaderArrivalException
[ageUponArrival] :: HeaderArrivalException -> !NominalDiffTime
[arrivedPoint] :: HeaderArrivalException -> !RealPoint blk
[arrivalTime] :: HeaderArrivalException -> !RelativeTime
[tolerableClockSkew] :: HeaderArrivalException -> !NominalDiffTime
realHeaderInFutureCheck :: (HasHeader blk, HasHeader (Header blk), HasHardForkHistory blk, MonadDelay m) => ClockSkew -> SystemTime m -> SomeHeaderInFutureCheck m blk
instance GHC.Show.Show Ouroboros.Consensus.MiniProtocol.ChainSync.Client.InFutureCheck.HeaderArrivalException
instance GHC.Exception.Type.Exception Ouroboros.Consensus.MiniProtocol.ChainSync.Client.InFutureCheck.HeaderArrivalException
instance GHC.Classes.Eq Ouroboros.Consensus.MiniProtocol.ChainSync.Client.InFutureCheck.HeaderArrivalException

module Ouroboros.Consensus.Config.SupportsNode

-- | The <a>BlockConfig</a> needs to contain some information in order to
--   support running a node.
class ConfigSupportsNode blk
getSystemStart :: ConfigSupportsNode blk => BlockConfig blk -> SystemStart
getNetworkMagic :: ConfigSupportsNode blk => BlockConfig blk -> NetworkMagic

module Ouroboros.Consensus.Ledger.Query

-- | Different queries supported by the ledger, indexed by the result type.
data family BlockQuery blk :: Type -> Type

-- | The <a>BlockConfig</a> needs to contain some information in order to
--   support running a node.
class ConfigSupportsNode blk
getSystemStart :: ConfigSupportsNode blk => BlockConfig blk -> SystemStart
getNetworkMagic :: ConfigSupportsNode blk => BlockConfig blk -> NetworkMagic

-- | Different queries supported by the ledger for all block types, indexed
--   by the result type.
--   
--   Additions to the set of queries is versioned by <a>QueryVersion</a>
data Query blk result

-- | This constructor is supported by all <tt>QueryVersion</tt>s. The
--   <tt>BlockQuery</tt> argument is versioned by the
--   <tt>BlockNodeToClientVersion blk</tt>.
[BlockQuery] :: BlockQuery blk result -> Query blk result

-- | Get the <a>SystemStart</a> time.
--   
--   Supported by <a>QueryVersion</a> &gt;= <a>QueryVersion1</a>.
[GetSystemStart] :: Query blk SystemStart

-- | Get the <a>GetChainBlockNo</a> time.
--   
--   Supported by <a>QueryVersion</a> &gt;= <a>QueryVersion2</a>.
[GetChainBlockNo] :: Query blk (WithOrigin BlockNo)

-- | Get the <a>GetChainPoint</a> time.
--   
--   Supported by <a>QueryVersion</a> &gt;= <a>QueryVersion2</a>.
[GetChainPoint] :: Query blk (Point blk)

-- | Query the ledger extended state.
--   
--   Used by the LocalStateQuery protocol to allow clients to query the
--   extended ledger state.
class (ShowQuery (BlockQuery blk), SameDepIndex (BlockQuery blk)) => QueryLedger blk

-- | Answer the given query about the extended ledger state.
answerBlockQuery :: QueryLedger blk => ExtLedgerCfg blk -> BlockQuery blk result -> ExtLedgerState blk -> result

-- | Version of the `Query blk` type.
--   
--   Multiple top level queries are now supported. The encoding now has
--   constructor tags for the different top level queries for QueryVersion1
--   onwards.
data QueryVersion
QueryVersion1 :: QueryVersion
QueryVersion2 :: QueryVersion
class forall result. () => Show query result => ShowQuery (query :: Type -> Type)
showResult :: ShowQuery query => query result -> result -> String

-- | Answer the given query about the extended ledger state.
answerQuery :: (QueryLedger blk, ConfigSupportsNode blk, HasAnnTip blk) => ExtLedgerCfg blk -> Query blk result -> ExtLedgerState blk -> result

-- | Get the <tt>QueryVersion</tt> supported by this
--   <tt>NodeToClientVersion</tt>.
nodeToClientVersionToQueryVersion :: NodeToClientVersion -> QueryVersion
queryDecodeNodeToClient :: forall blk. SerialiseNodeToClient blk (SomeSecond BlockQuery blk) => CodecConfig blk -> QueryVersion -> BlockNodeToClientVersion blk -> forall s. Decoder s (SomeSecond Query blk)
queryEncodeNodeToClient :: forall blk. Typeable blk => Show (SomeSecond BlockQuery blk) => SerialiseNodeToClient blk (SomeSecond BlockQuery blk) => CodecConfig blk -> QueryVersion -> BlockNodeToClientVersion blk -> SomeSecond Query blk -> Encoding
instance GHC.Show.Show (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery blk) => GHC.Show.Show (Ouroboros.Consensus.Ledger.Query.QueryEncoderException blk)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.Query.BlockQuery blk result) => GHC.Show.Show (Ouroboros.Consensus.Ledger.Query.Query blk result)
instance (forall result. GHC.Show.Show (Ouroboros.Consensus.Ledger.Query.BlockQuery blk result)) => GHC.Show.Show (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery blk)
instance (Data.Typeable.Internal.Typeable blk, GHC.Show.Show (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery blk)) => GHC.Exception.Type.Exception (Ouroboros.Consensus.Ledger.Query.QueryEncoderException blk)
instance Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.Query.BlockQuery blk) => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.Query.Query blk)
instance (Ouroboros.Network.Protocol.LocalStateQuery.Type.ShowQuery (Ouroboros.Consensus.Ledger.Query.BlockQuery blk), Ouroboros.Network.Block.StandardHash blk) => Ouroboros.Network.Protocol.LocalStateQuery.Type.ShowQuery (Ouroboros.Consensus.Ledger.Query.Query blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery blk) => GHC.Classes.Eq (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.Query blk)
instance GHC.Show.Show (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery blk) => GHC.Show.Show (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.Query blk)
instance (Ouroboros.Consensus.Node.Serialisation.SerialiseResult blk (Ouroboros.Consensus.Ledger.Query.BlockQuery blk), Codec.Serialise.Class.Serialise (Ouroboros.Network.Block.HeaderHash blk)) => Ouroboros.Consensus.Node.Serialisation.SerialiseResult blk (Ouroboros.Consensus.Ledger.Query.Query blk)
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Ledger.Query.BlockQuery blk) => Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Ledger.Query.Query blk)
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Ledger.Query.BlockQuery blk) => GHC.Classes.Eq (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery blk)

module Ouroboros.Consensus.MiniProtocol.LocalStateQuery.Server
localStateQueryServer :: forall m blk. (IOLike m, QueryLedger blk, ConfigSupportsNode blk, HasAnnTip blk) => ExtLedgerCfg blk -> STM m (Point blk) -> (Point blk -> STM m (Maybe (ExtLedgerState blk))) -> STM m (Point blk) -> LocalStateQueryServer blk (Point blk) (Query blk) m ()

module Ouroboros.Consensus.Ledger.Dual

-- | Bridge the two ledgers
class (HasHeader m, GetHeader m, HasHeader (Header m), LedgerSupportsProtocol m, HasHardForkHistory m, LedgerSupportsMempool m, CommonProtocolParams m, HasTxId (GenTx m), Show (ApplyTxErr m), Typeable a, UpdateLedger a, LedgerSupportsMempool a, Show (ApplyTxErr a), NoThunks (LedgerConfig a), NoThunks (CodecConfig a), NoThunks (StorageConfig a), Show (BridgeLedger m a), Eq (BridgeLedger m a), Serialise (BridgeLedger m a), Serialise (BridgeBlock m a), Serialise (BridgeTx m a), Show (BridgeTx m a)) => Bridge m a where {
    
    -- | Additional information relating both ledgers
    type BridgeLedger m a :: Type;
    
    -- | Information required to update the bridge when applying a block
    type BridgeBlock m a :: Type;
    
    -- | Information required to update the bridge when applying a transaction
    type BridgeTx m a :: Type;
}
updateBridgeWithBlock :: Bridge m a => DualBlock m a -> BridgeLedger m a -> BridgeLedger m a
updateBridgeWithTx :: Bridge m a => Validated (GenTx (DualBlock m a)) -> BridgeLedger m a -> BridgeLedger m a

-- | Dual block
--   
--   The dual block is used to instantiate the consensus with a dual
--   ledger, consisting of two ledger states associated with two types of
--   blocks. The (consensus) chain state will still be updated based on one
--   block type only, which is therefore designed as the <i>main</i> block,
--   while the other block is designated as the <i>auxiliary</i> block.
--   
--   The auxiliary block is optional; this can be used if some " main "
--   blocks should have no effect on the auxiliary ledger state at all. The
--   motivating example is EBBs: if the main blocks are real Byron blocks,
--   and the auxiliary blocks are Byron spec blocks, then regular Byron
--   blocks correspond to Byron spec blocks, but EBBs don't correspond to a
--   spec block at all and should leave the Byron spec ledger state
--   unchanged.
--   
--   NOTE: The dual ledger is used for testing purposes only; we do not do
--   any meaningful <a>NoThunks</a> checks here.
data DualBlock m a
DualBlock :: m -> Maybe a -> BridgeBlock m a -> DualBlock m a
[dualBlockMain] :: DualBlock m a -> m
[dualBlockAux] :: DualBlock m a -> Maybe a
[dualBlockBridge] :: DualBlock m a -> BridgeBlock m a
data DualGenTxErr m a
DualGenTxErr :: ApplyTxErr m -> ApplyTxErr a -> DualGenTxErr m a
[dualGenTxErrMain] :: DualGenTxErr m a -> ApplyTxErr m
[dualGenTxErrAux] :: DualGenTxErr m a -> ApplyTxErr a
type DualHeader m a = Header (DualBlock m a)
data DualLedgerConfig m a
DualLedgerConfig :: LedgerConfig m -> LedgerConfig a -> DualLedgerConfig m a
[dualLedgerConfigMain] :: DualLedgerConfig m a -> LedgerConfig m
[dualLedgerConfigAux] :: DualLedgerConfig m a -> LedgerConfig a

-- | Both ledger rules threw an error
--   
--   We do not verify that the errors agree, merely that they both report
--   <i>some</i> error.
--   
--   If only <i>one</i> of the two semantics reports an error, we fail with
--   an <a>error</a> (see <a>agreeOnError</a>), rather than a regular chain
--   failure; if this happens, it indicates a bug, and the node should fail
--   (rather than just, for example, reject a block).
data DualLedgerError m a
DualLedgerError :: LedgerError m -> LedgerError a -> DualLedgerError m a
[dualLedgerErrorMain] :: DualLedgerError m a -> LedgerError m
[dualLedgerErrorAux] :: DualLedgerError m a -> LedgerError a
ctxtDualMain :: NestedCtxt_ (DualBlock m a) f x -> NestedCtxt_ m f x
dualExtValidationErrorMain :: ExtValidationError (DualBlock m a) -> ExtValidationError m

-- | This is only used for block production
dualTopLevelConfigMain :: TopLevelConfig (DualBlock m a) -> TopLevelConfig m

-- | Static configuration required to work with this type of blocks
data family BlockConfig blk :: Type

-- | Static configuration required for serialisation and deserialisation of
--   types pertaining to this type of block.
--   
--   Data family instead of type family to get better type inference.
data family CodecConfig blk :: Type

-- | Generalized transaction
--   
--   The mempool (and, accordingly, blocks) consist of "generalized
--   transactions"; this could be "proper" transactions (transferring
--   funds) but also other kinds of things such as update proposals,
--   delegations, etc.
data family GenTx blk :: Type
data family Header blk :: Type

-- | Ledger state associated with a block
data family LedgerState blk :: Type

-- | Context identifying what kind of block we have
--   
--   In almost all places we will use <a>NestedCtxt</a> rather than
--   <a>NestedCtxt_</a>.
data family NestedCtxt_ blk :: (Type -> Type) -> (Type -> Type)

-- | Config needed for the <a>NodeInitStorage</a> class. Defined here to
--   avoid circular dependencies.
data family StorageConfig blk :: Type

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type

-- | A generalized transaction, <a>GenTx</a>, identifier.
data family TxId tx :: Type

-- | " Validated " transaction or block
--   
--   The ledger defines how to validate transactions and blocks. It's
--   possible the type before and after validation may be distinct (eg
--   Alonzo transactions), which originally motivated this family.
--   
--   We also gain the related benefit that certain interface functions,
--   such as those that <i>reapply</i> blocks, can have a more precise type
--   now. TODO
--   
--   Similarly, the Node-to-Client mini protocols can explicitly indicate
--   that the client trusts the blocks from the local server, by having the
--   server send <a>Validated</a> blocks to the client. TODO
--   
--   Note that validation has different implications for a transaction than
--   for a block. In particular, a validated transaction can be " reapplied
--   " to different ledger states, whereas a validated block must only be "
--   reapplied " to the exact same ledger state (eg as part of rebuilding
--   from an on-disk ledger snapshot).
--   
--   Since the ledger defines validation, see the ledger details for
--   concrete examples of what determines the validity (wrt to a
--   <a>LedgerState</a>) of a transaction and/or block. Example properties
--   include: a transaction's claimed inputs exist and are still unspent, a
--   block carries a sufficient cryptographic signature, etc.
data family Validated x :: Type
decodeDualBlock :: (Bridge m a, Serialise a) => Decoder s (ByteString -> m) -> Decoder s (ByteString -> DualBlock m a)
decodeDualGenTx :: (Bridge m a, Serialise (GenTx a)) => Decoder s (GenTx m) -> Decoder s (GenTx (DualBlock m a))
decodeDualGenTxErr :: Serialise (ApplyTxErr a) => Decoder s (ApplyTxErr m) -> Decoder s (ApplyTxErr (DualBlock m a))
decodeDualGenTxId :: Decoder s (GenTxId m) -> Decoder s (GenTxId (DualBlock m a))
decodeDualHeader :: Decoder s (ByteString -> Header m) -> Decoder s (ByteString -> Header (DualBlock m a))
decodeDualLedgerState :: (Bridge m a, Serialise (LedgerState a)) => Decoder s (LedgerState m) -> Decoder s (LedgerState (DualBlock m a))
encodeDualBlock :: (Bridge m a, Serialise a) => (m -> Encoding) -> DualBlock m a -> Encoding
encodeDualGenTx :: (Bridge m a, Serialise (GenTx a)) => (GenTx m -> Encoding) -> GenTx (DualBlock m a) -> Encoding
encodeDualGenTxErr :: Serialise (ApplyTxErr a) => (ApplyTxErr m -> Encoding) -> ApplyTxErr (DualBlock m a) -> Encoding
encodeDualGenTxId :: (GenTxId m -> Encoding) -> GenTxId (DualBlock m a) -> Encoding
encodeDualHeader :: (Header m -> Encoding) -> Header (DualBlock m a) -> Encoding
encodeDualLedgerState :: (Bridge m a, Serialise (LedgerState a)) => (LedgerState m -> Encoding) -> LedgerState (DualBlock m a) -> Encoding
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.BlockConfig (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance GHC.Generics.Generic (Ouroboros.Consensus.Block.Abstract.CodecConfig (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance GHC.Generics.Generic (Ouroboros.Consensus.Block.Abstract.StorageConfig (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ticked.Ticked (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.Ledger.Dual.DualBlock m a) result)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Dual.DualLedgerError m a)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Dual.DualLedgerConfig m a)
instance (GHC.Show.Show m, GHC.Show.Show a, GHC.Show.Show (Ouroboros.Consensus.Ledger.Dual.BridgeBlock m a)) => GHC.Show.Show (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance (GHC.Classes.Eq m, GHC.Classes.Eq a, GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Dual.BridgeBlock m a)) => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header m) => GHC.Show.Show (Ouroboros.Consensus.Ledger.Dual.DualHeader m a)
instance (GHC.Show.Show (Ouroboros.Consensus.Ledger.Basics.LedgerError m), GHC.Show.Show (Ouroboros.Consensus.Ledger.Basics.LedgerError a)) => GHC.Show.Show (Ouroboros.Consensus.Ledger.Dual.DualLedgerError m a)
instance (GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Basics.LedgerError m), GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Basics.LedgerError a)) => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Dual.DualLedgerError m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => GHC.Show.Show (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => GHC.Show.Show (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => GHC.Show.Show (Ouroboros.Consensus.Ledger.Dual.DualGenTxErr m a)
instance GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId m) => GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId m) => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance GHC.Classes.Ord (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId m) => GHC.Classes.Ord (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance GHC.Show.Show (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ m f x) => GHC.Show.Show (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ (Ouroboros.Consensus.Ledger.Dual.DualBlock m a) f x)
instance (Data.Typeable.Internal.Typeable m, Data.Typeable.Internal.Typeable a) => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.Dual.DualGenTxErr m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.SupportsMempool.LedgerSupportsMempool (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.Basics.IsLedger (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.Abstract.ApplyBlock (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)) (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.HardFork.Abstract.HasHardForkHistory (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance (Data.Typeable.Internal.Typeable m, Data.Typeable.Internal.Typeable a) => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.Dual.DualHeader m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Network.Block.HasHeader (Ouroboros.Consensus.Ledger.Dual.DualHeader m a)
instance (Data.Typeable.Internal.Typeable m, Data.Typeable.Internal.Typeable a) => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Util.Condense.Condense m => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Network.Block.StandardHash m => Ouroboros.Network.Block.StandardHash (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Block.Abstract.ConvertRawHash m => Ouroboros.Consensus.Block.Abstract.ConvertRawHash (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Block.Abstract.GetHeader (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Config.SupportsNode.ConfigSupportsNode m => Ouroboros.Consensus.Config.SupportsNode.ConfigSupportsNode (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance (NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.CodecConfig m), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.CodecConfig a)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.CodecConfig (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance (NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.StorageConfig m), NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.StorageConfig a)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.StorageConfig (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Network.Block.HasHeader (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Block.Abstract.GetPrevHash (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.Basics.GetTip (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.Basics.GetTip (Ouroboros.Consensus.Ticked.Ticked (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.Abstract.UpdateLedger (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.HeaderValidation.HasAnnTip (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.HeaderValidation.BasicEnvelopeValidation (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.HeaderValidation.ValidateEnvelope (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance (Data.Typeable.Internal.Typeable m, Data.Typeable.Internal.Typeable a) => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.Query.QueryLedger (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Network.Protocol.LocalStateQuery.Type.ShowQuery (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.CommonProtocolParams.CommonProtocolParams (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance (Data.Typeable.Internal.Typeable m, Data.Typeable.Internal.Typeable a) => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance (Data.Typeable.Internal.Typeable m, Data.Typeable.Internal.Typeable a) => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)))
instance Ouroboros.Consensus.Ledger.Dual.Bridge m a => Ouroboros.Consensus.Ledger.SupportsMempool.HasTxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.Ledger.Dual.DualBlock m a))
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ m f) => Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ (Ouroboros.Consensus.Ledger.Dual.DualBlock m a) f)
instance Ouroboros.Consensus.Block.NestedContent.HasNestedContent Ouroboros.Consensus.Block.Abstract.Header m => Ouroboros.Consensus.Block.NestedContent.HasNestedContent Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Storage.Serialisation.ReconstructNestedCtxt Ouroboros.Consensus.Block.Abstract.Header m => Ouroboros.Consensus.Storage.Serialisation.ReconstructNestedCtxt Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDepIx (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) m => Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDepIx (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDep (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) m => Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDep (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Storage.Serialisation.HasBinaryBlockInfo m => Ouroboros.Consensus.Storage.Serialisation.HasBinaryBlockInfo (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.Inspect.InspectLedger m => Ouroboros.Consensus.Ledger.Inspect.InspectLedger (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)
instance Ouroboros.Consensus.Ledger.SupportsPeerSelection.LedgerSupportsPeerSelection m => Ouroboros.Consensus.Ledger.SupportsPeerSelection.LedgerSupportsPeerSelection (Ouroboros.Consensus.Ledger.Dual.DualBlock m a)

module Ouroboros.Consensus.Util.Versioned

-- | How to decode a version of a format.
data VersionDecoder a

-- | This version is incompatible, fail with <a>IncompatibleVersion</a> and
--   the given message.
[Incompatible] :: String -> VersionDecoder a

-- | Decode the version using the given <a>Decoder</a>.
[Decode] :: (forall s. Decoder s a) -> VersionDecoder a

-- | Decode an other format (<tt>from</tt>) and migrate from that. When
--   migration fails, the version decoder will fail with
--   <tt>MigrationFailed</tt>.
[Migrate] :: VersionDecoder from -> (from -> Either String to) -> VersionDecoder to
data VersionError

-- | We cannot deserialise the version of the data with the given
--   <a>VersionNumber</a> because its data format is incompatible.
--   
--   For example, the given format lacks data that was added in later
--   version that cannot be reconstructed from scratch.
IncompatibleVersion :: VersionNumber -> String -> VersionError

-- | The given <a>VersionNumber</a> is unknown and thus not supported.
UnknownVersion :: VersionNumber -> VersionError

-- | A migration from the given <a>VersionNumber</a> failed. See
--   <a>Migrate</a>.
MigrationFailed :: VersionNumber -> String -> VersionError
data Versioned a
Versioned :: !VersionNumber -> !a -> Versioned a
[versionNumber] :: Versioned a -> !VersionNumber
[versioned] :: Versioned a -> !a

-- | Decode a <i>versioned</i> <tt>a</tt> (encoded using
--   <a>encodeVersion</a> or <a>encodeVersioned</a>).
--   
--   The corresponding <a>VersionDecoder</a> for the deserialised
--   <a>VersionNumber</a> is looked up in the given list. The first match
--   is used (using the semantics of <a>lookup</a>). When no match is
--   found, a decoder that fails with <a>UnknownVersion</a> is returned.
decodeVersion :: [(VersionNumber, VersionDecoder a)] -> forall s. Decoder s a

-- | Same as <a>decodeVersion</a>, but with a hook that gets called in case
--   the encoding was not produced by a versioned encoder. This allows a
--   transition from non-versioned to versioned encodings.
--   
--   Versioned encodings start with list length 2. Whenever the encoding
--   starts this way, this decoder will use the regular versioned decoder.
--   When the encoding starts differently, either with a different list
--   length (<a>Just</a> as argument) or with another token (<a>Nothing</a>
--   as argument), the hook is called, allowing the previous non-versioned
--   decoder to try to decode the encoding.
--   
--   Note that the hook should <i>not</i> try to decode the list length
--   <i>again</i>.
--   
--   Note that this will not work if the previous encoding can start with
--   list length 2, as the new versioned decoder will be called in those
--   cases, not the hook.
decodeVersionWithHook :: forall a. (forall s. Maybe Int -> Decoder s a) -> [(VersionNumber, VersionDecoder a)] -> forall s. Decoder s a
decodeVersioned :: [(VersionNumber, VersionDecoder a)] -> forall s. Decoder s (Versioned a)

-- | Given a <a>VersionNumber</a> and the encoding of an <tt>a</tt>, encode
--   the corresponding <tt><a>Versioned</a> a</tt>. Use
--   <a>decodeVersion</a> to decode it.
encodeVersion :: VersionNumber -> Encoding -> Encoding
encodeVersioned :: (a -> Encoding) -> Versioned a -> Encoding
data VersionNumber
instance GHC.Show.Show Ouroboros.Consensus.Util.Versioned.VersionNumber
instance GHC.Num.Num Ouroboros.Consensus.Util.Versioned.VersionNumber
instance GHC.Classes.Ord Ouroboros.Consensus.Util.Versioned.VersionNumber
instance GHC.Classes.Eq Ouroboros.Consensus.Util.Versioned.VersionNumber
instance GHC.Show.Show a => GHC.Show.Show (Ouroboros.Consensus.Util.Versioned.Versioned a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Ouroboros.Consensus.Util.Versioned.Versioned a)
instance GHC.Exception.Type.Exception Ouroboros.Consensus.Util.Versioned.VersionError
instance GHC.Show.Show Ouroboros.Consensus.Util.Versioned.VersionError
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.Util.Versioned.VersionNumber

module Ouroboros.Consensus.Storage.LedgerDB.Snapshots
data DiskSnapshot
DiskSnapshot :: Word64 -> Maybe String -> DiskSnapshot

-- | Snapshots are numbered. We will try the snapshots with the highest
--   number first.
--   
--   When creating a snapshot, we use the slot number of the ledger state
--   it corresponds to as the snapshot number. This gives an indication of
--   how recent the snapshot is.
--   
--   Note that the snapshot names are only indicative, we don't rely on the
--   snapshot number matching the slot number of the corresponding ledger
--   state. We only use the snapshots numbers to determine the order in
--   which we try them.
[dsNumber] :: DiskSnapshot -> Word64

-- | Snapshots can optionally have a suffix, separated by the snapshot
--   number with an underscore, e.g., <tt>4492799_last_Byron</tt>. This
--   suffix acts as metadata for the operator of the node. Snapshots with a
--   suffix will <i>not be trimmed</i>.
[dsSuffix] :: DiskSnapshot -> Maybe String
data SnapshotFailure blk

-- | We failed to deserialise the snapshot
--   
--   This can happen due to data corruption in the ledger DB.
InitFailureRead :: ReadIncrementalErr -> SnapshotFailure blk

-- | This snapshot is too recent (ahead of the tip of the chain)
InitFailureTooRecent :: RealPoint blk -> SnapshotFailure blk

-- | This snapshot was of the ledger state at genesis, even though we never
--   take snapshots at genesis, so this is unexpected.
InitFailureGenesis :: SnapshotFailure blk

-- | The snapshots that are periodically created are temporary, they will
--   be deleted when trimming
diskSnapshotIsTemporary :: DiskSnapshot -> Bool

-- | List on-disk snapshots, highest number first.
listSnapshots :: Monad m => SomeHasFS m -> m [DiskSnapshot]

-- | Read snapshot from disk
readSnapshot :: forall m blk. IOLike m => SomeHasFS m -> (forall s. Decoder s (ExtLedgerState blk)) -> (forall s. Decoder s (HeaderHash blk)) -> DiskSnapshot -> ExceptT ReadIncrementalErr m (ExtLedgerState blk)

-- | Take a snapshot of the <i>oldest ledger state</i> in the ledger DB
--   
--   We write the <i>oldest</i> ledger state to disk because the intention
--   is to only write ledger states to disk that we know to be immutable.
--   Primarily for testing purposes, <a>takeSnapshot</a> returns the block
--   reference corresponding to the snapshot that we wrote.
--   
--   If a snapshot with the same number already exists on disk or if the
--   tip is at genesis, no snapshot is taken.
--   
--   Note that an EBB can have the same slot number and thus snapshot
--   number as the block after it. This doesn't matter. The one block
--   difference in the ledger state doesn't warrant an additional snapshot.
--   The number in the name of the snapshot is only indicative, we don't
--   rely on it being correct.
--   
--   NOTE: This is a lower-level API that takes a snapshot independent from
--   whether this snapshot corresponds to a state that is more than
--   <tt>k</tt> back.
--   
--   TODO: Should we delete the file if an error occurs during writing?
takeSnapshot :: forall m blk. (MonadThrow m, IsLedger (LedgerState blk)) => Tracer m (TraceSnapshotEvent blk) -> SomeHasFS m -> (ExtLedgerState blk -> Encoding) -> ExtLedgerState blk -> m (Maybe (DiskSnapshot, RealPoint blk))

-- | Trim the number of on disk snapshots so that at most
--   <a>onDiskNumSnapshots</a> snapshots are stored on disk. The oldest
--   snapshots are deleted.
--   
--   The deleted snapshots are returned.
trimSnapshots :: Monad m => Tracer m (TraceSnapshotEvent r) -> SomeHasFS m -> DiskPolicy -> m [DiskSnapshot]

-- | Write snapshot to disk
writeSnapshot :: forall m blk. MonadThrow m => SomeHasFS m -> (ExtLedgerState blk -> Encoding) -> DiskSnapshot -> ExtLedgerState blk -> m ()

-- | To remain backwards compatible with existing snapshots stored on disk,
--   we must accept the old format as well as the new format.
--   
--   The old format: * The tip: <tt>WithOrigin (RealPoint blk)</tt> * The
--   chain length: <tt>Word64</tt> * The ledger state: <tt>l</tt>
--   
--   The new format is described by <a>snapshotEncodingVersion1</a>.
--   
--   This decoder will accept and ignore them. The encoder
--   (<a>encodeSnapshot</a>) will no longer encode them.
decodeSnapshotBackwardsCompatible :: forall l blk. Proxy blk -> (forall s. Decoder s l) -> (forall s. Decoder s (HeaderHash blk)) -> forall s. Decoder s l

-- | Delete snapshot from disk
deleteSnapshot :: HasCallStack => SomeHasFS m -> DiskSnapshot -> m ()

-- | Encoder to be used in combination with
--   <a>decodeSnapshotBackwardsCompatible</a>.
encodeSnapshot :: (l -> Encoding) -> l -> Encoding
snapshotToFileName :: DiskSnapshot -> String
snapshotToPath :: DiskSnapshot -> FsPath
data TraceSnapshotEvent blk

-- | An on disk snapshot was skipped because it was invalid.
InvalidSnapshot :: DiskSnapshot -> SnapshotFailure blk -> TraceSnapshotEvent blk

-- | A snapshot was written to disk.
TookSnapshot :: DiskSnapshot -> RealPoint blk -> TraceSnapshotEvent blk

-- | An old or invalid on-disk snapshot was deleted
DeletedSnapshot :: DiskSnapshot -> TraceSnapshotEvent blk
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.Snapshots.SnapshotFailure blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Snapshots.SnapshotFailure blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Snapshots.SnapshotFailure blk)
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.LedgerDB.Snapshots.DiskSnapshot
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.LedgerDB.Snapshots.DiskSnapshot
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.LedgerDB.Snapshots.DiskSnapshot
instance GHC.Show.Show Ouroboros.Consensus.Storage.LedgerDB.Snapshots.DiskSnapshot
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Snapshots.TraceSnapshotEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Snapshots.TraceSnapshotEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.Snapshots.TraceSnapshotEvent blk)


-- | LedgerDB initialization either from a LedgerState or from a
--   DiskSnapshot
module Ouroboros.Consensus.Storage.LedgerDB.Init

-- | Initialization log
--   
--   The initialization log records which snapshots from disk were
--   considered, in which order, and why some snapshots were rejected. It
--   is primarily useful for monitoring purposes.
data InitLog blk

-- | Defaulted to initialization from genesis
--   
--   NOTE: Unless the blockchain is near genesis, we should see this
--   <i>only</i> if data corrupted occurred.
InitFromGenesis :: InitLog blk

-- | Used a snapshot corresponding to the specified tip
InitFromSnapshot :: DiskSnapshot -> RealPoint blk -> InitLog blk

-- | Initialization skipped a snapshot
--   
--   We record the reason why it was skipped.
--   
--   NOTE: We should <i>only</i> see this if data corrupted occurred.
InitFailure :: DiskSnapshot -> SnapshotFailure blk -> InitLog blk -> InitLog blk

-- | Which point the replay started from
newtype ReplayStart blk
ReplayStart :: Point blk -> ReplayStart blk

-- | Initialize the ledger DB from the most recent snapshot on disk
--   
--   If no such snapshot can be found, use the genesis ledger DB. Returns
--   the initialized DB as well as the block reference corresponding to the
--   snapshot we found on disk (the latter primarily for testing/monitoring
--   purposes).
--   
--   We do <i>not</i> catch any exceptions thrown during streaming; should
--   any be thrown, it is the responsibility of the <tt>ChainDB</tt> to
--   catch these and trigger (further) validation. We only discard
--   snapshots if
--   
--   <ul>
--   <li>We cannot deserialise them, or</li>
--   <li>they are <i>ahead</i> of the chain</li>
--   </ul>
--   
--   It is possible that the Ledger DB will not be able to roll back
--   <tt>k</tt> blocks after initialization if the chain has been truncated
--   (data corruption).
--   
--   We do <i>not</i> attempt to use multiple ledger states from disk to
--   construct the ledger DB. Instead we load only a <i>single</i> ledger
--   state from disk, and <i>compute</i> all subsequent ones. This is
--   important, because the ledger states obtained in this way will
--   (hopefully) share much of their memory footprint with their
--   predecessors.
initLedgerDB :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasCallStack) => Tracer m (ReplayGoal blk -> TraceReplayEvent blk) -> Tracer m (TraceSnapshotEvent blk) -> SomeHasFS m -> (forall s. Decoder s (ExtLedgerState blk)) -> (forall s. Decoder s (HeaderHash blk)) -> LedgerDbCfg (ExtLedgerState blk) -> m (ExtLedgerState blk) -> StreamAPI m blk -> m (InitLog blk, LedgerDB' blk, Word64)

-- | Which point the replay is expected to end at
newtype ReplayGoal blk
ReplayGoal :: Point blk -> ReplayGoal blk

-- | Events traced while replaying blocks against the ledger to bring it up
--   to date w.r.t. the tip of the ImmutableDB during initialisation. As
--   this process takes a while, we trace events to inform higher layers of
--   our progress.
data TraceReplayEvent blk

-- | There were no LedgerDB snapshots on disk, so we're replaying all
--   blocks starting from Genesis against the initial ledger.
ReplayFromGenesis :: ReplayGoal blk -> TraceReplayEvent blk
ReplayFromSnapshot :: DiskSnapshot -> RealPoint blk -> ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk
ReplayedBlock :: RealPoint blk -> [LedgerEvent blk] -> ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk

-- | Add the tip of the Immutable DB to the trace event
--   
--   Between the tip of the immutable DB and the point of the starting
--   block, the node could (if it so desired) easily compute a "percentage
--   complete".
decorateReplayTracerWithGoal :: Point blk -> Tracer m (TraceReplayEvent blk) -> Tracer m (ReplayGoal blk -> TraceReplayEvent blk)

-- | Add the block at which a replay started.
--   
--   This allows to compute a "percentage complete" when tracing the
--   events.
decorateReplayTracerWithStart :: Point blk -> Tracer m (ReplayGoal blk -> TraceReplayEvent blk) -> Tracer m (ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.Init.InitLog blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Init.InitLog blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Init.InitLog blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Init.ReplayStart blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Init.ReplayStart blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Init.ReplayGoal blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Init.ReplayGoal blk)
instance (Ouroboros.Network.Block.StandardHash blk, Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.LedgerDB.Init.TraceReplayEvent blk)
instance (Ouroboros.Network.Block.StandardHash blk, Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.LedgerDB.Init.TraceReplayEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.LedgerDB.Init.TraceReplayEvent blk)

module Ouroboros.Consensus.Storage.LedgerDB

-- | Internal newtype wrapper around a ledger state <tt>l</tt> so that we
--   can define a non-blanket <a>Anchorable</a> instance.
newtype Checkpoint l
Checkpoint :: l -> Checkpoint l
[unCheckpoint] :: Checkpoint l -> l

-- | Internal state of the ledger DB
--   
--   The ledger DB looks like
--   
--   <pre>
--   anchor |&gt; snapshots &lt;| current
--   </pre>
--   
--   where <tt>anchor</tt> records the oldest known snapshot and
--   <tt>current</tt> the most recent. The anchor is the oldest point we
--   can roll back to.
--   
--   We take a snapshot after each block is applied and keep in memory a
--   window of the last <tt>k</tt> snapshots. We have verified empirically
--   (#1936) that the overhead of keeping <tt>k</tt> snapshots in memory is
--   small, i.e., about 5% compared to keeping a snapshot every 100 blocks.
--   This is thanks to sharing between consecutive snapshots.
--   
--   As an example, suppose we have <tt>k = 6</tt>. The ledger DB grows as
--   illustrated below, where we indicate the anchor number of blocks, the
--   stored snapshots, and the current ledger.
--   
--   <pre>
--   anchor |&gt; #   [ snapshots ]                   &lt;| tip
--   ---------------------------------------------------------------------------
--   G      |&gt; (0) [ ]                             &lt;| G
--   G      |&gt; (1) [ L1]                           &lt;| L1
--   G      |&gt; (2) [ L1,  L2]                      &lt;| L2
--   G      |&gt; (3) [ L1,  L2,  L3]                 &lt;| L3
--   G      |&gt; (4) [ L1,  L2,  L3,  L4]            &lt;| L4
--   G      |&gt; (5) [ L1,  L2,  L3,  L4,  L5]       &lt;| L5
--   G      |&gt; (6) [ L1,  L2,  L3,  L4,  L5,  L6]  &lt;| L6
--   L1     |&gt; (6) [ L2,  L3,  L4,  L5,  L6,  L7]  &lt;| L7
--   L2     |&gt; (6) [ L3,  L4,  L5,  L6,  L7,  L8]  &lt;| L8
--   L3     |&gt; (6) [ L4,  L5,  L6,  L7,  L8,  L9]  &lt;| L9   (*)
--   L4     |&gt; (6) [ L5,  L6,  L7,  L8,  L9,  L10] &lt;| L10
--   L5     |&gt; (6) [*L6,  L7,  L8,  L9,  L10, L11] &lt;| L11
--   L6     |&gt; (6) [ L7,  L8,  L9,  L10, L11, L12] &lt;| L12
--   L7     |&gt; (6) [ L8,  L9,  L10, L12, L12, L13] &lt;| L13
--   L8     |&gt; (6) [ L9,  L10, L12, L12, L13, L14] &lt;| L14
--   </pre>
--   
--   The ledger DB must guarantee that at all times we are able to roll
--   back <tt>k</tt> blocks. For example, if we are on line (*), and roll
--   back 6 blocks, we get
--   
--   <pre>
--   L3 |&gt; []
--   </pre>
newtype LedgerDB l
LedgerDB :: AnchoredSeq (WithOrigin SlotNo) (Checkpoint l) (Checkpoint l) -> LedgerDB l

-- | Ledger states
[ledgerDbCheckpoints] :: LedgerDB l -> AnchoredSeq (WithOrigin SlotNo) (Checkpoint l) (Checkpoint l)
type LedgerDB' blk = LedgerDB (ExtLedgerState blk)
data LedgerDbCfg l
LedgerDbCfg :: !SecurityParam -> !LedgerCfg l -> LedgerDbCfg l
[ledgerDbCfgSecParam] :: LedgerDbCfg l -> !SecurityParam
[ledgerDbCfg] :: LedgerDbCfg l -> !LedgerCfg l
configLedgerDb :: ConsensusProtocol (BlockProtocol blk) => TopLevelConfig blk -> LedgerDbCfg (ExtLedgerState blk)

-- | Initialization log
--   
--   The initialization log records which snapshots from disk were
--   considered, in which order, and why some snapshots were rejected. It
--   is primarily useful for monitoring purposes.
data InitLog blk

-- | Defaulted to initialization from genesis
--   
--   NOTE: Unless the blockchain is near genesis, we should see this
--   <i>only</i> if data corrupted occurred.
InitFromGenesis :: InitLog blk

-- | Used a snapshot corresponding to the specified tip
InitFromSnapshot :: DiskSnapshot -> RealPoint blk -> InitLog blk

-- | Initialization skipped a snapshot
--   
--   We record the reason why it was skipped.
--   
--   NOTE: We should <i>only</i> see this if data corrupted occurred.
InitFailure :: DiskSnapshot -> SnapshotFailure blk -> InitLog blk -> InitLog blk

-- | Which point the replay started from
newtype ReplayStart blk
ReplayStart :: Point blk -> ReplayStart blk

-- | Initialize the ledger DB from the most recent snapshot on disk
--   
--   If no such snapshot can be found, use the genesis ledger DB. Returns
--   the initialized DB as well as the block reference corresponding to the
--   snapshot we found on disk (the latter primarily for testing/monitoring
--   purposes).
--   
--   We do <i>not</i> catch any exceptions thrown during streaming; should
--   any be thrown, it is the responsibility of the <tt>ChainDB</tt> to
--   catch these and trigger (further) validation. We only discard
--   snapshots if
--   
--   <ul>
--   <li>We cannot deserialise them, or</li>
--   <li>they are <i>ahead</i> of the chain</li>
--   </ul>
--   
--   It is possible that the Ledger DB will not be able to roll back
--   <tt>k</tt> blocks after initialization if the chain has been truncated
--   (data corruption).
--   
--   We do <i>not</i> attempt to use multiple ledger states from disk to
--   construct the ledger DB. Instead we load only a <i>single</i> ledger
--   state from disk, and <i>compute</i> all subsequent ones. This is
--   important, because the ledger states obtained in this way will
--   (hopefully) share much of their memory footprint with their
--   predecessors.
initLedgerDB :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasCallStack) => Tracer m (ReplayGoal blk -> TraceReplayEvent blk) -> Tracer m (TraceSnapshotEvent blk) -> SomeHasFS m -> (forall s. Decoder s (ExtLedgerState blk)) -> (forall s. Decoder s (HeaderHash blk)) -> LedgerDbCfg (ExtLedgerState blk) -> m (ExtLedgerState blk) -> StreamAPI m blk -> m (InitLog blk, LedgerDB' blk, Word64)

-- | Which point the replay is expected to end at
newtype ReplayGoal blk
ReplayGoal :: Point blk -> ReplayGoal blk

-- | Events traced while replaying blocks against the ledger to bring it up
--   to date w.r.t. the tip of the ImmutableDB during initialisation. As
--   this process takes a while, we trace events to inform higher layers of
--   our progress.
data TraceReplayEvent blk

-- | There were no LedgerDB snapshots on disk, so we're replaying all
--   blocks starting from Genesis against the initial ledger.
ReplayFromGenesis :: ReplayGoal blk -> TraceReplayEvent blk
ReplayFromSnapshot :: DiskSnapshot -> RealPoint blk -> ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk
ReplayedBlock :: RealPoint blk -> [LedgerEvent blk] -> ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk

-- | Add the tip of the Immutable DB to the trace event
--   
--   Between the tip of the immutable DB and the point of the starting
--   block, the node could (if it so desired) easily compute a "percentage
--   complete".
decorateReplayTracerWithGoal :: Point blk -> Tracer m (TraceReplayEvent blk) -> Tracer m (ReplayGoal blk -> TraceReplayEvent blk)

-- | Add the block at which a replay started.
--   
--   This allows to compute a "percentage complete" when tracing the
--   events.
decorateReplayTracerWithStart :: Point blk -> Tracer m (ReplayGoal blk -> TraceReplayEvent blk) -> Tracer m (ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk)

-- | Information about the state of the ledger at the anchor
ledgerDbAnchor :: LedgerDB l -> l

-- | The ledger state at the tip of the chain
ledgerDbCurrent :: GetTip l => LedgerDB l -> l

-- | Have we seen at least <tt>k</tt> blocks?
ledgerDbIsSaturated :: GetTip l => SecurityParam -> LedgerDB l -> Bool

-- | How many blocks can we currently roll back?
ledgerDbMaxRollback :: GetTip l => LedgerDB l -> Word64

-- | Get a past ledger state
--   
--   &lt;math&gt;
--   
--   When no ledger state (or anchor) has the given <a>Point</a>,
--   <a>Nothing</a> is returned.
ledgerDbPast :: (HasHeader blk, IsLedger l, HeaderHash l ~ HeaderHash blk) => Point blk -> LedgerDB l -> Maybe l

-- | All snapshots currently stored by the ledger DB (new to old)
--   
--   This also includes the snapshot at the anchor. For each snapshot we
--   also return the distance from the tip.
ledgerDbSnapshots :: LedgerDB l -> [(Word64, l)]

-- | Reference to the block at the tip of the chain
ledgerDbTip :: GetTip l => LedgerDB l -> Point l

-- | Ledger DB starting at the specified ledger state
ledgerDbWithAnchor :: GetTip l => l -> LedgerDB l

-- | Annotated ledger errors
data AnnLedgerError l blk
AnnLedgerError :: LedgerDB l -> RealPoint blk -> LedgerErr l -> AnnLedgerError l blk

-- | The ledger DB just <i>before</i> this block was applied
[annLedgerState] :: AnnLedgerError l blk -> LedgerDB l

-- | Reference to the block that had the error
[annLedgerErrRef] :: AnnLedgerError l blk -> RealPoint blk

-- | The ledger error itself
[annLedgerErr] :: AnnLedgerError l blk -> LedgerErr l
type AnnLedgerError' blk = AnnLedgerError (ExtLedgerState blk) blk

-- | <a>Ap</a> is used to pass information about blocks to ledger DB
--   updates
--   
--   The constructors serve two purposes:
--   
--   <ul>
--   <li>Specify the various parameters a. Are we passing the block by
--   value or by reference? b. Are we applying or reapplying the
--   block?</li>
--   <li>Compute the constraint <tt>c</tt> on the monad <tt>m</tt> in order
--   to run the query: a. If we are passing a block by reference, we must
--   be able to resolve it. b. If we are applying rather than reapplying,
--   we might have ledger errors.</li>
--   </ul>
data Ap m l blk c
[ReapplyVal] :: blk -> Ap m l blk ()
[ApplyVal] :: blk -> Ap m l blk (ThrowsLedgerError m l blk)
[ReapplyRef] :: RealPoint blk -> Ap m l blk (ResolvesBlocks m blk)
[ApplyRef] :: RealPoint blk -> Ap m l blk (ResolvesBlocks m blk, ThrowsLedgerError m l blk)

-- | <a>Weaken</a> increases the constraint on the monad <tt>m</tt>.
--   
--   This is primarily useful when combining multiple <a>Ap</a>s in a
--   single homogeneous structure.
[Weaken] :: (c' => c) => Ap m l blk c -> Ap m l blk c'

-- | Exceeded maximum rollback supported by the current ledger DB state
--   
--   Under normal circumstances this will not arise. It can really only
--   happen in the presence of data corruption (or when switching to a
--   shorter fork, but that is disallowed by all currently known Ouroboros
--   protocols).
--   
--   Records both the supported and the requested rollback.
data ExceededRollback
ExceededRollback :: Word64 -> Word64 -> ExceededRollback
[rollbackMaximum] :: ExceededRollback -> Word64
[rollbackRequested] :: ExceededRollback -> Word64
class Monad m => ThrowsLedgerError m l blk
throwLedgerError :: ThrowsLedgerError m l blk => LedgerDB l -> RealPoint blk -> LedgerErr l -> m a
defaultThrowLedgerErrors :: ExceptT (AnnLedgerError l blk) m a -> m (Either (AnnLedgerError l blk) a)

-- | Resolve a block
--   
--   Resolving a block reference to the actual block lives in <tt>m</tt>
--   because it might need to read the block from disk (and can therefore
--   not be done inside an STM transaction).
--   
--   NOTE: The ledger DB will only ask the <tt>ChainDB</tt> for blocks it
--   knows must exist. If the <tt>ChainDB</tt> is unable to fulfill the
--   request, data corruption must have happened and the <tt>ChainDB</tt>
--   should trigger validation mode.
type ResolveBlock m blk = RealPoint blk -> m blk

-- | Monads in which we can resolve blocks
--   
--   To guide type inference, we insist that we must be able to infer the
--   type of the block we are resolving from the type of the monad.
class Monad m => ResolvesBlocks m blk | m -> blk
doResolveBlock :: ResolvesBlocks m blk => ResolveBlock m blk
defaultResolveBlocks :: ResolveBlock m blk -> ReaderT (ResolveBlock m blk) m a -> m a
defaultResolveWithErrors :: ResolveBlock m blk -> ExceptT (AnnLedgerError l blk) (ReaderT (ResolveBlock m blk) m) a -> m (Either (AnnLedgerError l blk) a)

-- | Transform the underlying <a>AnchoredSeq</a> using the given functions.
ledgerDbBimap :: Anchorable (WithOrigin SlotNo) a b => (l -> a) -> (l -> b) -> LedgerDB l -> AnchoredSeq (WithOrigin SlotNo) a b

-- | Prune snapshots until at we have at most <tt>k</tt> snapshots in the
--   LedgerDB, excluding the snapshots stored at the anchor.
ledgerDbPrune :: GetTip l => SecurityParam -> LedgerDB l -> LedgerDB l
ledgerDbPush :: forall m c l blk. (ApplyBlock l blk, Monad m, c) => LedgerDbCfg l -> Ap m l blk c -> LedgerDB l -> m (LedgerDB l)

-- | Switch to a fork
ledgerDbSwitch :: (ApplyBlock l blk, Monad m, c) => LedgerDbCfg l -> Word64 -> (UpdateLedgerDbTraceEvent blk -> m ()) -> [Ap m l blk c] -> LedgerDB l -> m (Either ExceededRollback (LedgerDB l))
ledgerDbPush' :: ApplyBlock l blk => LedgerDbCfg l -> blk -> LedgerDB l -> LedgerDB l
ledgerDbPushMany' :: ApplyBlock l blk => LedgerDbCfg l -> [blk] -> LedgerDB l -> LedgerDB l
ledgerDbSwitch' :: forall l blk. ApplyBlock l blk => LedgerDbCfg l -> Word64 -> [blk] -> LedgerDB l -> Maybe (LedgerDB l)
newtype PushGoal blk
PushGoal :: RealPoint blk -> PushGoal blk
[unPushGoal] :: PushGoal blk -> RealPoint blk
newtype PushStart blk
PushStart :: RealPoint blk -> PushStart blk
[unPushStart] :: PushStart blk -> RealPoint blk
newtype Pushing blk
Pushing :: RealPoint blk -> Pushing blk
[unPushing] :: Pushing blk -> RealPoint blk
data UpdateLedgerDbTraceEvent blk

-- | Event fired when we are about to push a block to the LedgerDB
StartedPushingBlockToTheLedgerDb :: !PushStart blk -> PushGoal blk -> !Pushing blk -> UpdateLedgerDbTraceEvent blk

-- | Next block returned during streaming
data NextBlock blk
NoMoreBlocks :: NextBlock blk
NextBlock :: blk -> NextBlock blk

-- | Stream blocks from the immutable DB
--   
--   When we initialize the ledger DB, we try to find a snapshot close to
--   the tip of the immutable DB, and then stream blocks from the immutable
--   DB to its tip to bring the ledger up to date with the tip of the
--   immutable DB.
--   
--   In CPS form to enable the use of <tt>withXYZ</tt> style iterator init
--   functions.
data StreamAPI m blk
StreamAPI :: (forall a. HasCallStack => Point blk -> (Either (RealPoint blk) (m (NextBlock blk)) -> m a) -> m a) -> StreamAPI m blk

-- | Start streaming after the specified block
[streamAfter] :: StreamAPI m blk -> forall a. HasCallStack => Point blk -> (Either (RealPoint blk) (m (NextBlock blk)) -> m a) -> m a

-- | Stream all blocks
streamAll :: forall m blk e a. (Monad m, HasCallStack) => StreamAPI m blk -> Point blk -> (RealPoint blk -> e) -> a -> (blk -> a -> m a) -> ExceptT e m a
data DiskSnapshot
DiskSnapshot :: Word64 -> Maybe String -> DiskSnapshot

-- | Snapshots are numbered. We will try the snapshots with the highest
--   number first.
--   
--   When creating a snapshot, we use the slot number of the ledger state
--   it corresponds to as the snapshot number. This gives an indication of
--   how recent the snapshot is.
--   
--   Note that the snapshot names are only indicative, we don't rely on the
--   snapshot number matching the slot number of the corresponding ledger
--   state. We only use the snapshots numbers to determine the order in
--   which we try them.
[dsNumber] :: DiskSnapshot -> Word64

-- | Snapshots can optionally have a suffix, separated by the snapshot
--   number with an underscore, e.g., <tt>4492799_last_Byron</tt>. This
--   suffix acts as metadata for the operator of the node. Snapshots with a
--   suffix will <i>not be trimmed</i>.
[dsSuffix] :: DiskSnapshot -> Maybe String
data SnapshotFailure blk

-- | We failed to deserialise the snapshot
--   
--   This can happen due to data corruption in the ledger DB.
InitFailureRead :: ReadIncrementalErr -> SnapshotFailure blk

-- | This snapshot is too recent (ahead of the tip of the chain)
InitFailureTooRecent :: RealPoint blk -> SnapshotFailure blk

-- | This snapshot was of the ledger state at genesis, even though we never
--   take snapshots at genesis, so this is unexpected.
InitFailureGenesis :: SnapshotFailure blk

-- | The snapshots that are periodically created are temporary, they will
--   be deleted when trimming
diskSnapshotIsTemporary :: DiskSnapshot -> Bool

-- | List on-disk snapshots, highest number first.
listSnapshots :: Monad m => SomeHasFS m -> m [DiskSnapshot]

-- | Read snapshot from disk
readSnapshot :: forall m blk. IOLike m => SomeHasFS m -> (forall s. Decoder s (ExtLedgerState blk)) -> (forall s. Decoder s (HeaderHash blk)) -> DiskSnapshot -> ExceptT ReadIncrementalErr m (ExtLedgerState blk)

-- | Take a snapshot of the <i>oldest ledger state</i> in the ledger DB
--   
--   We write the <i>oldest</i> ledger state to disk because the intention
--   is to only write ledger states to disk that we know to be immutable.
--   Primarily for testing purposes, <a>takeSnapshot</a> returns the block
--   reference corresponding to the snapshot that we wrote.
--   
--   If a snapshot with the same number already exists on disk or if the
--   tip is at genesis, no snapshot is taken.
--   
--   Note that an EBB can have the same slot number and thus snapshot
--   number as the block after it. This doesn't matter. The one block
--   difference in the ledger state doesn't warrant an additional snapshot.
--   The number in the name of the snapshot is only indicative, we don't
--   rely on it being correct.
--   
--   NOTE: This is a lower-level API that takes a snapshot independent from
--   whether this snapshot corresponds to a state that is more than
--   <tt>k</tt> back.
--   
--   TODO: Should we delete the file if an error occurs during writing?
takeSnapshot :: forall m blk. (MonadThrow m, IsLedger (LedgerState blk)) => Tracer m (TraceSnapshotEvent blk) -> SomeHasFS m -> (ExtLedgerState blk -> Encoding) -> ExtLedgerState blk -> m (Maybe (DiskSnapshot, RealPoint blk))

-- | Trim the number of on disk snapshots so that at most
--   <a>onDiskNumSnapshots</a> snapshots are stored on disk. The oldest
--   snapshots are deleted.
--   
--   The deleted snapshots are returned.
trimSnapshots :: Monad m => Tracer m (TraceSnapshotEvent r) -> SomeHasFS m -> DiskPolicy -> m [DiskSnapshot]

-- | Write snapshot to disk
writeSnapshot :: forall m blk. MonadThrow m => SomeHasFS m -> (ExtLedgerState blk -> Encoding) -> DiskSnapshot -> ExtLedgerState blk -> m ()

-- | To remain backwards compatible with existing snapshots stored on disk,
--   we must accept the old format as well as the new format.
--   
--   The old format: * The tip: <tt>WithOrigin (RealPoint blk)</tt> * The
--   chain length: <tt>Word64</tt> * The ledger state: <tt>l</tt>
--   
--   The new format is described by <a>snapshotEncodingVersion1</a>.
--   
--   This decoder will accept and ignore them. The encoder
--   (<a>encodeSnapshot</a>) will no longer encode them.
decodeSnapshotBackwardsCompatible :: forall l blk. Proxy blk -> (forall s. Decoder s l) -> (forall s. Decoder s (HeaderHash blk)) -> forall s. Decoder s l

-- | Delete snapshot from disk
deleteSnapshot :: HasCallStack => SomeHasFS m -> DiskSnapshot -> m ()

-- | Encoder to be used in combination with
--   <a>decodeSnapshotBackwardsCompatible</a>.
encodeSnapshot :: (l -> Encoding) -> l -> Encoding
snapshotToFileName :: DiskSnapshot -> String
snapshotToPath :: DiskSnapshot -> FsPath
data TraceSnapshotEvent blk

-- | An on disk snapshot was skipped because it was invalid.
InvalidSnapshot :: DiskSnapshot -> SnapshotFailure blk -> TraceSnapshotEvent blk

-- | A snapshot was written to disk.
TookSnapshot :: DiskSnapshot -> RealPoint blk -> TraceSnapshotEvent blk

-- | An old or invalid on-disk snapshot was deleted
DeletedSnapshot :: DiskSnapshot -> TraceSnapshotEvent blk

-- | On-disk policy
--   
--   We only write ledger states that are older than <tt>k</tt> blocks to
--   disk (that is, snapshots that are guaranteed valid). The on-disk
--   policy determines how often we write to disk and how many checkpoints
--   we keep.
data DiskPolicy
DiskPolicy :: Word -> (TimeSinceLast DiffTime -> Word64 -> Bool) -> DiskPolicy

-- | How many snapshots do we want to keep on disk?
--   
--   A higher number of on-disk snapshots is primarily a safe-guard against
--   disk corruption: it trades disk space for reliability.
--   
--   Examples:
--   
--   <ul>
--   <li><tt>0</tt>: Delete the snapshot immediately after writing.
--   Probably not a useful value :-D</li>
--   <li><tt>1</tt>: Delete the previous snapshot immediately after writing
--   the next Dangerous policy: if for some reason the deletion happens
--   before the new snapshot is written entirely to disk (we don't
--   <tt>fsync</tt>), we have no choice but to start at the genesis
--   snapshot on the next startup.</li>
--   <li><tt>2</tt>: Always keep 2 snapshots around. This means that when
--   we write the next snapshot, we delete the oldest one, leaving the
--   middle one available in case of truncation of the write. This is
--   probably a sane value in most circumstances.</li>
--   </ul>
[onDiskNumSnapshots] :: DiskPolicy -> Word

-- | Should we write a snapshot of the ledger state to disk?
--   
--   This function is passed two bits of information:
--   
--   <ul>
--   <li>The time since the last snapshot, or <a>NoSnapshotTakenYet</a> if
--   none was taken yet. Note that <a>NoSnapshotTakenYet</a> merely means
--   no snapshot had been taking yet since the node was started; it does
--   not necessarily mean that none exist on disk.</li>
--   <li>The distance in terms of blocks applied to the <i>oldest</i>
--   ledger snapshot in memory. During normal operation, this is the number
--   of blocks written to the ImmutableDB since the last snapshot. On
--   startup, it is computed by counting how many immutable blocks we had
--   to reapply to get to the chain tip. This is useful, as it allows the
--   policy to decide to take a snapshot <i>on node startup</i> if a lot of
--   blocks had to be replayed.</li>
--   </ul>
--   
--   See also <a>defaultDiskPolicy</a>
[onDiskShouldTakeSnapshot] :: DiskPolicy -> TimeSinceLast DiffTime -> Word64 -> Bool

-- | Length of time, requested by the user, that has to pass after which a
--   snapshot is taken. It can be:
--   
--   <ol>
--   <li>either explicitly provided by user in seconds</li>
--   <li>or default value can be requested - the specific DiskPolicy
--   determines what that is exactly, see <a>defaultDiskPolicy</a> as an
--   example</li>
--   </ol>
data SnapshotInterval
DefaultSnapshotInterval :: SnapshotInterval
RequestedSnapshotInterval :: DiffTime -> SnapshotInterval
data TimeSinceLast time
NoSnapshotTakenYet :: TimeSinceLast time
TimeSinceLast :: time -> TimeSinceLast time

-- | Default on-disk policy suitable to use with cardano-node
defaultDiskPolicy :: SecurityParam -> SnapshotInterval -> DiskPolicy

module Ouroboros.Consensus.Util.TraceSize

-- | Generic helper to trace a value and its size
traceSize :: MonadIO m => Tracer m (a, Either CountFailure Word64) -> Tracer m a
data LedgerDbSize l
LedgerDbSize :: Point l -> Either CountFailure Word64 -> Either CountFailure Word64 -> LedgerDbSize l

-- | The tip of the ledger DB
[ledgerDbTip] :: LedgerDbSize l -> Point l

-- | Size of the ledger at the tip of the DB
[ledgerDbSizeTip] :: LedgerDbSize l -> Either CountFailure Word64

-- | Size of the entire (in-memory) ledger DB
[ledgerDbSizeTotal] :: LedgerDbSize l -> Either CountFailure Word64

-- | Trace the size of the ledger
--   
--   Only traces slots for which the predicate results true (genesis will
--   be considered to be slot 0).
traceLedgerDbSize :: forall m l. (MonadIO m, GetTip l) => (Word64 -> Bool) -> Tracer m (LedgerDbSize l) -> Tracer m (LedgerDB l)
instance Ouroboros.Network.Block.StandardHash l => GHC.Show.Show (Ouroboros.Consensus.Util.TraceSize.LedgerDbSize l)

module Ouroboros.Consensus.Storage.ChainDB.API

-- | The chain database
--   
--   The chain database provides a unified interface on top of:
--   
--   <ul>
--   <li>The ImmutableDB, storing the part of the chain that can't roll
--   back.</li>
--   <li>The VolatileDB, storing the blocks near the tip of the chain,
--   possibly in multiple competing forks.</li>
--   <li>The LedgerDB, storing snapshots of the ledger state for blocks in
--   the ImmutableDB (and in-memory snapshots for the rest).</li>
--   </ul>
--   
--   In addition to providing a unifying interface on top of these
--   disparate components, the main responsibilities that the ChainDB
--   itself has are:
--   
--   <ul>
--   <li>Chain selection (on initialization and whenever a block is
--   added)</li>
--   <li>Trigger full recovery whenever we detect disk failure in any
--   component</li>
--   <li>Provide iterators across fixed fragments of the current chain</li>
--   <li>Provide followers that track the status of the current chain</li>
--   </ul>
--   
--   The ChainDB instantiates all the various type parameters of these
--   databases to conform to the unified interface we provide here.
data ChainDB m blk
ChainDB :: (InvalidBlockPunishment m -> blk -> m (AddBlockPromise m blk)) -> STM m (AnchoredFragment (Header blk)) -> STM m (LedgerDB' blk) -> m (Maybe blk) -> m (Maybe (Header blk)) -> STM m (Point blk) -> (forall b. BlockComponent blk b -> RealPoint blk -> m (Maybe b)) -> STM m (Point blk -> Bool) -> STM m (RealPoint blk -> Maybe Bool) -> STM m MaxSlotNo -> (forall b. ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (UnknownRange blk) (Iterator m blk b))) -> (forall b. ResourceRegistry m -> ChainType -> BlockComponent blk b -> m (Follower m blk b)) -> STM m (WithFingerprint (HeaderHash blk -> Maybe (InvalidBlockReason blk))) -> m () -> STM m Bool -> ChainDB m blk

-- | Add a block to the heap of blocks
--   
--   We do <i>not</i> assume that the block is valid (under the legder
--   rules); it is the responsibility of the Chain DB itself to only select
--   chains that are valid.
--   
--   Conversely, the caller cannot assume that the new block will be added
--   to the current chain; even if the block is valid, it will not become
--   part of the chain if there are other chains available that are
--   preferred by the consensus algorithm (typically, longer chains).
--   
--   This function typically returns immediately, yielding a
--   <a>AddBlockPromise</a> which can be used to wait for the result. You
--   can use <a>addBlock</a> to add the block synchronously.
--   
--   NOTE: back pressure can be applied when overloaded.
[addBlockAsync] :: ChainDB m blk -> InvalidBlockPunishment m -> blk -> m (AddBlockPromise m blk)

-- | Get the current chain fragment
--   
--   Suppose the current chain is
--   
--   <pre>
--   a -&gt; b -&gt; c -&gt; d -&gt; e -&gt; f
--   </pre>
--   
--   and suppose <tt>k = 2</tt>; this means that the most distant fork we
--   can switch to is something like
--   
--   <pre>
--   a -&gt; b -&gt; c -&gt; d -&gt; e' -&gt; f'
--   </pre>
--   
--   The fragment we return will be <tt>[e, f]</tt>, anchored at
--   <tt>d</tt>. In other words, the length of the fragment will under
--   normal circumstances be exactly <tt>k</tt> blocks long. It may be
--   shorter if
--   
--   <ul>
--   <li>We are near genesis The anchor will be the genesis point (which
--   does not correspond to an actual block)</li>
--   <li>The volatile DB suffered some data loss Typically (but not
--   necessarily) the volatile DB will not be empty and the anchor will be
--   pointing to the tip of the immutable DB.</li>
--   </ul>
--   
--   POSTCONDITION: The Chain DB will be able to switch to any fork
--   starting from the anchor of the returned fragment or any subsequent
--   block (provided the new fork is at least of the same length as the
--   old).
--   
--   NOTE: A direct consequence of this guarantee is that the anchor of the
--   fragment will move as the chain grows.
[getCurrentChain] :: ChainDB m blk -> STM m (AnchoredFragment (Header blk))

-- | Return the LedgerDB containing the last <tt>k</tt> ledger states.
[getLedgerDB] :: ChainDB m blk -> STM m (LedgerDB' blk)

-- | Get block at the tip of the chain, if one exists
--   
--   Returns <a>Nothing</a> if the database is empty.
[getTipBlock] :: ChainDB m blk -> m (Maybe blk)

-- | Get header at the tip of the chain
--   
--   NOTE: Calling <a>getTipHeader</a> is cheaper than <a>getTipBlock</a>
--   and then extracting the header: most of the time the header at the tip
--   is actually in memory, whereas the block never is.
--   
--   Returns <a>Nothing</a> if the database is empty.
[getTipHeader] :: ChainDB m blk -> m (Maybe (Header blk))

-- | Get point of the tip of the chain
--   
--   Will return <tt>genesisPoint</tt> if the database is empty; if the
--   current chain fragment is empty due to data loss in the volatile DB,
--   <a>getTipPoint</a> will return the tip of the immutable DB.
[getTipPoint] :: ChainDB m blk -> STM m (Point blk)

-- | Get the given component(s) of the block at the specified point. If
--   there is no block at the given point, <a>Nothing</a> is returned.
[getBlockComponent] :: ChainDB m blk -> forall b. BlockComponent blk b -> RealPoint blk -> m (Maybe b)

-- | Return membership check function for recent blocks
--   
--   This check is only reliable for blocks up to <tt>k</tt> away from the
--   tip. For blocks older than that the results should be regarded as
--   non-deterministic.
[getIsFetched] :: ChainDB m blk -> STM m (Point blk -> Bool)

-- | Return a function that tells whether a block is known to be valid or
--   invalid.
--   
--   The function will return:
--   
--   <ul>
--   <li><tt>Just True</tt>: for blocks in the volatile DB that have been
--   validated and were found to be valid. All blocks in the current chain
--   fragment (i.e., <a>getCurrentChain</a>) are valid.</li>
--   <li><tt>Just False</tt>: for blocks in the volatile DB that have been
--   validated and were found to be invalid.</li>
--   <li><tt>Nothing</tt>: for blocks not or no longer in the volatile DB,
--   whether they are valid or not, including blocks in the immutable DB.
--   Also for blocks in the volatile DB that haven't been validated (yet),
--   e.g., because they are disconnected from the current chain or they are
--   part of a shorter fork.</li>
--   </ul>
[getIsValid] :: ChainDB m blk -> STM m (RealPoint blk -> Maybe Bool)

-- | Get the highest slot number stored in the ChainDB.
--   
--   Note that the corresponding block doesn't have to be part of the
--   current chain, it could be part of some fork, or even be a
--   disconnected block.
[getMaxSlotNo] :: ChainDB m blk -> STM m MaxSlotNo

-- | Stream blocks
--   
--   Streaming is not restricted to the current fork, but there must be an
--   unbroken path from the starting point to the end point /at the time of
--   initialization/ of the iterator. Once the iterator has been
--   initialized, it will not be affected by subsequent calls to
--   <a>addBlock</a>. To track the current chain, use a <a>Follower</a>
--   instead.
--   
--   Streaming blocks older than <tt>k</tt> is permitted, but only when
--   they are part of the current fork (at the time of initialization).
--   Streaming a fork that forks off more than <tt>k</tt> blocks in the
--   past is not permitted and an <a>UnknownRange</a> error will be
--   returned in that case.
--   
--   The iterator <i>does</i> have a limited lifetime, however. The chain
--   DB internally partitions the chain into an " immutable " part and a "
--   volatile " part, moving blocks from the volatile DB to the immutable
--   DB when they become more than <tt>k</tt> deep into the chain. When a
--   block with slot number <tt>n</tt> is added to the immutble DB, a time
--   delay <tt>t</tt> kicks in; after that time delay expires, all blocks
--   older than <tt>n</tt> may be removed from the volatile DB, /including
--   any blocks that happen to live on other forks/ (since those forks must
--   now, by definition, be too distant). This time delay <tt>t</tt> also
--   provides a worst-case bound for the lifetime of the iterator: if the
--   iterator traverses a chain that forks off from our current chain at
--   the tip of the immutable DB, then the first block on that fork will
--   become unavailable as soon as another block is pushed to the current
--   chain and the subsequent time delay expires.
--   
--   Note: although blocks are moved from the volatile DB to the immutable
--   DB after they have become <tt>k</tt> deep into the chain, due to data
--   corruption the suffix of the chain in the volatile DB might be shorter
--   than <tt>k</tt>. The immutable DB <i>always</i> determines the maximum
--   rollback, which may therefore be shorter than <tt>k</tt> under such
--   circumstances. In addition, streaming blocks which aren't on the
--   current fork is permitted, but the oldest volatile block must fit on
--   to the tip of the immutable DB.
--   
--   When the given bounds are nonsensical, an <a>InvalidIteratorRange</a>
--   is thrown.
--   
--   When the given bounds are not part of the chain DB, an
--   <a>UnknownRange</a> error is returned.
--   
--   To stream all blocks from the current chain, use <a>streamAll</a>, as
--   it correctly handles an empty ChainDB.
[stream] :: ChainDB m blk -> forall b. ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (UnknownRange blk) (Iterator m blk b))

-- | Chain follower
--   
--   A chain follower is an iterator that tracks the state of the
--   <i>current</i> chain: calling <tt>next</tt> on the iterator will
--   either give you the next block header, or (if we have switched to a
--   fork) the instruction to rollback.
--   
--   The tracking iterator starts at genesis (see also
--   <tt>trackForward</tt>).
--   
--   This is intended for use by chain consumers to <i>reliably</i> follow
--   a chain, desipite the chain being volatile.
--   
--   Examples of users: * The server side of the chain sync mini-protocol
--   for the node-to-node protocol using headers and the block size. * The
--   server side of the chain sync mini-protocol for the node-to-client
--   protocol using blocks.
[newFollower] :: ChainDB m blk -> forall b. ResourceRegistry m -> ChainType -> BlockComponent blk b -> m (Follower m blk b)

-- | Function to check whether a block is known to be invalid.
--   
--   Blocks unknown to the ChainDB will result in <a>Nothing</a>.
--   
--   If the hash corresponds to a block that is known to be invalid, but is
--   now older than <tt>k</tt>, this function may return <a>Nothing</a>.
--   
--   Whenever a new invalid block is added, the <tt>Fingerprint</tt> will
--   be changed. This is useful when "watching" this function in a
--   transaction.
--   
--   Note that when invalid blocks are garbage collected and thus no longer
--   detected by this function, the <tt>Fingerprint</tt> doesn't have to
--   change, since the function will not detect new invalid blocks.
--   
--   It might seem natural to have this function also return whether the
--   ChainDB knows that a block is valid, thereby subsuming the
--   <a>getIsValid</a> function and simplifying the API. However, this adds
--   the overhead of checking whether the block is valid for blocks that
--   are not known to be invalid that does not give useful information to
--   current clients (ChainSync), since they are only interested in whether
--   a block is known to be invalid. The extra information of whether a
--   block is valid is only used for testing.
--   
--   In particular, this affects the watcher in
--   <tt>bracketChainSyncClient</tt>, which rechecks the blocks in all
--   candidate chains whenever a new invalid block is detected. These
--   blocks are likely to be valid.
[getIsInvalidBlock] :: ChainDB m blk -> STM m (WithFingerprint (HeaderHash blk -> Maybe (InvalidBlockReason blk)))

-- | Close the ChainDB
--   
--   Idempotent.
--   
--   Should only be called on shutdown.
[closeDB] :: ChainDB m blk -> m ()

-- | Return <a>True</a> when the database is open.
--   
--   <a>False</a> when the database is closed.
[isOpen] :: ChainDB m blk -> STM m Bool

-- | Get current ledger
getCurrentLedger :: (Monad (STM m), IsLedger (LedgerState blk)) => ChainDB m blk -> STM m (ExtLedgerState blk)
getCurrentTip :: (Monad (STM m), HasHeader (Header blk)) => ChainDB m blk -> STM m (Tip blk)

-- | Get a <a>HeaderStateHistory</a> populated with the
--   <tt>HeaderState</tt>s of the last <tt>k</tt> blocks of the current
--   chain.
getHeaderStateHistory :: Monad (STM m) => ChainDB m blk -> STM m (HeaderStateHistory blk)

-- | Get the immutable ledger, i.e., typically <tt>k</tt> blocks back.
getImmutableLedger :: Monad (STM m) => ChainDB m blk -> STM m (ExtLedgerState blk)

-- | Get the ledger for the given point.
--   
--   When the given point is not among the last <tt>k</tt> blocks of the
--   current chain (i.e., older than <tt>k</tt> or not on the current
--   chain), <a>Nothing</a> is returned.
getPastLedger :: (Monad (STM m), LedgerSupportsProtocol blk) => ChainDB m blk -> Point blk -> STM m (Maybe (ExtLedgerState blk))
getTipBlockNo :: (Monad (STM m), HasHeader (Header blk)) => ChainDB m blk -> STM m (WithOrigin BlockNo)
data AddBlockPromise m blk
AddBlockPromise :: STM m Bool -> STM m (AddBlockResult blk) -> AddBlockPromise m blk

-- | Use this <a>STM</a> transaction to wait until the block has been
--   written to disk.
--   
--   Returns <a>True</a> when the block was written to disk or <a>False</a>
--   when it was ignored, e.g., because it was older than <tt>k</tt>.
--   
--   If the <a>STM</a> transaction has returned <a>True</a> then
--   <a>getIsFetched</a> will return <a>True</a> for the added block.
--   
--   NOTE: Even when the result is <a>False</a>, <a>getIsFetched</a> might
--   still return <a>True</a>, e.g., the block was older than <tt>k</tt>,
--   but it has been downloaded and stored on disk before.
[blockWrittenToDisk] :: AddBlockPromise m blk -> STM m Bool

-- | Use this <a>STM</a> transaction to wait until the block has been
--   processed: the block has been written to disk and chain selection has
--   been performed for the block, <i>unless</i> the block is from the
--   future.
--   
--   The ChainDB's tip after chain selection is returned. When this tip
--   doesn't match the added block, it doesn't necessarily mean the block
--   wasn't adopted. We might have adopted a longer chain of which the
--   added block is a part, but not the tip.
--   
--   It returns <a>FailedToAddBlock</a> if the thread adding the block
--   died.
--   
--   NOTE: When the block is from the future, chain selection for the block
--   won't be performed until the block is no longer in the future, which
--   might take some time. For that reason, this transaction will not wait
--   for chain selection of a block from the future. It will return the
--   current tip of the ChainDB after writing the block to disk.
[blockProcessed] :: AddBlockPromise m blk -> STM m (AddBlockResult blk)

-- | This is a wrapper type for <a>blockProcessed</a> function above.
--   
--   As it is mentioned the <tt>SuccessfullyAddedBlock</tt> constructor
--   will containt the ChainDB's tip after chain selection is returned.
--   
--   The <a>FailedToAddBlock</a> case will be returned if the thread adding
--   the block died.
data AddBlockResult blk
SuccesfullyAddedBlock :: Point blk -> AddBlockResult blk
FailedToAddBlock :: String -> AddBlockResult blk

-- | Add a block synchronously: wait until the block has been processed
--   (see <a>blockProcessed</a>). The new tip of the ChainDB is returned
--   unless the thread adding the block died, in that case
--   <a>FailedToAddBlock</a> will be returned.
--   
--   Note: this is a partial function, only to support tests.
addBlock :: IOLike m => ChainDB m blk -> InvalidBlockPunishment m -> blk -> m (AddBlockResult blk)

-- | Add a block synchronously: wait until the block has been written to
--   disk (see <a>blockWrittenToDisk</a>).
addBlockWaitWrittenToDisk :: IOLike m => ChainDB m blk -> InvalidBlockPunishment m -> blk -> m Bool

-- | Add a block synchronously. Variant of <a>addBlock</a> that doesn't
--   return the new tip of the ChainDB.
--   
--   Note: this is a partial function, only to support tests.
addBlock_ :: IOLike m => ChainDB m blk -> InvalidBlockPunishment m -> blk -> m ()

-- | A <tt>b</tt> together with its <a>Point</a>.
--   
--   The <a>Point</a> is needed because we often need to know the hash,
--   slot, or point itself of the block or header in question, and we don't
--   want to deserialise the block to obtain it.
data WithPoint blk b
WithPoint :: !b -> !Point blk -> WithPoint blk b
[withoutPoint] :: WithPoint blk b -> !b
[point] :: WithPoint blk b -> !Point blk
getPoint :: BlockComponent blk (Point blk)
getSerialisedBlockWithPoint :: BlockComponent blk (WithPoint blk (Serialised blk))
getSerialisedHeaderWithPoint :: BlockComponent blk (WithPoint blk (SerialisedHeader blk))

-- | Which component of the block to read from a database: the whole block,
--   its header, its hash, the block size, ..., or combinations thereof.
--   
--   NOTE: when requesting multiple components, we will not optimise/cache
--   them.
data BlockComponent blk a

-- | Verify the integrity of the block by checking its signature and/or
--   hashes. The interpreter should throw an exception when the block does
--   not pass the check.
[GetVerifiedBlock] :: BlockComponent blk blk
[GetBlock] :: BlockComponent blk blk
[GetRawBlock] :: BlockComponent blk ByteString
[GetHeader] :: BlockComponent blk (Header blk)
[GetRawHeader] :: BlockComponent blk ByteString
[GetHash] :: BlockComponent blk (HeaderHash blk)
[GetSlot] :: BlockComponent blk SlotNo
[GetIsEBB] :: BlockComponent blk IsEBB
[GetBlockSize] :: BlockComponent blk SizeInBytes
[GetHeaderSize] :: BlockComponent blk Word16
[GetNestedCtxt] :: BlockComponent blk (SomeSecond (NestedCtxt Header) blk)
[GetPure] :: a -> BlockComponent blk a
[GetApply] :: BlockComponent blk (a -> b) -> BlockComponent blk a -> BlockComponent blk b
fromChain :: forall m blk. IOLike m => m (ChainDB m blk) -> Chain blk -> m (ChainDB m blk)
toChain :: forall m blk. (HasCallStack, IOLike m, HasHeader blk) => ChainDB m blk -> m (Chain blk)
data Iterator m blk b
Iterator :: m (IteratorResult blk b) -> m () -> Iterator m blk b
[iteratorNext] :: Iterator m blk b -> m (IteratorResult blk b)

-- | When <a>fmap</a>-ing or <a>traverse</a>-ing (or using
--   <a>traverseIterator</a>) an <a>Iterator</a>, the resulting iterator
--   will still refer to and use the original one. This means that when
--   either of them is closed, both will be closed in practice.
[iteratorClose] :: Iterator m blk b -> m ()
data IteratorResult blk b
IteratorExhausted :: IteratorResult blk b
IteratorResult :: b -> IteratorResult blk b

-- | The block that was supposed to be streamed was garbage-collected from
--   the VolatileDB, but not added to the ImmutableDB.
--   
--   This will only happen when streaming very old forks very slowly.
--   
--   The following example illustrates a situation in which an iterator
--   result could be a <a>IteratorBlockGCed</a> value. Suppose we start
--   with an iterator positioned at block <tt>c</tt>, where <tt>[[ x
--   ]]</tt> denotes a block in the immutable DB:
--   
--   <pre>
--                             iterator i
--                               ↓
--   ... ⟶ [[ a ]] → [[ b ]] → [ c ] -&gt; [ d ]
--   ──────────────────────╯   ╰────────────╯
--     Immutable DB             Current chain
--   </pre>
--   
--   Suppose we switch to a longer fork that branches off from the
--   immutable tip ('[[b]]').
--   
--   <pre>
--                              iterator i
--                               ↓
--   ... ⟶ [[ a ]] → [[ b ]] → [ c ] -&gt; [ d ]
--   ──────────────────────╯│
--      Immutable DB        │
--                          ╰-→ [ e ] -&gt; [ f ] -&gt; [ g ]
--                              ╰─────────────────────╯
--                                    Current chain
--   </pre>
--   
--   Assume <tt>k=2</tt>. This means that block <tt>e</tt> is the new
--   immutable tip. If we would call <a>iteratorNext</a> on <tt>i</tt>
--   <b>after</b> block <tt>e</tt> is copied to the immutable DB and
--   <tt>c</tt> and <tt>d</tt> are garbage collected, then we will get
--   <a>IteratorBlockGCed</a>.
IteratorBlockGCed :: RealPoint blk -> IteratorResult blk b

-- | The lower bound for an iterator
--   
--   Hint: use <tt><a>StreamFromExclusive</a> <tt>genesisPoint</tt></tt> to
--   start streaming from Genesis.
data StreamFrom blk
StreamFromInclusive :: !RealPoint blk -> StreamFrom blk
StreamFromExclusive :: !Point blk -> StreamFrom blk
newtype StreamTo blk
StreamToInclusive :: RealPoint blk -> StreamTo blk
data UnknownRange blk

-- | The block at the given point was not found in the ChainDB.
MissingBlock :: RealPoint blk -> UnknownRange blk

-- | The requested range forks off too far in the past, i.e. it doesn't fit
--   on the tip of the ImmutableDB.
ForkTooOld :: StreamFrom blk -> UnknownRange blk

-- | An iterator that is immediately exhausted.
emptyIterator :: Monad m => Iterator m blk b

-- | Stream all blocks from the current chain.
streamAll :: (MonadSTM m, HasHeader blk, HasCallStack) => ChainDB m blk -> ResourceRegistry m -> BlockComponent blk b -> m (Iterator m blk b)

-- | Stream blocks from the given point up to the tip from the current
--   chain.
--   
--   To stream all blocks from the current chain from the ChainDB, one
--   would use <tt><a>StreamFromExclusive</a> <tt>genesisPoint</tt></tt> as
--   the lower bound and <tt><a>StreamToInclusive</a> tip</tt> as the upper
--   bound where <tt>tip</tt> is retrieved with <a>getTipPoint</a>.
--   
--   However, when the ChainDB is empty, <tt>tip</tt> will be
--   <tt>genesisPoint</tt> too, in which case the bounds don't make sense.
--   This function correctly handles this case.
--   
--   Note that this is not a <a>Follower</a>, so the stream will not
--   include blocks that are added to the current chain after starting the
--   stream.
streamFrom :: (MonadSTM m, HasHeader blk, HasCallStack) => StreamFrom blk -> ChainDB m blk -> ResourceRegistry m -> BlockComponent blk b -> m (Iterator m blk b)

-- | Variant of <a>traverse</a> instantiated to <tt><a>Iterator</a> m
--   blk</tt> that executes the monadic function when calling
--   <a>iteratorNext</a>.
traverseIterator :: Monad m => (b -> m b') -> Iterator m blk b -> Iterator m blk b'

-- | Check whether the bounds make sense
--   
--   An example of bounds that don't make sense:
--   
--   <pre>
--   StreamFromExclusive (BlockPoint 3 ..)
--   StreamToInclusive   (RealPoint  3 ..)
--   </pre>
--   
--   This function does not check whether the bounds correspond to existing
--   blocks.
validBounds :: StandardHash blk => StreamFrom blk -> StreamTo blk -> Bool

-- | The reason why a block is invalid.
data InvalidBlockReason blk

-- | The ledger found the block to be invalid.
ValidationError :: !ExtValidationError blk -> InvalidBlockReason blk

-- | The block's slot is in the future, exceeding the allowed clock skew.
--   
--   Possible causes, order by decreasing likelihood:
--   
--   <ol>
--   <li>Our clock is behind (significantly more likely than the
--   others)</li>
--   <li>Their clock is ahead</li>
--   <li>It's intentional, i.e., an attack</li>
--   </ol>
InFutureExceedsClockSkew :: !RealPoint blk -> InvalidBlockReason blk

-- | Chain type
--   
--   <a>Follower</a>s can choose to track changes to the "normal"
--   <a>SelectedChain</a>, or track the <a>TentativeChain</a>, which might
--   contain a pipelineable header at the tip.
data ChainType
SelectedChain :: ChainType
TentativeChain :: ChainType

-- | Follower
--   
--   Unlike an <a>Iterator</a>, which is used to request a static segment
--   of the current chain or a recent fork, a follower is used to
--   <b>follow</b> the <b>current chain</b> either from the start or from a
--   given point.
--   
--   Unlike an <a>Iterator</a>, a <a>Follower</a> is <b>dynamic</b>, that
--   is, it will follow the chain when it grows or forks.
--   
--   A follower is <b>pull-based</b>, which avoids the neeed to have a
--   growing queue of changes to the chain on the server side in case the
--   client is slower.
--   
--   A follower always has an <b>implicit position</b> associated with it.
--   The <a>followerInstruction</a> and <a>followerInstructionBlocking</a>
--   operations request the next <a>ChainUpdate</a> wrt the follower's
--   implicit position.
--   
--   The type parameter <tt>a</tt> will be instantiated with <tt>blk</tt>
--   or <tt><a>Header</a> blk</tt>.
data Follower m blk a
Follower :: m (Maybe (ChainUpdate blk a)) -> m (ChainUpdate blk a) -> ([Point blk] -> m (Maybe (Point blk))) -> m () -> Follower m blk a

-- | The next chain update (if one exists)
--   
--   The <tt>AddBlock</tt> instruction (see <a>ChainUpdate</a>) indicates
--   that, to follow the current chain, the follower should extend its
--   chain with the given block component (which will be a value of type
--   <tt>a</tt>).
--   
--   The <tt>RollBack</tt> instruction indicates that the follower should
--   perform a rollback by first backtracking to a certain point.
--   
--   If a follower should switch to a fork, then it will first receive a
--   <tt>RollBack</tt> instruction followed by as many <tt>AddBlock</tt> as
--   necessary to reach the tip of the new chain.
--   
--   When the follower's (implicit) position is in the immutable part of
--   the chain, no rollback instructions will be encountered.
--   
--   Not in <tt>STM</tt> because might have to read the blocks or headers
--   from disk.
--   
--   We may roll back more than <tt>k</tt>, but only in case of data loss.
[followerInstruction] :: Follower m blk a -> m (Maybe (ChainUpdate blk a))

-- | Blocking version of <a>followerInstruction</a>
[followerInstructionBlocking] :: Follower m blk a -> m (ChainUpdate blk a)

-- | Move the follower forward
--   
--   Must be given a list of points in order of preference; the iterator
--   will move forward to the first point on the list that is on the
--   current chain. Returns <a>Nothing</a> if the iterator did not move, or
--   the new point otherwise.
--   
--   When successful, the first call to <a>followerInstruction</a> after
--   <a>followerForward</a> will be a <tt>RollBack</tt> to the point
--   returned by <a>followerForward</a>.
--   
--   Cannot live in <tt>STM</tt> because the points specified might live in
--   the immutable DB.
[followerForward] :: Follower m blk a -> [Point blk] -> m (Maybe (Point blk))

-- | Close the follower.
--   
--   Idempotent.
--   
--   After closing, all other operations on the follower will throw
--   <a>ClosedFollowerError</a>.
[followerClose] :: Follower m blk a -> m ()

-- | Variant of <a>traverse</a> instantiated to <tt><a>Follower</a> m
--   blk</tt> that executes the monadic function when calling
--   <a>followerInstruction</a> and <a>followerInstructionBlocking</a>.
traverseFollower :: Monad m => (b -> m b') -> Follower m blk b -> Follower m blk b'

-- | Database failure
--   
--   This exception wraps any kind of unexpected problem with the on-disk
--   storage of the chain.
--   
--   The various constructors only serve to give more detailed information
--   about what went wrong, in case sysadmins want to investigate the disk
--   failure. The Chain DB itself does not differentiate; all disk failures
--   are treated equal and all trigger the same recovery procedure.
data ChainDbFailure blk

-- | The ledger DB threw a file-system error
LgrDbFailure :: FsError -> ChainDbFailure blk

-- | Block missing from the chain DB
--   
--   Thrown when we are not sure in which DB the block <i>should</i> have
--   been.
ChainDbMissingBlock :: RealPoint blk -> ChainDbFailure blk

-- | Whether a block is an Epoch Boundary Block (EBB)
--   
--   See <a>Ouroboros.Storage.ImmutableDB.API</a> for a discussion of EBBs.
--   Key idiosyncracies:
--   
--   <ul>
--   <li>An EBB carries no unique information.</li>
--   <li>An EBB has the same <tt>BlockNo</tt> as its predecessor.</li>
--   <li>EBBs are vestigial. As of Shelley, nodes no longer forge EBBs:
--   they are only a legacy/backwards-compatibility concern.</li>
--   </ul>
data IsEBB
IsEBB :: IsEBB
IsNotEBB :: IsEBB

-- | Database error
--   
--   Thrown upon incorrect use: invalid input.
data ChainDbError blk

-- | The ChainDB is closed.
--   
--   This will be thrown when performing any operation on the ChainDB
--   except for <a>isOpen</a> and <a>closeDB</a>. The <tt>CallStack</tt> of
--   the operation on the ChainDB is included in the error.
ClosedDBError :: PrettyCallStack -> ChainDbError blk

-- | The follower is closed.
--   
--   This will be thrown when performing any operation on a closed
--   followers, except for <a>followerClose</a>.
ClosedFollowerError :: ChainDbError blk

-- | When there is no chain/fork that satisfies the bounds passed to
--   <tt>streamBlocks</tt>.
--   
--   <ul>
--   <li>The lower and upper bound are not on the same chain.</li>
--   <li>The bounds don't make sense, e.g., the lower bound starts after
--   the upper bound, or the lower bound starts from genesis,
--   <i>inclusive</i>.</li>
--   </ul>
InvalidIteratorRange :: StreamFrom blk -> StreamTo blk -> ChainDbError blk
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.API.AddBlockResult blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.API.AddBlockResult blk)
instance Data.Traversable.Traversable (Ouroboros.Consensus.Storage.ChainDB.API.IteratorResult blk)
instance Data.Foldable.Foldable (Ouroboros.Consensus.Storage.ChainDB.API.IteratorResult blk)
instance GHC.Base.Functor (Ouroboros.Consensus.Storage.ChainDB.API.IteratorResult blk)
instance Data.Traversable.Traversable m => Data.Traversable.Traversable (Ouroboros.Consensus.Storage.ChainDB.API.Iterator m blk)
instance Data.Foldable.Foldable m => Data.Foldable.Foldable (Ouroboros.Consensus.Storage.ChainDB.API.Iterator m blk)
instance GHC.Base.Functor m => GHC.Base.Functor (Ouroboros.Consensus.Storage.ChainDB.API.Iterator m blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.API.UnknownRange blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.API.UnknownRange blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.API.InvalidBlockReason blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.API.InvalidBlockReason blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.API.InvalidBlockReason blk)
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ChainDB.API.ChainType
instance GHC.Show.Show Ouroboros.Consensus.Storage.ChainDB.API.ChainType
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ChainDB.API.ChainType
instance GHC.Base.Functor m => GHC.Base.Functor (Ouroboros.Consensus.Storage.ChainDB.API.Follower m blk)
instance (GHC.Classes.Eq blk, GHC.Classes.Eq b, Ouroboros.Network.Block.StandardHash blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.API.IteratorResult blk b)
instance (GHC.Show.Show blk, GHC.Show.Show b, Ouroboros.Network.Block.StandardHash blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.API.IteratorResult blk b)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.API.ChainDbFailure blk)
instance (Data.Typeable.Internal.Typeable blk, Ouroboros.Network.Block.StandardHash blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.API.ChainDbError blk)
instance (Data.Typeable.Internal.Typeable blk, Ouroboros.Network.Block.StandardHash blk) => GHC.Exception.Type.Exception (Ouroboros.Consensus.Storage.ChainDB.API.ChainDbError blk)
instance (Data.Typeable.Internal.Typeable blk, Ouroboros.Network.Block.StandardHash blk) => GHC.Exception.Type.Exception (Ouroboros.Consensus.Storage.ChainDB.API.ChainDbFailure blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.API.InvalidBlockReason blk)
instance Ouroboros.Network.Block.StandardHash blk => Ouroboros.Network.Block.StandardHash (Ouroboros.Consensus.Storage.ChainDB.API.WithPoint blk b)


-- | Intended for qualified import
--   
--   <pre>
--   import Ouroboros.Consensus.Storage.ChainDB.Init (InitChainDB)
--   import qualified Ouroboros.Consensus.Storage.ChainDB.Init as InitChainDB
--   </pre>
module Ouroboros.Consensus.Storage.ChainDB.Init

-- | Restricted interface to the <a>ChainDB</a> used on node initialization
data InitChainDB m blk
InitChainDB :: (blk -> m ()) -> m (LedgerState blk) -> InitChainDB m blk

-- | Add a block to the DB
[addBlock] :: InitChainDB m blk -> blk -> m ()

-- | Return the current ledger state
[getCurrentLedger] :: InitChainDB m blk -> m (LedgerState blk)
fromFull :: (IsLedger (LedgerState blk), IOLike m) => ChainDB m blk -> InitChainDB m blk
map :: Functor m => (blk' -> blk) -> (LedgerState blk -> LedgerState blk') -> InitChainDB m blk -> InitChainDB m blk'

module Ouroboros.Consensus.Node.InitStorage

-- | Functionality needed to initialise the storage layer of the node.
class NodeInitStorage blk

-- | The <a>ChunkInfo</a> to use for the ImmutableDB, i.e., how many slots
--   to put together in a single chunk file.
--   
--   For example, for Byron, one would use the epoch size.
nodeImmutableDbChunkInfo :: NodeInitStorage blk => StorageConfig blk -> ChunkInfo

-- | Check the integrity of a block, i.e., that it has not been corrupted
--   by a bitflip.
--   
--   Check this by, e.g., verifying whether the block has a valid signature
--   and that the hash of the body matches the body hash stores in the
--   header.
--   
--   This does not check the validity of the contents of the block, e.g.,
--   whether the transactions are valid w.r.t. the ledger, or whether it's
--   sent by a malicious node.
nodeCheckIntegrity :: NodeInitStorage blk => StorageConfig blk -> blk -> Bool

-- | This function is called when starting up the node, right after the
--   ChainDB was opened, and before we connect to other nodes and start
--   block production.
--   
--   This function can be used to, for example, create the genesis EBB in
--   case the chain(DB) is empty.
--   
--   We only provide a limited interface to the chain DB. This is primarily
--   useful for the definition of combinators (which may need to turn a
--   <a>InitChainDB</a> for one type of block into an <a>InitChainDB</a>
--   for a closely related type of block).
nodeInitChainDB :: (NodeInitStorage blk, IOLike m) => StorageConfig blk -> InitChainDB m blk -> m ()

module Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock

-- | Blocks from which we can assemble a hard fork
class (LedgerSupportsProtocol blk, InspectLedger blk, LedgerSupportsMempool blk, HasTxId (GenTx blk), QueryLedger blk, HasPartialConsensusConfig (BlockProtocol blk), HasPartialLedgerConfig blk, ConvertRawHash blk, ReconstructNestedCtxt Header blk, CommonProtocolParams blk, LedgerSupportsPeerSelection blk, ConfigSupportsNode blk, NodeInitStorage blk, BlockSupportsMetrics blk, Eq (GenTx blk), Eq (Validated (GenTx blk)), Eq (ApplyTxErr blk), Show blk, Show (Header blk), Show (CannotForge blk), Show (ForgeStateInfo blk), Show (ForgeStateUpdateError blk), Show (LedgerState blk), Eq (LedgerState blk), NoThunks (LedgerState blk)) => SingleEraBlock blk

-- | Era transition
--   
--   This should only report the transition point once it is stable
--   (rollback cannot affect it anymore).
--   
--   Since we need this to construct the <tt>HardForkSummary</tt> (and
--   hence the <a>EpochInfo</a>), this takes the <i>partial</i> config, not
--   the full config (or we'd end up with a catch-22).
singleEraTransition :: SingleEraBlock blk => PartialLedgerConfig blk -> EraParams -> Bound -> LedgerState blk -> Maybe EpochNo

-- | Era information (for use in error messages)
singleEraInfo :: SingleEraBlock blk => proxy blk -> SingleEraInfo blk
proxySingle :: Proxy SingleEraBlock
singleEraTransition' :: SingleEraBlock blk => WrapPartialLedgerConfig blk -> EraParams -> Bound -> LedgerState blk -> Maybe EpochNo
newtype EraIndex xs
EraIndex :: NS (K ()) xs -> EraIndex xs
[getEraIndex] :: EraIndex xs -> NS (K ()) xs
eraIndexEmpty :: EraIndex '[] -> Void
eraIndexFromIndex :: Index xs blk -> EraIndex xs
eraIndexFromNS :: SListI xs => NS f xs -> EraIndex xs
eraIndexSucc :: EraIndex xs -> EraIndex (x ': xs)
eraIndexToInt :: EraIndex xs -> Int
eraIndexZero :: EraIndex (x ': xs)
instance GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.EraIndex xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.EraIndex xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.EraIndex xs)
instance Data.SOP.Constraint.SListI xs => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.EraIndex xs)

module Ouroboros.Consensus.HardFork.Combinator.State.Infra
initHardForkState :: f x -> HardForkState f (x ': xs)
fromTZ :: HardForkState f '[blk] -> f blk
match :: SListI xs => NS h xs -> HardForkState f xs -> Either (Mismatch h (Current f) xs) (HardForkState (Product h f) xs)
sequence :: forall f m xs. (SListI xs, Functor m) => HardForkState (m :.: f) xs -> m (HardForkState f xs)
tip :: SListI xs => HardForkState f xs -> NS f xs

-- | A <tt>h</tt> situated in time
data Situated h f xs
[SituatedCurrent] :: Current f x -> h x -> Situated h f (x ': xs)
[SituatedNext] :: Current f x -> h y -> Situated h f (x ': (y ': xs))
[SituatedFuture] :: Current f x -> NS h xs -> Situated h f (x ': (y ': xs))
[SituatedPast] :: K Past x -> h x -> Situated h f (x ': xs)
[SituatedShift] :: Situated h f xs -> Situated h f (x ': xs)
situate :: NS h xs -> HardForkState f xs -> Situated h f xs
align :: forall xs f f' f''. All SingleEraBlock xs => InPairs (Translate f) xs -> NP (f' -.-> (f -.-> f'')) xs -> HardForkState f' xs -> HardForkState f xs -> HardForkState f'' xs
reconstructSummary :: Shape xs -> TransitionInfo -> HardForkState f xs -> Summary xs


-- | Infrastructure for doing chain selection across eras
module Ouroboros.Consensus.HardFork.Combinator.Protocol.ChainSel
data AcrossEraSelection :: Type -> Type -> Type

-- | Just compare block numbers
--   
--   This is a useful default when two eras run totally different consensus
--   protocols, and we just want to choose the longer chain.
[CompareBlockNo] :: AcrossEraSelection x y

-- | Two eras using the same <a>SelectView</a>. In this case, we can just
--   compare chains even across eras, as the chain ordering is fully
--   captured by <a>SelectView</a> and its <a>Ord</a> instance.
[CompareSameSelectView] :: SelectView (BlockProtocol x) ~ SelectView (BlockProtocol y) => AcrossEraSelection x y
data WithBlockNo (f :: k -> Type) (a :: k)
WithBlockNo :: BlockNo -> f a -> WithBlockNo (f :: k -> Type) (a :: k)
[getBlockNo] :: WithBlockNo (f :: k -> Type) (a :: k) -> BlockNo
[dropBlockNo] :: WithBlockNo (f :: k -> Type) (a :: k) -> f a
acrossEraSelection :: All SingleEraBlock xs => Tails AcrossEraSelection xs -> WithBlockNo (NS WrapSelectView) xs -> WithBlockNo (NS WrapSelectView) xs -> Ordering
mapWithBlockNo :: (f x -> g y) -> WithBlockNo f x -> WithBlockNo g y
instance forall k (f :: k -> *) (a :: k). NoThunks.Class.NoThunks (f a) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Protocol.ChainSel.WithBlockNo f a)
instance forall k (f :: k -> *) (a :: k). GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.Protocol.ChainSel.WithBlockNo f a)
instance forall k (f :: k -> *) (a :: k). GHC.Classes.Eq (f a) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Protocol.ChainSel.WithBlockNo f a)
instance forall k (f :: k -> *) (a :: k). GHC.Show.Show (f a) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Protocol.ChainSel.WithBlockNo f a)

module Ouroboros.Consensus.HardFork.Combinator.Abstract.NoHardForks
class SingleEraBlock blk => NoHardForks blk

-- | Extract <a>EraParams</a> from the top-level config
--   
--   The HFC itself does not care about this, as it must be given the full
--   shape across <i>all</i> eras.
getEraParams :: NoHardForks blk => TopLevelConfig blk -> EraParams

-- | Construct partial ledger config from full ledger config
--   
--   See also <a>toPartialConsensusConfig</a>
toPartialLedgerConfig :: NoHardForks blk => proxy blk -> LedgerConfig blk -> PartialLedgerConfig blk
noHardForksEpochInfo :: (Monad m, NoHardForks blk) => TopLevelConfig blk -> EpochInfo m

module Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork
class (All SingleEraBlock xs, Typeable xs, IsNonEmpty xs) => CanHardFork xs
hardForkEraTranslation :: CanHardFork xs => EraTranslation xs
hardForkChainSel :: CanHardFork xs => Tails AcrossEraSelection xs
hardForkInjectTxs :: CanHardFork xs => InPairs (RequiringBoth WrapLedgerConfig (Product2 InjectTx InjectValidatedTx)) xs
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock blk => Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork '[blk]

module Ouroboros.Consensus.HardFork.Combinator.Abstract
class () => IsNonEmpty (xs :: [a])
isNonEmpty :: IsNonEmpty xs => proxy xs -> ProofNonEmpty xs
data () => ProofNonEmpty (xs :: [a])
[ProofNonEmpty] :: forall {a} (x :: a) (xs1 :: [a]). Proxy x -> Proxy xs1 -> ProofNonEmpty (x ': xs1)

module Ouroboros.Consensus.HardFork.Combinator.Lifting
newtype LiftMismatch f g xs
LiftMismatch :: Mismatch f g xs -> LiftMismatch f g xs
newtype LiftNP f xs
LiftNP :: NP f xs -> LiftNP f xs
newtype LiftNS f xs
LiftNS :: NS f xs -> LiftNS f xs
newtype LiftNamedMismatch (name :: Symbol) f g xs
LiftNamedMismatch :: Mismatch f g xs -> LiftNamedMismatch (name :: Symbol) f g xs
newtype LiftNamedNP (name :: Symbol) f xs
LiftNamedNP :: NP f xs -> LiftNamedNP (name :: Symbol) f xs
newtype LiftNamedNS (name :: Symbol) f xs
LiftNamedNS :: NS f xs -> LiftNamedNS (name :: Symbol) f xs
newtype LiftNamedTelescope (name :: Symbol) f g xs
LiftNamedTelescope :: Telescope f g xs -> LiftNamedTelescope (name :: Symbol) f g xs
newtype LiftOptNP empty f xs
LiftOptNP :: OptNP empty f xs -> LiftOptNP empty f xs
newtype LiftTelescope g f xs
LiftTelescope :: Telescope g f xs -> LiftTelescope g f xs
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => NoThunks.Class.NoThunks (f x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => NoThunks.Class.NoThunks (g x), GHC.TypeLits.KnownSymbol name) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNamedMismatch name f g xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => NoThunks.Class.NoThunks (f x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => NoThunks.Class.NoThunks (g x), GHC.TypeLits.KnownSymbol name) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNamedTelescope name f g xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => NoThunks.Class.NoThunks (f x), GHC.TypeLits.KnownSymbol name) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNamedNP name f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => NoThunks.Class.NoThunks (f x), GHC.TypeLits.KnownSymbol name) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNamedNS name f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Eq (f x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Eq (g x)) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftMismatch f g xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Ord (f x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Ord (g x)) => GHC.Classes.Ord (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftMismatch f g xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Show.Show (f x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Show.Show (g x)) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftMismatch f g xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Eq (g x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Eq (f x)) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftTelescope g f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Ord (f x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Ord (g x)) => GHC.Classes.Ord (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftTelescope g f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Show.Show (g x), forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Show.Show (f x)) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftTelescope g f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Eq (f x)) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftOptNP empty f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Show.Show (f x)) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftOptNP empty f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Eq (f x)) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNP f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Ord (f x)) => GHC.Classes.Ord (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNP f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Show.Show (f x)) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNP f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Eq (f x)) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNS f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Classes.Ord (f x)) => GHC.Classes.Ord (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNS f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall x. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock x => GHC.Show.Show (f x)) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Lifting.LiftNS f xs)

module Ouroboros.Consensus.HardFork.Combinator.State.Instances
decodeCurrent :: Decoder s (f blk) -> Decoder s (Current f blk)
decodePast :: Decoder s Past
encodeCurrent :: (f blk -> Encoding) -> Current f blk -> Encoding
encodePast :: Past -> Encoding
instance GHC.Classes.Eq (f blk) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.State.Types.Current f blk)
instance GHC.Show.Show (f blk) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.State.Types.Current f blk)
instance NoThunks.Class.NoThunks (f blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.State.Types.Current f blk)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall blk. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock blk => GHC.Show.Show (f blk)) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.State.Types.HardForkState f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall blk. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock blk => GHC.Classes.Eq (f blk)) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.State.Types.HardForkState f xs)
instance (Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs, forall blk. Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock blk => NoThunks.Class.NoThunks (f blk)) => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.State.Types.HardForkState f xs)
instance Data.SOP.Classes.HAp Ouroboros.Consensus.HardFork.Combinator.State.Types.HardForkState
instance Data.SOP.Classes.HSequence Ouroboros.Consensus.HardFork.Combinator.State.Types.HardForkState
instance Data.SOP.Classes.HCollapse Ouroboros.Consensus.HardFork.Combinator.State.Types.HardForkState
instance Codec.Serialise.Class.Serialise (f blk) => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.Combinator.State.Types.Current f blk)
instance Codec.Serialise.Class.Serialise Ouroboros.Consensus.HardFork.Combinator.State.Types.Past

module Ouroboros.Consensus.HardFork.Combinator.Protocol.LedgerView
type HardForkLedgerView = HardForkLedgerView_ WrapLedgerView
data HardForkLedgerView_ f xs
HardForkLedgerView :: !TransitionInfo -> !HardForkState f xs -> HardForkLedgerView_ f xs

-- | Information about the transition to the next era, if known
[hardForkLedgerViewTransition] :: HardForkLedgerView_ f xs -> !TransitionInfo

-- | The underlying ledger view
[hardForkLedgerViewPerEra] :: HardForkLedgerView_ f xs -> !HardForkState f xs

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Protocol.LedgerView.HardForkLedgerView_ Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerView xs)
instance (Data.SOP.Constraint.SListI xs, GHC.Show.Show a) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Protocol.LedgerView.HardForkLedgerView_ (Data.SOP.BasicFunctors.K a) xs)

module Ouroboros.Consensus.HardFork.Combinator.AcrossEras
newtype PerEraBlockConfig xs
PerEraBlockConfig :: NP BlockConfig xs -> PerEraBlockConfig xs
[getPerEraBlockConfig] :: PerEraBlockConfig xs -> NP BlockConfig xs
newtype PerEraCodecConfig xs
PerEraCodecConfig :: NP CodecConfig xs -> PerEraCodecConfig xs
[getPerEraCodecConfig] :: PerEraCodecConfig xs -> NP CodecConfig xs
newtype PerEraConsensusConfig xs
PerEraConsensusConfig :: NP WrapPartialConsensusConfig xs -> PerEraConsensusConfig xs
[getPerEraConsensusConfig] :: PerEraConsensusConfig xs -> NP WrapPartialConsensusConfig xs
newtype PerEraLedgerConfig xs
PerEraLedgerConfig :: NP WrapPartialLedgerConfig xs -> PerEraLedgerConfig xs
[getPerEraLedgerConfig] :: PerEraLedgerConfig xs -> NP WrapPartialLedgerConfig xs
newtype PerEraProtocolParams xs
PerEraProtocolParams :: NP ProtocolParams xs -> PerEraProtocolParams xs
[getPerEraProtocolParams] :: PerEraProtocolParams xs -> NP ProtocolParams xs
newtype PerEraStorageConfig xs
PerEraStorageConfig :: NP StorageConfig xs -> PerEraStorageConfig xs
[getPerEraStorageConfig] :: PerEraStorageConfig xs -> NP StorageConfig xs
newtype SomeErasCanBeLeader xs
SomeErasCanBeLeader :: NonEmptyOptNP WrapCanBeLeader xs -> SomeErasCanBeLeader xs
[getSomeErasCanBeLeader] :: SomeErasCanBeLeader xs -> NonEmptyOptNP WrapCanBeLeader xs
newtype OneEraApplyTxErr xs
OneEraApplyTxErr :: NS WrapApplyTxErr xs -> OneEraApplyTxErr xs
[getOneEraApplyTxErr] :: OneEraApplyTxErr xs -> NS WrapApplyTxErr xs
newtype OneEraBlock xs
OneEraBlock :: NS I xs -> OneEraBlock xs
[getOneEraBlock] :: OneEraBlock xs -> NS I xs
newtype OneEraCannotForge xs
OneEraCannotForge :: NS WrapCannotForge xs -> OneEraCannotForge xs
[getOneEraCannotForge] :: OneEraCannotForge xs -> NS WrapCannotForge xs
newtype OneEraEnvelopeErr xs
OneEraEnvelopeErr :: NS WrapEnvelopeErr xs -> OneEraEnvelopeErr xs
[getOneEraEnvelopeErr] :: OneEraEnvelopeErr xs -> NS WrapEnvelopeErr xs
newtype OneEraForgeStateInfo xs
OneEraForgeStateInfo :: NS WrapForgeStateInfo xs -> OneEraForgeStateInfo xs
[getOneEraForgeStateInfo] :: OneEraForgeStateInfo xs -> NS WrapForgeStateInfo xs
newtype OneEraForgeStateUpdateError xs
OneEraForgeStateUpdateError :: NS WrapForgeStateUpdateError xs -> OneEraForgeStateUpdateError xs
[getOneEraForgeStateUpdateError] :: OneEraForgeStateUpdateError xs -> NS WrapForgeStateUpdateError xs
newtype OneEraGenTx xs
OneEraGenTx :: NS GenTx xs -> OneEraGenTx xs
[getOneEraGenTx] :: OneEraGenTx xs -> NS GenTx xs
newtype OneEraGenTxId xs
OneEraGenTxId :: NS WrapGenTxId xs -> OneEraGenTxId xs
[getOneEraGenTxId] :: OneEraGenTxId xs -> NS WrapGenTxId xs

-- | The hash for an era
--   
--   This type is special: we don't use an NS here, because the hash by
--   itself should not allow us to differentiate between eras. If it did,
--   the <i>size</i> of the hash would necessarily have to increase, and
--   that leads to trouble. So, the type parameter <tt>xs</tt> here is
--   merely a phantom one, and we just store the underlying raw hash.
newtype OneEraHash (xs :: [k])
OneEraHash :: ShortByteString -> OneEraHash (xs :: [k])
[getOneEraHash] :: OneEraHash (xs :: [k]) -> ShortByteString
newtype OneEraHeader xs
OneEraHeader :: NS Header xs -> OneEraHeader xs
[getOneEraHeader] :: OneEraHeader xs -> NS Header xs
newtype OneEraIsLeader xs
OneEraIsLeader :: NS WrapIsLeader xs -> OneEraIsLeader xs
[getOneEraIsLeader] :: OneEraIsLeader xs -> NS WrapIsLeader xs
newtype OneEraLedgerError xs
OneEraLedgerError :: NS WrapLedgerErr xs -> OneEraLedgerError xs
[getOneEraLedgerError] :: OneEraLedgerError xs -> NS WrapLedgerErr xs
newtype OneEraLedgerEvent xs
OneEraLedgerEvent :: NS WrapLedgerEvent xs -> OneEraLedgerEvent xs
[getOneEraLedgerEvent] :: OneEraLedgerEvent xs -> NS WrapLedgerEvent xs
newtype OneEraLedgerUpdate xs
OneEraLedgerUpdate :: NS WrapLedgerUpdate xs -> OneEraLedgerUpdate xs
[getOneEraLedgerUpdate] :: OneEraLedgerUpdate xs -> NS WrapLedgerUpdate xs
newtype OneEraLedgerWarning xs
OneEraLedgerWarning :: NS WrapLedgerWarning xs -> OneEraLedgerWarning xs
[getOneEraLedgerWarning] :: OneEraLedgerWarning xs -> NS WrapLedgerWarning xs
newtype OneEraSelectView xs
OneEraSelectView :: NS WrapSelectView xs -> OneEraSelectView xs
[getOneEraSelectView] :: OneEraSelectView xs -> NS WrapSelectView xs
newtype OneEraTipInfo xs
OneEraTipInfo :: NS WrapTipInfo xs -> OneEraTipInfo xs
[getOneEraTipInfo] :: OneEraTipInfo xs -> NS WrapTipInfo xs
newtype OneEraValidateView xs
OneEraValidateView :: NS WrapValidateView xs -> OneEraValidateView xs
[getOneEraValidateView] :: OneEraValidateView xs -> NS WrapValidateView xs
newtype OneEraValidatedGenTx xs
OneEraValidatedGenTx :: NS WrapValidatedGenTx xs -> OneEraValidatedGenTx xs
[getOneEraValidatedGenTx] :: OneEraValidatedGenTx xs -> NS WrapValidatedGenTx xs
newtype OneEraValidationErr xs
OneEraValidationErr :: NS WrapValidationErr xs -> OneEraValidationErr xs
[getOneEraValidationErr] :: OneEraValidationErr xs -> NS WrapValidationErr xs

-- | Extra info for errors caused by applying a block, header, transaction,
--   or query from one era to a ledger from a different era.
data EraMismatch
EraMismatch :: !Text -> !Text -> EraMismatch

-- | Name of the era of the ledger (<a>Byron</a> or <a>Shelley</a>).
[ledgerEraName] :: EraMismatch -> !Text

-- | Era of the block, header, transaction, or query.
[otherEraName] :: EraMismatch -> !Text
newtype MismatchEraInfo xs
MismatchEraInfo :: Mismatch SingleEraInfo LedgerEraInfo xs -> MismatchEraInfo xs

-- | Era mismatch
--   
--   We have an era mismatch between the era of a
--   block<i>header</i>tx/query and the era of the current ledger.
[getMismatchEraInfo] :: MismatchEraInfo xs -> Mismatch SingleEraInfo LedgerEraInfo xs

-- | A mismatch _must_ involve a future era
mismatchFutureEra :: SListI xs => MismatchEraInfo (x ': xs) -> NS SingleEraInfo xs
mismatchOneEra :: MismatchEraInfo '[b] -> Void

-- | When a transaction or block from a certain era was applied to a ledger
--   from another era, we get a <a>MismatchEraInfo</a>.
--   
--   Given such a <a>MismatchEraInfo</a>, return the name of the era of the
--   transaction/block and the name of the era of the ledger.
mkEraMismatch :: SListI xs => MismatchEraInfo xs -> EraMismatch
getSameValue :: forall xs a. (IsNonEmpty xs, Eq a, SListI xs, HasCallStack) => NP (K a) xs -> a
oneEraBlockHeader :: CanHardFork xs => OneEraBlock xs -> OneEraHeader xs
instance forall k (xs :: [k]). Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHash xs)
instance forall k (xs :: [k]). NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHash xs)
instance forall k (xs :: [k]). GHC.Classes.Ord (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHash xs)
instance forall k (xs :: [k]). GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHash xs)
instance GHC.Generics.Generic Ouroboros.Consensus.HardFork.Combinator.AcrossEras.EraMismatch
instance GHC.Show.Show Ouroboros.Consensus.HardFork.Combinator.AcrossEras.EraMismatch
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.Combinator.AcrossEras.EraMismatch
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.PerEraBlockConfig xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.PerEraCodecConfig xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.PerEraConsensusConfig xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.PerEraLedgerConfig xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.PerEraStorageConfig xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraEnvelopeErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraGenTx xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraGenTxId xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHeader xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraLedgerError xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraSelectView xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraTipInfo xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraValidatedGenTx xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraValidationErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.MismatchEraInfo xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraApplyTxErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraEnvelopeErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraGenTx xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraGenTxId xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraLedgerError xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraLedgerUpdate xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraLedgerWarning xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraSelectView xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraTipInfo xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraValidatedGenTx xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraValidationErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Ord (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraGenTxId xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraEnvelopeErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraForgeStateInfo xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraForgeStateUpdateError xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraLedgerError xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraLedgerUpdate xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraLedgerWarning xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraTipInfo xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraValidatedGenTx xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraValidationErr xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.MismatchEraInfo xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.MismatchEraInfo xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraApplyTxErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraCannotForge xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraGenTx xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraGenTxId xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHeader xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraSelectView xs)
instance forall k (xs :: [k]). GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHash xs)
instance forall k (xs :: [k]). Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.HardFork.Combinator.AcrossEras.OneEraHash xs)

module Ouroboros.Consensus.HardFork.Combinator.Basics
newtype HardForkBlock xs
HardForkBlock :: OneEraBlock xs -> HardForkBlock xs
[getHardForkBlock] :: HardForkBlock xs -> OneEraBlock xs
data HardForkProtocol (xs :: [Type])

-- | Ledger state associated with a block
data family LedgerState blk :: Type

-- | Static configuration required to work with this type of blocks
data family BlockConfig blk :: Type

-- | Static configuration required for serialisation and deserialisation of
--   types pertaining to this type of block.
--   
--   Data family instead of type family to get better type inference.
data family CodecConfig blk :: Type

-- | Static configuration required to run the consensus protocol
--   
--   Every method in the <a>ConsensusProtocol</a> class takes the consensus
--   configuration as a parameter, so having this as a data family rather
--   than a type family resolves most ambiguity.
--   
--   Defined out of the class so that protocols can define this type
--   without having to define the entire protocol at the same time (or
--   indeed in the same module).
data family ConsensusConfig p :: Type
data HardForkLedgerConfig xs
HardForkLedgerConfig :: !Shape xs -> !PerEraLedgerConfig xs -> HardForkLedgerConfig xs
[hardForkLedgerConfigShape] :: HardForkLedgerConfig xs -> !Shape xs
[hardForkLedgerConfigPerEra] :: HardForkLedgerConfig xs -> !PerEraLedgerConfig xs

-- | Config needed for the <a>NodeInitStorage</a> class. Defined here to
--   avoid circular dependencies.
data family StorageConfig blk :: Type
completeConsensusConfig' :: forall blk. HasPartialConsensusConfig (BlockProtocol blk) => EpochInfo (Except PastHorizonException) -> WrapPartialConsensusConfig blk -> ConsensusConfig (BlockProtocol blk)
completeConsensusConfig'' :: forall blk. HasPartialConsensusConfig (BlockProtocol blk) => EpochInfo (Except PastHorizonException) -> WrapPartialConsensusConfig blk -> WrapConsensusConfig blk
completeLedgerConfig' :: forall blk. HasPartialLedgerConfig blk => EpochInfo (Except PastHorizonException) -> WrapPartialLedgerConfig blk -> LedgerConfig blk
completeLedgerConfig'' :: forall blk. HasPartialLedgerConfig blk => EpochInfo (Except PastHorizonException) -> WrapPartialLedgerConfig blk -> WrapLedgerConfig blk
distribLedgerConfig :: CanHardFork xs => EpochInfo (Except PastHorizonException) -> LedgerConfig (HardForkBlock xs) -> NP WrapLedgerConfig xs
distribTopLevelConfig :: All SingleEraBlock xs => EpochInfo (Except PastHorizonException) -> TopLevelConfig (HardForkBlock xs) -> NP TopLevelConfig xs
data () => EpochInfo (m :: Type -> Type)

-- | The parameterizable exception monad.
--   
--   Computations are either exceptions or normal values.
--   
--   The <a>return</a> function returns a normal value, while
--   <tt>&gt;&gt;=</tt> exits on the first exception. For a variant that
--   continues after an error and collects all the errors, see
--   <a>Errors</a>.
type Except e = ExceptT e Identity
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkProtocol xs))
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkProtocol xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.BlockConfig (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.CodecConfig (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.StorageConfig (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkLedgerConfig xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkLedgerConfig xs)
instance Data.Typeable.Internal.Typeable xs => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)


-- | Intended for qualified import
--   
--   <pre>
--   import Ouroboros.Consensus.HardFork.Combinator.State (HardForkState(..))
--   import qualified Ouroboros.Consensus.HardFork.Combinator.State as State
--   </pre>
module Ouroboros.Consensus.HardFork.Combinator.State

-- | Generic hard fork state
--   
--   This is used both for the consensus state and the ledger state.
--   
--   By using a telescope with <tt>f ~ LedgerState</tt>, we will keep track
--   of <a>Past</a> information for eras before the current one:
--   
--   <pre>
--   TZ currentByronState
--   TZ pastByronState $ TZ currentShelleyState
--   TZ pastByronState $ TS pastShelleyState $ TZ currentAllegraState
--   ...
--   </pre>
--   
--   These are some intuitions on how the Telescope operations behave for
--   this type:
--   
--   <h1><tt>extend</tt></h1>
--   
--   Suppose we have a telescope containing the ledger state. The "how to
--   extend" argument would take, say, the final Byron state to the initial
--   Shelley state; and "where to extend from" argument would indicate when
--   we want to extend: when the current slot number has gone past the end
--   of the Byron era.
--   
--   <h1><tt>retract</tt></h1>
--   
--   Suppose we have a telescope containing the consensus state. When we
--   rewind the consensus state, we might cross a hard fork transition
--   point. So we first <i>retract</i> the telescope <i>to</i> the era
--   containing the slot number that we want to rewind to, and only then
--   call <tt>rewindChainDepState</tt> on that era. Of course, retraction
--   may fail (we might not <i>have</i> past consensus state to rewind to
--   anymore); this failure would require a choice for a particular monad
--   <tt>m</tt>.
--   
--   <h1><tt>align</tt></h1>
--   
--   Suppose we have one telescope containing the already-ticked ledger
--   state, and another telescope containing the consensus state. Since the
--   ledger state has already been ticked, it might have been advanced to
--   the next era. If this happens, we should then align the consensus
--   state with the ledger state, moving <i>it</i> also to the next era,
--   before we can do the consensus header validation check. Note that in
--   this particular example, the ledger state will always be ahead of the
--   consensus state, never behind; <tt>alignExtend</tt> can be used in
--   this case.
newtype HardForkState f xs
HardForkState :: Telescope (K Past) (Current f) xs -> HardForkState f xs
[getHardForkState] :: HardForkState f xs -> Telescope (K Past) (Current f) xs

-- | Information about the current era
data Current f blk
Current :: !Bound -> !f blk -> Current f blk
[currentStart] :: Current f blk -> !Bound
[currentState] :: Current f blk -> !f blk

-- | Information about a past era
data Past
Past :: !Bound -> !Bound -> Past
[pastStart] :: Past -> !Bound
[pastEnd] :: Past -> !Bound

-- | A <tt>h</tt> situated in time
data Situated h f xs
[SituatedCurrent] :: Current f x -> h x -> Situated h f (x ': xs)
[SituatedNext] :: Current f x -> h y -> Situated h f (x ': (y ': xs))
[SituatedFuture] :: Current f x -> NS h xs -> Situated h f (x ': (y ': xs))
[SituatedPast] :: K Past x -> h x -> Situated h f (x ': xs)
[SituatedShift] :: Situated h f xs -> Situated h f (x ': xs)

-- | Translate <tt>f x</tt> to <tt>f y</tt> across an era transition
--   
--   Typically <tt>f</tt> will be <tt>LedgerState</tt> or
--   <tt>WrapChainDepState</tt>.
newtype Translate f x y
Translate :: (EpochNo -> f x -> f y) -> Translate f x y
[translateWith] :: Translate f x y -> EpochNo -> f x -> f y

-- | Forecast a <tt>view y</tt> from a <tt>state x</tt> across an era
--   transition.
--   
--   In addition to the <a>Bound</a> of the transition, this is also told
--   the <a>SlotNo</a> we're constructing a forecast for. This enables the
--   translation function to take into account any scheduled changes that
--   the final ledger view in the preceding era might have.
newtype CrossEraForecaster state view x y
CrossEraForecaster :: (Bound -> SlotNo -> state x -> Except OutsideForecastRange (view y)) -> CrossEraForecaster state view x y
[crossEraForecastWith] :: CrossEraForecaster state view x y -> Bound -> SlotNo -> state x -> Except OutsideForecastRange (view y)

-- | Knowledge in a particular era of the transition to the next era
data TransitionInfo

-- | No transition is yet known for this era We instead record the ledger
--   tip (which must be in <i>this</i> era)
--   
--   NOTE: If we are forecasting, this will be set to the slot number of
--   the (past) ledger state in which the forecast was created. This means
--   that when we construct an <tt>EpochInfo</tt> using a
--   <tt>HardForkLedgerView</tt>, the range of that <tt>EpochInfo</tt> will
--   extend a safe zone from that <i>past</i> ledger state.
TransitionUnknown :: !WithOrigin SlotNo -> TransitionInfo

-- | Transition to the next era is known to happen at this <a>EpochNo</a>
TransitionKnown :: !EpochNo -> TransitionInfo

-- | The transition is impossible
--   
--   This can be due to one of two reasons:
--   
--   <ul>
--   <li>We are in the final era</li>
--   <li>This era has not actually begun yet (we are forecasting). In this
--   case, we cannot look past the safe zone of this era and hence, by
--   definition, the transition to the <i>next</i> era cannot happen.</li>
--   </ul>
TransitionImpossible :: TransitionInfo
sequence :: forall f m xs. (SListI xs, Functor m) => HardForkState (m :.: f) xs -> m (HardForkState f xs)
match :: SListI xs => NS h xs -> HardForkState f xs -> Either (Mismatch h (Current f) xs) (HardForkState (Product h f) xs)
initHardForkState :: f x -> HardForkState f (x ': xs)
fromTZ :: HardForkState f '[blk] -> f blk
tip :: SListI xs => HardForkState f xs -> NS f xs
situate :: NS h xs -> HardForkState f xs -> Situated h f xs
align :: forall xs f f' f''. All SingleEraBlock xs => InPairs (Translate f) xs -> NP (f' -.-> (f -.-> f'')) xs -> HardForkState f' xs -> HardForkState f xs -> HardForkState f'' xs
reconstructSummary :: Shape xs -> TransitionInfo -> HardForkState f xs -> Summary xs

-- | Thin wrapper around <a>sequence</a>
sequenceHardForkState :: forall m f xs. (All Top xs, Functor m) => HardForkState (m :.: f) xs -> m (HardForkState f xs)
getTip :: forall f xs. CanHardFork xs => (forall blk. SingleEraBlock blk => f blk -> Point blk) -> HardForkState f xs -> Point (HardForkBlock xs)

-- | Recover <a>HardForkState</a> from partial information
--   
--   The primary goal of this is to make sure that for the <i>current</i>
--   state we really only need to store the underlying <tt>f</tt>. It is
--   not strictly essential that this is possible but it helps with the
--   unary hardfork case, and it may in general help with binary
--   compatibility.
recover :: forall f xs. CanHardFork xs => Telescope (K Past) f xs -> HardForkState f xs

-- | Construct <a>EpochInfo</a> from the ledger state
--   
--   NOTE: The resulting <a>EpochInfo</a> is a snapshot only, with a
--   limited range. It should not be stored.
epochInfoLedger :: All SingleEraBlock xs => HardForkLedgerConfig xs -> HardForkState LedgerState xs -> EpochInfo (Except PastHorizonException)

-- | Construct <a>EpochInfo</a> given precomputed <a>TransitionInfo</a>
--   
--   The transition and state arguments are acquired either from a ticked
--   ledger state or a ledger view.
epochInfoPrecomputedTransitionInfo :: Shape xs -> TransitionInfo -> HardForkState f xs -> EpochInfo (Except PastHorizonException)
mostRecentTransitionInfo :: All SingleEraBlock xs => HardForkLedgerConfig xs -> HardForkState LedgerState xs -> TransitionInfo
reconstructSummaryLedger :: All SingleEraBlock xs => HardForkLedgerConfig xs -> HardForkState LedgerState xs -> Summary xs

-- | Extend the telescope until the specified slot is within the era at the
--   tip
extendToSlot :: forall xs. CanHardFork xs => HardForkLedgerConfig xs -> SlotNo -> HardForkState LedgerState xs -> HardForkState LedgerState xs

module Ouroboros.Consensus.HardFork.Combinator.Block
data family Header blk :: Type

-- | Context identifying what kind of block we have
--   
--   In almost all places we will use <a>NestedCtxt</a> rather than
--   <a>NestedCtxt_</a>.
data family NestedCtxt_ blk :: (Type -> Type) -> (Type -> Type)
distribAnnTip :: SListI xs => AnnTip (HardForkBlock xs) -> NS AnnTip xs
undistribAnnTip :: SListI xs => NS AnnTip xs -> AnnTip (HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => GHC.Show.Show (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) Ouroboros.Consensus.Block.Abstract.Header a)
instance Data.Typeable.Internal.Typeable xs => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Block.Abstract.GetHeader (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Network.Block.StandardHash (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Network.Block.HasHeader (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Network.Block.HasHeader (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Block.Abstract.GetPrevHash (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Block.NestedContent.NestedCtxt_ (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) Ouroboros.Consensus.Block.Abstract.Header)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Block.NestedContent.HasNestedContent Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Block.Abstract.ConvertRawHash (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.HeaderValidation.HasAnnTip (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.HeaderValidation.BasicEnvelopeValidation (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Data.SOP.Constraint.All GHC.Classes.Eq xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Data.SOP.Constraint.All (Data.SOP.Constraint.Compose GHC.Classes.Eq Ouroboros.Consensus.Block.Abstract.Header) xs => GHC.Classes.Eq (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))

module Ouroboros.Consensus.HardFork.Combinator.Protocol
newtype HardForkSelectView xs
HardForkSelectView :: WithBlockNo OneEraSelectView xs -> HardForkSelectView xs
[getHardForkSelectView] :: HardForkSelectView xs -> WithBlockNo OneEraSelectView xs

-- | We have one or more <a>BlockForging</a>s, and thus <a>CanBeLeader</a>
--   proofs, for each era in which we can forge blocks.
type HardForkCanBeLeader xs = SomeErasCanBeLeader xs
type HardForkChainDepState xs = HardForkState WrapChainDepState xs

-- | We are a leader if we have a proof from one of the eras
type HardForkIsLeader xs = OneEraIsLeader xs
data HardForkValidationErr xs

-- | Validation error from one of the eras
HardForkValidationErrFromEra :: OneEraValidationErr xs -> HardForkValidationErr xs

-- | We tried to apply a block from the wrong era
HardForkValidationErrWrongEra :: MismatchEraInfo xs -> HardForkValidationErr xs
type HardForkLedgerView = HardForkLedgerView_ WrapLedgerView
data HardForkLedgerView_ f xs
HardForkLedgerView :: !TransitionInfo -> !HardForkState f xs -> HardForkLedgerView_ f xs

-- | Information about the transition to the next era, if known
[hardForkLedgerViewTransition] :: HardForkLedgerView_ f xs -> !TransitionInfo

-- | The underlying ledger view
[hardForkLedgerViewPerEra] :: HardForkLedgerView_ f xs -> !HardForkState f xs

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkSelectView xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkSelectView xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkSelectView xs)
instance GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkValidationErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkValidationErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkValidationErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkValidationErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkProtocol xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Ord (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkSelectView xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)

module Ouroboros.Consensus.HardFork.Combinator.Node.Metrics
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Block.SupportsMetrics.BlockSupportsMetrics (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)

module Ouroboros.Consensus.HardFork.Combinator.Ledger
data HardForkEnvelopeErr xs

-- | Validation error from one of the eras
HardForkEnvelopeErrFromEra :: OneEraEnvelopeErr xs -> HardForkEnvelopeErr xs

-- | We tried to apply a block from the wrong era
HardForkEnvelopeErrWrongEra :: MismatchEraInfo xs -> HardForkEnvelopeErr xs
data HardForkLedgerError xs

-- | Validation error from one of the eras
HardForkLedgerErrorFromEra :: OneEraLedgerError xs -> HardForkLedgerError xs

-- | We tried to apply a block from the wrong era
HardForkLedgerErrorWrongEra :: MismatchEraInfo xs -> HardForkLedgerError xs
data HardForkLedgerUpdate xs
HardForkUpdateInEra :: OneEraLedgerUpdate xs -> HardForkLedgerUpdate xs

-- | Hard fork transition got confirmed
HardForkUpdateTransitionConfirmed :: EraIndex xs -> EraIndex xs -> EpochNo -> HardForkLedgerUpdate xs

-- | Hard fork transition happened
--   
--   We record the <a>EpochNo</a> at the start of the era after the
--   transition
HardForkUpdateTransitionDone :: EraIndex xs -> EraIndex xs -> EpochNo -> HardForkLedgerUpdate xs

-- | The hard fork transition rolled back
HardForkUpdateTransitionRolledBack :: EraIndex xs -> EraIndex xs -> HardForkLedgerUpdate xs
data HardForkLedgerWarning xs

-- | Warning from the underlying era
HardForkWarningInEra :: OneEraLedgerWarning xs -> HardForkLedgerWarning xs

-- | The transition to the next era does not match the <a>EraParams</a>
--   
--   The <a>EraParams</a> can specify a lower bound on when the transition
--   to the next era will happen. If the actual transition, when confirmed,
--   is <i>before</i> this lower bound, the node is misconfigured and will
--   likely not work correctly. This should be taken care of as soon as
--   possible (before the transition happens).
HardForkWarningTransitionMismatch :: EraIndex xs -> EraParams -> EpochNo -> HardForkLedgerWarning xs

-- | Transition in the final era
--   
--   The final era should never confirm any transitions. For clarity, we
--   also record the index of that final era.
HardForkWarningTransitionInFinalEra :: EraIndex xs -> EpochNo -> HardForkLedgerWarning xs

-- | An already-confirmed transition got un-confirmed
HardForkWarningTransitionUnconfirmed :: EraIndex xs -> HardForkLedgerWarning xs

-- | An already-confirmed transition got changed
--   
--   We record the indices of the era we are transitioning from and to, as
--   well as the old and new <a>EpochNo</a> of that transition, in that
--   order.
HardForkWarningTransitionReconfirmed :: EraIndex xs -> EraIndex xs -> EpochNo -> EpochNo -> HardForkLedgerWarning xs

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type

-- | Forecast annotated with details about the ledger it was derived from
data AnnForecast state view blk
AnnForecast :: Forecast (view blk) -> state blk -> WithOrigin SlotNo -> Maybe Bound -> AnnForecast state view blk
[annForecast] :: AnnForecast state view blk -> Forecast (view blk)
[annForecastState] :: AnnForecast state view blk -> state blk
[annForecastTip] :: AnnForecast state view blk -> WithOrigin SlotNo
[annForecastEnd] :: AnnForecast state view blk -> Maybe Bound

-- | Change a telescope of a forecast into a forecast of a telescope
mkHardForkForecast :: forall state view xs. SListI xs => InPairs (CrossEraForecaster state view) xs -> HardForkState (AnnForecast state view) xs -> Forecast (HardForkLedgerView_ view xs)
instance GHC.Generics.Generic (Ouroboros.Consensus.Ticked.Ticked (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerError xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerError xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerError xs)
instance GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerError xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkEnvelopeErr xs)
instance GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkEnvelopeErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkEnvelopeErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkEnvelopeErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ticked.Ticked (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerWarning xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerWarning xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerUpdate xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerUpdate xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.HardFork.Combinator.Ledger.HardForkLedgerUpdate xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.Inspect.InspectLedger (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.HeaderValidation.ValidateEnvelope (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.Basics.IsLedger (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.Abstract.ApplyBlock (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.Basics.GetTip (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.Basics.GetTip (Ouroboros.Consensus.Ticked.Ticked (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.Abstract.UpdateLedger (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Consensus.HardFork.Abstract.HasHardForkHistory (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)

module Ouroboros.Consensus.HardFork.Combinator.Mempool

-- | Generalized transaction
--   
--   The mempool (and, accordingly, blocks) consist of "generalized
--   transactions"; this could be "proper" transactions (transferring
--   funds) but also other kinds of things such as update proposals,
--   delegations, etc.
data family GenTx blk :: Type
data HardForkApplyTxErr xs

-- | Validation error from one of the eras
HardForkApplyTxErrFromEra :: !OneEraApplyTxErr xs -> HardForkApplyTxErr xs

-- | We tried to apply a block from the wrong era
HardForkApplyTxErrWrongEra :: !MismatchEraInfo xs -> HardForkApplyTxErr xs

-- | A generalized transaction, <a>GenTx</a>, identifier.
data family TxId tx :: Type

-- | " Validated " transaction or block
--   
--   The ledger defines how to validate transactions and blocks. It's
--   possible the type before and after validation may be distinct (eg
--   Alonzo transactions), which originally motivated this family.
--   
--   We also gain the related benefit that certain interface functions,
--   such as those that <i>reapply</i> blocks, can have a more precise type
--   now. TODO
--   
--   Similarly, the Node-to-Client mini protocols can explicitly indicate
--   that the client trusts the blocks from the local server, by having the
--   server send <a>Validated</a> blocks to the client. TODO
--   
--   Note that validation has different implications for a transaction than
--   for a block. In particular, a validated transaction can be " reapplied
--   " to different ledger states, whereas a validated block must only be "
--   reapplied " to the exact same ledger state (eg as part of rebuilding
--   from an on-disk ledger snapshot).
--   
--   Since the ledger defines validation, see the ledger details for
--   concrete examples of what determines the validity (wrt to a
--   <a>LedgerState</a>) of a transaction and/or block. Example properties
--   include: a transaction's claimed inputs exist and are still unspent, a
--   block carries a sufficient cryptographic signature, etc.
data family Validated x :: Type
hardForkApplyTxErrFromEither :: Either (MismatchEraInfo xs) (OneEraApplyTxErr xs) -> HardForkApplyTxErr xs
hardForkApplyTxErrToEither :: HardForkApplyTxErr xs -> Either (MismatchEraInfo xs) (OneEraApplyTxErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance GHC.Generics.Generic (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance GHC.Generics.Generic (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Ord (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance GHC.Generics.Generic (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance GHC.Generics.Generic (Ouroboros.Consensus.HardFork.Combinator.Mempool.HardForkApplyTxErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Mempool.HardForkApplyTxErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Mempool.HardForkApplyTxErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.SupportsMempool.LedgerSupportsMempool (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Data.Typeable.Internal.Typeable xs => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.HardFork.Combinator.Mempool.HardForkApplyTxErr xs)
instance Data.Typeable.Internal.Typeable xs => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Data.Typeable.Internal.Typeable xs => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.SupportsMempool.HasTxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Data.SOP.Constraint.All Ouroboros.Consensus.Ledger.SupportsMempool.HasTxs xs => Ouroboros.Consensus.Ledger.SupportsMempool.HasTxs (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)

module Ouroboros.Consensus.HardFork.Combinator.Ledger.Query

-- | Different queries supported by the ledger, indexed by the result type.
data family BlockQuery blk :: Type -> Type
type HardForkQueryResult xs = Either (MismatchEraInfo xs)
data QueryAnytime result
[GetEraStart] :: QueryAnytime (Maybe Bound)
data QueryHardFork xs result
[GetInterpreter] :: QueryHardFork xs (Interpreter xs)
[GetCurrentEra] :: QueryHardFork xs (EraIndex xs)
data QueryIfCurrent :: [Type] -> Type -> Type
[QZ] :: BlockQuery x result -> QueryIfCurrent (x ': xs) result
[QS] :: QueryIfCurrent xs result -> QueryIfCurrent (x ': xs) result
decodeQueryAnytimeResult :: QueryAnytime result -> forall s. Decoder s result
decodeQueryHardForkResult :: SListI xs => QueryHardFork xs result -> forall s. Decoder s result
encodeQueryAnytimeResult :: QueryAnytime result -> result -> Encoding
encodeQueryHardForkResult :: SListI xs => QueryHardFork xs result -> result -> Encoding
getHardForkQuery :: BlockQuery (HardForkBlock xs) result -> (forall result'. (result :~: HardForkQueryResult xs result') -> QueryIfCurrent xs result' -> r) -> (forall x' xs'. (xs :~: (x' ': xs')) -> ProofNonEmpty xs' -> QueryAnytime result -> EraIndex xs -> r) -> (forall x' xs'. (xs :~: (x' ': xs')) -> ProofNonEmpty xs' -> QueryHardFork xs result -> r) -> r
hardForkQueryInfo :: All SingleEraBlock xs => QueryIfCurrent xs result -> NS SingleEraInfo xs
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => GHC.Show.Show (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) result)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryIfCurrent xs result)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryAnytime result)
instance GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryHardFork xs result)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Network.Protocol.LocalStateQuery.Type.ShowQuery (Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryHardFork xs)
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryHardFork xs)
instance Ouroboros.Network.Protocol.LocalStateQuery.Type.ShowQuery Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryAnytime
instance Ouroboros.Consensus.Util.DepPair.SameDepIndex Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryAnytime
instance Codec.Serialise.Class.Serialise (Ouroboros.Network.Protocol.LocalStateQuery.Codec.Some Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryAnytime)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Network.Protocol.LocalStateQuery.Type.ShowQuery (Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryIfCurrent xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.HardFork.Combinator.Ledger.Query.QueryIfCurrent xs)
instance Data.Typeable.Internal.Typeable xs => Ouroboros.Network.Util.ShowProxy.ShowProxy (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Network.Protocol.LocalStateQuery.Type.ShowQuery (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Consensus.Ledger.Query.QueryLedger (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Abstract.SingleEraBlock.SingleEraBlock xs => Ouroboros.Consensus.Util.DepPair.SameDepIndex (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))

module Ouroboros.Consensus.HardFork.Combinator.Compat

-- | Version of <tt>Query (HardForkBlock xs)</tt> without the restriction
--   to have at least two eras
data HardForkCompatQuery blk :: Type -> Type
[CompatIfCurrent] :: BlockQuery blk result -> HardForkCompatQuery blk result
[CompatAnytime] :: QueryAnytime result -> EraIndex (HardForkIndices blk) -> HardForkCompatQuery blk result
[CompatHardFork] :: QueryHardFork (HardForkIndices blk) result -> HardForkCompatQuery blk result

-- | Get the start of the specified era, if known
compatGetEraStart :: EraIndex (HardForkIndices blk) -> HardForkCompatQuery blk (Maybe Bound)

-- | Get an interpreter for history queries
--   
--   I.e., this can be used for slot<i>epoch</i>time conversions.
compatGetInterpreter :: HardForkCompatQuery blk (Interpreter (HardForkIndices blk))

-- | Submit query to underlying ledger
compatIfCurrent :: BlockQuery blk result -> HardForkCompatQuery blk result

-- | Wrapper used when connecting to a server that's running the HFC with
--   at least two eras
forwardCompatQuery :: forall m x xs. IsNonEmpty xs => (forall result. BlockQuery (HardForkBlock (x ': xs)) result -> m result) -> forall result. HardForkCompatQuery (HardForkBlock (x ': xs)) result -> m result

-- | Wrapper used when connecting to a server that's not using the HFC, or
--   is using the HFC but with a single era only.
singleEraCompatQuery :: forall m blk era. (Monad m, HardForkIndices blk ~ '[era]) => EpochSize -> SlotLength -> (forall result. BlockQuery blk result -> m result) -> forall result. HardForkCompatQuery blk result -> m result

module Ouroboros.Consensus.HardFork.Combinator.Ledger.PeerSelection
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.SupportsPeerSelection.LedgerSupportsPeerSelection (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)

module Ouroboros.Consensus.HardFork.Combinator.Ledger.CommonProtocolParams
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Ledger.CommonProtocolParams.CommonProtocolParams (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)

module Ouroboros.Consensus.HardFork.Combinator.Forging

-- | If we cannot forge, it's because the current era could not forge
type HardForkCannotForge xs = OneEraCannotForge xs

-- | For each era in which we want to forge blocks, we have a
--   <a>BlockForging</a>, and thus <a>ForgeStateInfo</a>.
--   
--   When we update the hard fork forge state, we only update the forge
--   state of the current era. However, the current era <i>might not</i>
--   have a forge state as it lacks a <a>BlockForging</a>.
--   
--   TODO #2766: expire past <tt>ForgeState</tt>
data HardForkForgeStateInfo xs

-- | There is no <a>BlockForging</a> record for the current era.
[CurrentEraLacksBlockForging] :: EraIndex (x ': (y ': xs)) -> HardForkForgeStateInfo (x ': (y ': xs))

-- | The <tt>ForgeState</tt> of the current era was updated.
[CurrentEraForgeStateUpdated] :: OneEraForgeStateInfo xs -> HardForkForgeStateInfo xs

-- | For each era in which we want to forge blocks, we have a
--   <a>BlockForging</a>, and thus <a>ForgeStateUpdateError</a>.
type HardForkForgeStateUpdateError xs = OneEraForgeStateUpdateError xs
hardForkBlockForging :: forall m xs. (CanHardFork xs, Monad m) => Text -> NonEmptyOptNP (BlockForging m) xs -> BlockForging m (HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Forging.HardForkForgeStateInfo xs)

module Ouroboros.Consensus.HardFork.Combinator.Node.InitStorage
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Node.InitStorage.NodeInitStorage (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)


-- | Witness isomorphism between <tt>b</tt> and <tt>HardForkBlock '[b]</tt>
module Ouroboros.Consensus.HardFork.Combinator.Embed.Unary
class Isomorphic f
project :: (Isomorphic f, NoHardForks blk) => f (HardForkBlock '[blk]) -> f blk
inject :: (Isomorphic f, NoHardForks blk) => f blk -> f (HardForkBlock '[blk])
inject' :: forall proxy f x y blk. (Isomorphic f, NoHardForks blk, Coercible x (f blk), Coercible y (f (HardForkBlock '[blk]))) => proxy (f blk) -> x -> y
project' :: forall proxy f x y blk. (Isomorphic f, NoHardForks blk, Coercible x (f (HardForkBlock '[blk])), Coercible y (f blk)) => proxy (f blk) -> x -> y
data ProjHardForkQuery b :: Type -> Type
[ProjHardForkQuery] :: BlockQuery b result' -> ProjHardForkQuery b (HardForkQueryResult '[b] result')
injNestedCtxt :: NestedCtxt f blk a -> NestedCtxt f (HardForkBlock '[blk]) a

-- | Inject <a>BlockQuery</a>
--   
--   Not an instance of <a>Isomorphic</a> because the types change.
injQuery :: BlockQuery b result -> BlockQuery (HardForkBlock '[b]) (HardForkQueryResult '[b] result)
injQueryResult :: result -> HardForkQueryResult '[b] result
projNestedCtxt :: NestedCtxt f (HardForkBlock '[blk]) a -> NestedCtxt f blk a

-- | Project <a>BlockQuery</a>
--   
--   Not an instance of <a>Isomorphic</a> because the types change.
projQuery :: BlockQuery (HardForkBlock '[b]) result -> (forall result'. (result :~: HardForkQueryResult '[b] result') -> BlockQuery b result' -> a) -> a
projQuery' :: BlockQuery (HardForkBlock '[b]) result -> ProjHardForkQuery b result
projQueryResult :: HardForkQueryResult '[b] result -> result
newtype () => I a
I :: a -> I a

-- | <a>Proxy</a> is a type that holds no data, but has a phantom parameter
--   of arbitrary type (or even kind). Its use is to provide type
--   information, even though there is no value available of that type (or
--   it may be too costly to create one).
--   
--   Historically, <tt><a>Proxy</a> :: <a>Proxy</a> a</tt> is a safer
--   alternative to the <tt><a>undefined</a> :: a</tt> idiom.
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy (Void, Int -&gt; Int)
--   Proxy
--   </pre>
--   
--   Proxy can even hold types of higher kinds,
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Either
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Functor
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy complicatedStructure
--   Proxy
--   </pre>
data () => Proxy (t :: k)
Proxy :: Proxy (t :: k)
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic ((->) a)
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapIsLeader
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapValidatedGenTx
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Data.SOP.BasicFunctors.I
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Ledger.SupportsMempool.GenTx
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Block.Abstract.Header
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Block.Abstract.BlockConfig
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Block.Abstract.CodecConfig
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Block.Abstract.StorageConfig
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Ledger.Basics.LedgerState
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapCannotForge
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapForgeStateUpdateError
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapTipInfo
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapHeaderHash
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Network.Block.ChainHash
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Config.TopLevelConfig
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.HeaderValidation.HeaderState
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic (Ouroboros.Consensus.Ticked.Ticked Data.SOP.BasicFunctors.:.: Ouroboros.Consensus.Ledger.Basics.LedgerState)
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Ledger.Extended.ExtLedgerState
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.HeaderValidation.AnnTip
instance GHC.Base.Functor m => Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic (Ouroboros.Consensus.Storage.ChainDB.Init.InitChainDB m)
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Node.ProtocolInfo.ProtocolClientInfo
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Block.Forging.ForgeStateUpdateInfo
instance GHC.Base.Functor m => Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic (Ouroboros.Consensus.Block.Forging.BlockForging m)
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Node.ProtocolInfo.ProtocolInfo
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapApplyTxErr
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapEnvelopeErr
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapCanBeLeader
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapForgeStateInfo
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerView
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic (Ouroboros.Consensus.Util.SomeSecond (Ouroboros.Consensus.Block.NestedContent.NestedCtxt f))
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.TypeFamilyWrappers.WrapLedgerErr
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Unary.Isomorphic Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader

module Ouroboros.Consensus.Storage.ChainDB.Impl.Paths

-- | Return the block info for the block with the given hash. Return
--   <a>Nothing</a> when not in the VolatileDB.
type LookupBlockInfo blk = HeaderHash blk -> Maybe (BlockInfo blk)

-- | Extend the <a>ChainDiff</a> with the successors found by
--   <a>maximalCandidates</a>.
--   
--   In case no successors were found, the original <a>ChainDiff</a> is
--   returned as a singleton.
--   
--   In case successors <i>were</i> found, the original <a>ChainDiff</a> is
--   <i>not</i> included, only its extensions.
--   
--   Only the longest possible extensions are returned, no intermediary
--   prefixes of extensions.
extendWithSuccessors :: forall blk. HasHeader blk => (ChainHash blk -> Set (HeaderHash blk)) -> LookupBlockInfo blk -> ChainDiff (HeaderFields blk) -> NonEmpty (ChainDiff (HeaderFields blk))

-- | Compute the <i>maximal</i> candidates starting at the specified point
--   
--   As discussed in the Consensus Report, the set of <i>maximal</i>
--   candidates doesn't include prefixes.
--   
--   PRECONDITION: the block to which the given point corresponds is part
--   of the VolatileDB.
--   
--   The first element in each list of hashes is the hash <i>after</i> the
--   specified hash. Thus, when building fragments from these lists of
--   hashes, they fragments must be <i>anchored</i> at the specified hash,
--   but not contain it.
--   
--   NOTE: it is possible that no candidates are found, but don't forget
--   that the chain (fragment) ending with <tt>B</tt> is also a potential
--   candidate.
maximalCandidates :: forall blk. (ChainHash blk -> Set (HeaderHash blk)) -> Point blk -> [NonEmpty (HeaderHash blk)]

-- | A path through the VolatileDB from a <a>StreamFrom</a> to a
--   <a>StreamTo</a>.
--   
--   Invariant: the <tt>AnchoredFragment</tt> (oldest first) constructed
--   using the blocks corresponding to the points in the path will be
--   valid, i.e., the blocks will fit onto each other.
data Path blk

-- | The <tt>end</tt> point (<tt><a>StreamToInclusive</a> end</tt>) was not
--   part of the VolatileDB.
NotInVolatileDB :: RealPoint blk -> Path blk

-- | A complete path, from start point to end point was constructed from
--   the VolatileDB. The list contains the points from oldest to newest.
--   
--   <ul>
--   <li>If the lower bound was <tt><a>StreamFromInclusive</a> pt</tt>,
--   then <tt>pt</tt> will be the first element of the list.</li>
--   <li>If the lower bound was <tt><a>StreamFromExclusive</a> pt</tt>,
--   then the first element of the list will correspond to the first block
--   after <tt>pt</tt>.</li>
--   <li>If the upper bound was <tt><a>StreamToInclusive</a> pt</tt>, then
--   <tt>pt</tt> will be the last element of the list.</li>
--   </ul>
CompletelyInVolatileDB :: [RealPoint blk] -> Path blk

-- | Only a partial path could be constructed from the VolatileDB. The
--   missing predecessor could still be in the ImmutableDB. The list
--   contains the points from oldest to newest.
--   
--   <ul>
--   <li>The first element in the list is the point for which no
--   predecessor is available in the VolatileDB. The block corresponding to
--   the point itself, <i>is</i> available in the VolatileDB.</li>
--   <li>The first argument is the hash of predecessor, the block that is
--   not available in the VolatileDB.</li>
--   </ul>
--   
--   Note: if the lower bound is exclusive, the block corresponding to it
--   doesn't have to be part of the VolatileDB, it will result in a
--   <tt>StartToEnd</tt>.
--   
--   The same invariants hold for the upper bound as for
--   <tt>StartToEnd</tt>.
PartiallyInVolatileDB :: HeaderHash blk -> [RealPoint blk] -> Path blk

-- | Construct a path backwards through the VolatileDB.
--   
--   We walk backwards through the VolatileDB, constructing a <a>Path</a>
--   from the <a>StreamTo</a> to the <a>StreamFrom</a>.
--   
--   If the range is invalid, <a>Nothing</a> is returned.
--   
--   See the documentation of <a>Path</a>.
computePath :: forall blk. HasHeader blk => LookupBlockInfo blk -> StreamFrom blk -> StreamTo blk -> Maybe (Path blk)

-- | A reverse path through the VolatileDB starting at a block in the
--   VolatileDB until we reach genesis or leave the VolatileDB.
data ReversePath blk

-- | The path stopped at genesis
StoppedAtGenesis :: ReversePath blk

-- | The path stopped at this hash, which is the hash of the predecessor of
--   the last block in the path (that was still stored in the VolatileDB).
--   
--   The block corresponding to the predecessor is <i>not</i> stored in the
--   VolatileDB. Either because it is missing, or because it is old and has
--   been garbage collected.
--   
--   Since block numbers are consecutive, we subtract 1 from the block
--   number of the last block to obtain the block number corresponding to
--   this hash.
--   
--   EBBs share their block number with their predecessor:
--   
--   <pre>
--   block:         regular block 1 | EBB | regular block 2
--   block number:                X |   X | X + 1
--   </pre>
--   
--   So when the hash refers to regular block 1, we see that the successor
--   block is an EBB and use its block number without subtracting 1.
--   
--   Edge case: if there are two or more consecutive EBBs, we might predict
--   the wrong block number, but there are no consecutive EBBs in practice,
--   they are one epoch apart.
StoppedAt :: HeaderHash blk -> BlockNo -> ReversePath blk
(::>) :: ReversePath blk -> (HeaderFields blk, IsEBB) -> ReversePath blk

-- | Lazily compute the <a>ReversePath</a> that starts (i.e., ends) with
--   the given <a>HeaderHash</a>.
computeReversePath :: forall blk. LookupBlockInfo blk -> HeaderHash blk -> Maybe (ReversePath blk)

-- | Try to connect the point <tt>P</tt> to the chain fragment by chasing
--   the predecessors.
--   
--   When successful, return a <a>ChainDiff</a>: the number of blocks to
--   roll back the chain fragment to the intersection point and a fragment
--   anchored at the intersection point containing the <a>HeaderFields</a>
--   corresponding to the blocks needed to connect to <tt>P</tt>. The
--   intersection point will be the most recent intersection point.
--   
--   Returns <a>Nothing</a> when <tt>P</tt> is not in the VolatileDB or
--   when <tt>P</tt> is not connected to the given chain fragment.
--   
--   POSTCONDITION: the returned number of blocks to roll back is less than
--   or equal to the length of the given chain fragment.
--   
--   Note that the number of returned points can be smaller than the number
--   of blocks to roll back. This means <tt>P</tt> is on a fork shorter
--   than the given chain fragment.
--   
--   A <a>ChainDiff</a> is returned iff <tt>P</tt> is on the chain
--   fragment. Moreover, when the number of blocks to roll back is also 0,
--   it must be that <tt>P</tt> is the tip of the chain fragment.
--   
--   When the suffix of the <a>ChainDiff</a> is non-empty, <tt>P</tt> will
--   be the last point in the suffix.
isReachable :: forall blk. (HasHeader blk, GetHeader blk) => LookupBlockInfo blk -> AnchoredFragment (Header blk) -> RealPoint blk -> Maybe (ChainDiff (HeaderFields blk))
instance Ouroboros.Network.Block.HasHeader blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Paths.Path blk)
instance Ouroboros.Network.Block.HasHeader blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Paths.Path blk)


-- | Thin wrapper around the LedgerDB
module Ouroboros.Consensus.Storage.ChainDB.Impl.LgrDB

-- | Thin wrapper around the ledger database
data LgrDB m blk
type LedgerDB' blk = LedgerDB (ExtLedgerState blk)

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   LgrDB.
type LgrDbSerialiseConstraints blk = (Serialise (HeaderHash blk), EncodeDisk blk (LedgerState blk), DecodeDisk blk (LedgerState blk), EncodeDisk blk (AnnTip blk), DecodeDisk blk (AnnTip blk), EncodeDisk blk (ChainDepState (BlockProtocol blk)), DecodeDisk blk (ChainDepState (BlockProtocol blk)))
data LgrDbArgs f m blk
LgrDbArgs :: DiskPolicy -> HKD f (m (ExtLedgerState blk)) -> SomeHasFS m -> HKD f (TopLevelConfig blk) -> Tracer m (LedgerDB' blk) -> Tracer m (TraceSnapshotEvent blk) -> LgrDbArgs f m blk
[lgrDiskPolicy] :: LgrDbArgs f m blk -> DiskPolicy
[lgrGenesis] :: LgrDbArgs f m blk -> HKD f (m (ExtLedgerState blk))
[lgrHasFS] :: LgrDbArgs f m blk -> SomeHasFS m
[lgrTopLevelConfig] :: LgrDbArgs f m blk -> HKD f (TopLevelConfig blk)
[lgrTraceLedger] :: LgrDbArgs f m blk -> Tracer m (LedgerDB' blk)
[lgrTracer] :: LgrDbArgs f m blk -> Tracer m (TraceSnapshotEvent blk)

-- | Default arguments
defaultArgs :: Applicative m => SomeHasFS m -> DiskPolicy -> LgrDbArgs Defaults m blk

-- | Open the ledger DB
--   
--   In addition to the ledger DB also returns the number of immutable
--   blocks that were replayed.
openDB :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, LgrDbSerialiseConstraints blk, InspectLedger blk, HasCallStack) => LgrDbArgs Identity m blk -> Tracer m (ReplayGoal blk -> TraceReplayEvent blk) -> ImmutableDB m blk -> (RealPoint blk -> m blk) -> m (LgrDB m blk, Word64)

-- | Add the tip of the Immutable DB to the trace event
--   
--   Between the tip of the immutable DB and the point of the starting
--   block, the node could (if it so desired) easily compute a "percentage
--   complete".
decorateReplayTracerWithGoal :: Point blk -> Tracer m (TraceReplayEvent blk) -> Tracer m (ReplayGoal blk -> TraceReplayEvent blk)
currentPoint :: forall blk. UpdateLedger blk => LedgerDB' blk -> Point blk
getCurrent :: IOLike m => LgrDB m blk -> STM m (LedgerDB' blk)
getDiskPolicy :: LgrDB m blk -> DiskPolicy

-- | PRECONDITION: The new <tt>LedgerDB</tt> must be the result of calling
--   either <a>ledgerDbSwitch</a> or <a>ledgerDbPushMany</a> on the current
--   <tt>LedgerDB</tt>.
setCurrent :: IOLike m => LgrDB m blk -> LedgerDB' blk -> STM m ()
takeSnapshot :: forall m blk. (IOLike m, LgrDbSerialiseConstraints blk, HasHeader blk, IsLedger (LedgerState blk)) => LgrDB m blk -> m (Maybe (DiskSnapshot, RealPoint blk))
trimSnapshots :: forall m blk. (MonadCatch m, HasHeader blk) => LgrDB m blk -> m [DiskSnapshot]
data ValidateResult blk
ValidateSuccessful :: LedgerDB' blk -> ValidateResult blk
ValidateLedgerError :: AnnLedgerError' blk -> ValidateResult blk
ValidateExceededRollBack :: ExceededRollback -> ValidateResult blk
validate :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, HasCallStack) => LgrDB m blk -> LedgerDB' blk -> BlockCache blk -> Word64 -> (UpdateLedgerDbTraceEvent blk -> m ()) -> [Header blk] -> m (ValidateResult blk)

-- | Remove all points with a slot older than the given slot from the set
--   of previously applied points.
garbageCollectPrevApplied :: IOLike m => LgrDB m blk -> SlotNo -> STM m ()
getPrevApplied :: IOLike m => LgrDB m blk -> STM m (Set (RealPoint blk))

-- | Annotated ledger errors
data AnnLedgerError l blk
AnnLedgerError :: LedgerDB l -> RealPoint blk -> LedgerErr l -> AnnLedgerError l blk

-- | The ledger DB just <i>before</i> this block was applied
[annLedgerState] :: AnnLedgerError l blk -> LedgerDB l

-- | Reference to the block that had the error
[annLedgerErrRef] :: AnnLedgerError l blk -> RealPoint blk

-- | The ledger error itself
[annLedgerErr] :: AnnLedgerError l blk -> LedgerErr l

-- | On-disk policy
--   
--   We only write ledger states that are older than <tt>k</tt> blocks to
--   disk (that is, snapshots that are guaranteed valid). The on-disk
--   policy determines how often we write to disk and how many checkpoints
--   we keep.
data DiskPolicy
DiskPolicy :: Word -> (TimeSinceLast DiffTime -> Word64 -> Bool) -> DiskPolicy

-- | How many snapshots do we want to keep on disk?
--   
--   A higher number of on-disk snapshots is primarily a safe-guard against
--   disk corruption: it trades disk space for reliability.
--   
--   Examples:
--   
--   <ul>
--   <li><tt>0</tt>: Delete the snapshot immediately after writing.
--   Probably not a useful value :-D</li>
--   <li><tt>1</tt>: Delete the previous snapshot immediately after writing
--   the next Dangerous policy: if for some reason the deletion happens
--   before the new snapshot is written entirely to disk (we don't
--   <tt>fsync</tt>), we have no choice but to start at the genesis
--   snapshot on the next startup.</li>
--   <li><tt>2</tt>: Always keep 2 snapshots around. This means that when
--   we write the next snapshot, we delete the oldest one, leaving the
--   middle one available in case of truncation of the write. This is
--   probably a sane value in most circumstances.</li>
--   </ul>
[onDiskNumSnapshots] :: DiskPolicy -> Word

-- | Should we write a snapshot of the ledger state to disk?
--   
--   This function is passed two bits of information:
--   
--   <ul>
--   <li>The time since the last snapshot, or <a>NoSnapshotTakenYet</a> if
--   none was taken yet. Note that <a>NoSnapshotTakenYet</a> merely means
--   no snapshot had been taking yet since the node was started; it does
--   not necessarily mean that none exist on disk.</li>
--   <li>The distance in terms of blocks applied to the <i>oldest</i>
--   ledger snapshot in memory. During normal operation, this is the number
--   of blocks written to the ImmutableDB since the last snapshot. On
--   startup, it is computed by counting how many immutable blocks we had
--   to reapply to get to the chain tip. This is useful, as it allows the
--   policy to decide to take a snapshot <i>on node startup</i> if a lot of
--   blocks had to be replayed.</li>
--   </ul>
--   
--   See also <a>defaultDiskPolicy</a>
[onDiskShouldTakeSnapshot] :: DiskPolicy -> TimeSinceLast DiffTime -> Word64 -> Bool
data DiskSnapshot

-- | Exceeded maximum rollback supported by the current ledger DB state
--   
--   Under normal circumstances this will not arise. It can really only
--   happen in the presence of data corruption (or when switching to a
--   shorter fork, but that is disallowed by all currently known Ouroboros
--   protocols).
--   
--   Records both the supported and the requested rollback.
data ExceededRollback
ExceededRollback :: Word64 -> Word64 -> ExceededRollback
[rollbackMaximum] :: ExceededRollback -> Word64
[rollbackRequested] :: ExceededRollback -> Word64

-- | Events traced while replaying blocks against the ledger to bring it up
--   to date w.r.t. the tip of the ImmutableDB during initialisation. As
--   this process takes a while, we trace events to inform higher layers of
--   our progress.
data TraceReplayEvent blk

-- | There were no LedgerDB snapshots on disk, so we're replaying all
--   blocks starting from Genesis against the initial ledger.
ReplayFromGenesis :: ReplayGoal blk -> TraceReplayEvent blk
ReplayFromSnapshot :: DiskSnapshot -> RealPoint blk -> ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk
ReplayedBlock :: RealPoint blk -> [LedgerEvent blk] -> ReplayStart blk -> ReplayGoal blk -> TraceReplayEvent blk
data TraceSnapshotEvent blk

-- | An on disk snapshot was skipped because it was invalid.
InvalidSnapshot :: DiskSnapshot -> SnapshotFailure blk -> TraceSnapshotEvent blk

-- | A snapshot was written to disk.
TookSnapshot :: DiskSnapshot -> RealPoint blk -> TraceSnapshotEvent blk

-- | An old or invalid on-disk snapshot was deleted
DeletedSnapshot :: DiskSnapshot -> TraceSnapshotEvent blk

-- | The ledger state at the tip of the chain
ledgerDbCurrent :: GetTip l => LedgerDB l -> l

-- | For testing purposes
mkLgrDB :: StrictTVar m (LedgerDB' blk) -> StrictTVar m (Set (RealPoint blk)) -> (RealPoint blk -> m blk) -> LgrDbArgs Identity m blk -> LgrDB m blk
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.LgrDB.LgrDB m blk)
instance (Ouroboros.Consensus.Util.IOLike.IOLike m, Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.LgrDB.LgrDB m blk)


-- | Types used throughout the implementation: handle, state, environment,
--   types, trace types, etc.
module Ouroboros.Consensus.Storage.ChainDB.Impl.Types
data ChainDbEnv m blk
CDB :: !ImmutableDB m blk -> !VolatileDB m blk -> !LgrDB m blk -> !StrictTVar m (AnchoredFragment (Header blk)) -> !StrictTVar m (TentativeState blk) -> !StrictTVar m (StrictMaybe (Header blk)) -> !StrictTVar m (Map IteratorKey (m ())) -> !StrictTVar m (Map FollowerKey (FollowerHandle m blk)) -> !TopLevelConfig blk -> !StrictTVar m (WithFingerprint (InvalidBlocks blk)) -> !StrictTVar m IteratorKey -> !StrictTVar m FollowerKey -> !StrictMVar m () -> !Tracer m (TraceEvent blk) -> !Tracer m (LedgerDB' blk) -> !ResourceRegistry m -> !DiffTime -> !DiffTime -> !StrictTVar m (m ()) -> !ChunkInfo -> !blk -> Bool -> !CheckInFuture m blk -> !BlocksToAdd m blk -> !StrictTVar m (FutureBlocks m blk) -> ChainDbEnv m blk
[cdbImmutableDB] :: ChainDbEnv m blk -> !ImmutableDB m blk
[cdbVolatileDB] :: ChainDbEnv m blk -> !VolatileDB m blk
[cdbLgrDB] :: ChainDbEnv m blk -> !LgrDB m blk

-- | Contains the current chain fragment.
--   
--   INVARIANT: the anchor point of this fragment is the tip of the
--   ImmutableDB. This implies that this fragment never contains any blocks
--   that are stored in the immutable DB.
--   
--   Note that this fragment might be shorter than <tt>k</tt> headers when
--   the whole chain is shorter than <tt>k</tt> or in case of corruption of
--   the VolatileDB.
--   
--   Note that this fragment might also be <i>longer</i> than <tt>k</tt>
--   headers, because the oldest blocks from the fragment might not yet
--   have been copied from the VolatileDB to the ImmutableDB.
--   
--   The anchor point of this chain should be the most recent "immutable"
--   block according to the protocol, i.e., a block that cannot be rolled
--   back.
--   
--   Note that the "immutable" block isn't necessarily at the tip of the
--   ImmutableDB, but could temporarily still be on the in-memory chain
--   fragment. When the background thread that copies blocks to the
--   ImmutableDB has caught up, the "immutable" block will be at the tip of
--   the ImmutableDB again.
--   
--   Note that the "immutable" block might be less than <tt>k</tt> blocks
--   from our tip in case the whole chain is shorter than <tt>k</tt> or in
--   case of corruption of the VolatileDB.
--   
--   Note that the "immutable" block will <i>never</i> be <i>more</i> than
--   <tt>k</tt> blocks back, as opposed to the anchor point of
--   <a>cdbChain</a>.
[cdbChain] :: ChainDbEnv m blk -> !StrictTVar m (AnchoredFragment (Header blk))
[cdbTentativeState] :: ChainDbEnv m blk -> !StrictTVar m (TentativeState blk)

-- | The tentative header, for diffusion pipelining.
--   
--   INVARIANT: It fits on top of the current chain, and its body is not
--   known to be invalid, but might turn out to be.
[cdbTentativeHeader] :: ChainDbEnv m blk -> !StrictTVar m (StrictMaybe (Header blk))

-- | The iterators.
--   
--   This maps the <a>IteratorKey</a>s of each open <tt>Iterator</tt> to a
--   function that, when called, closes the iterator. This is used when
--   closing the ChainDB: the open file handles used by iterators can be
--   closed, and the iterators themselves are closed so that it is
--   impossible to use an iterator after closing the ChainDB itself.
[cdbIterators] :: ChainDbEnv m blk -> !StrictTVar m (Map IteratorKey (m ()))

-- | The followers.
--   
--   A follower is open iff its <a>FollowerKey</a> is this <a>Map</a>.
--   
--   INVARIANT: the <tt>followerPoint</tt> of each follower is
--   <tt>withinFragmentBounds</tt> of the current chain fragment (retrieved
--   <tt>cdbGetCurrentChain</tt>, not by reading <a>cdbChain</a> directly).
[cdbFollowers] :: ChainDbEnv m blk -> !StrictTVar m (Map FollowerKey (FollowerHandle m blk))
[cdbTopLevelConfig] :: ChainDbEnv m blk -> !TopLevelConfig blk

-- | See the docstring of <a>InvalidBlocks</a>.
--   
--   The <tt>Fingerprint</tt> changes every time a hash is added to the
--   map, but not when hashes are garbage-collected from the map.
[cdbInvalid] :: ChainDbEnv m blk -> !StrictTVar m (WithFingerprint (InvalidBlocks blk))
[cdbNextIteratorKey] :: ChainDbEnv m blk -> !StrictTVar m IteratorKey
[cdbNextFollowerKey] :: ChainDbEnv m blk -> !StrictTVar m FollowerKey

-- | Lock used to ensure that <tt>copyToImmutableDB</tt> is not executed
--   more than once concurrently.
--   
--   Note that <tt>copyToImmutableDB</tt> can still be executed
--   concurrently with all others functions, just not with itself.
[cdbCopyLock] :: ChainDbEnv m blk -> !StrictMVar m ()
[cdbTracer] :: ChainDbEnv m blk -> !Tracer m (TraceEvent blk)
[cdbTraceLedger] :: ChainDbEnv m blk -> !Tracer m (LedgerDB' blk)

-- | Resource registry that will be used to (re)start the background
--   threads, see <tt>cdbBgThreads</tt>.
[cdbRegistry] :: ChainDbEnv m blk -> !ResourceRegistry m

-- | How long to wait between copying a block from the VolatileDB to
--   ImmutableDB and garbage collecting it from the VolatileDB
[cdbGcDelay] :: ChainDbEnv m blk -> !DiffTime

-- | Minimum time between two garbage collections. Is used to batch garbage
--   collections.
[cdbGcInterval] :: ChainDbEnv m blk -> !DiffTime

-- | A handle to kill the background threads.
[cdbKillBgThreads] :: ChainDbEnv m blk -> !StrictTVar m (m ())
[cdbChunkInfo] :: ChainDbEnv m blk -> !ChunkInfo
[cdbCheckIntegrity] :: ChainDbEnv m blk -> !blk -> Bool
[cdbCheckInFuture] :: ChainDbEnv m blk -> !CheckInFuture m blk

-- | Queue of blocks that still have to be added.
[cdbBlocksToAdd] :: ChainDbEnv m blk -> !BlocksToAdd m blk

-- | Blocks from the future
--   
--   Blocks that were added to the ChainDB but that were from the future
--   according to <a>CheckInFuture</a>, without exceeding the clock skew
--   (<tt>inFutureExceedsClockSkew</tt>). Blocks exceeding the clock skew
--   are considered to be invalid (<tt>InFutureExceedsClockSkew</tt>) and
--   will be added <a>cdbInvalid</a>.
--   
--   Whenever a block is added to the ChainDB, we first trigger chain
--   selection for all the blocks in this map so that blocks no longer from
--   the future can get adopted. Note that when no blocks are added to the
--   ChainDB, we will <i>not</i> actively trigger chain selection for the
--   blocks in this map.
--   
--   The number of blocks from the future is bounded by the number of
--   upstream peers multiplied by the max clock skew divided by the slot
--   length.
[cdbFutureBlocks] :: ChainDbEnv m blk -> !StrictTVar m (FutureBlocks m blk)

-- | A handle to the internal ChainDB state
newtype ChainDbHandle m blk
CDBHandle :: StrictTVar m (ChainDbState m blk) -> ChainDbHandle m blk
data ChainDbState m blk
ChainDbOpen :: !ChainDbEnv m blk -> ChainDbState m blk
ChainDbClosed :: ChainDbState m blk

-- | All the serialisation related constraints needed by the ChainDB.
class (ImmutableDbSerialiseConstraints blk, LgrDbSerialiseConstraints blk, VolatileDbSerialiseConstraints blk, EncodeDiskDep (NestedCtxt Header) blk) => SerialiseDiskConstraints blk

-- | Check if the ChainDB is open, if so, executing the given function on
--   the <a>ChainDbEnv</a>, otherwise, throw a <tt>CloseDBError</tt>.
getEnv :: forall m blk r. (IOLike m, HasCallStack, HasHeader blk) => ChainDbHandle m blk -> (ChainDbEnv m blk -> m r) -> m r

-- | Variant 'of <a>getEnv</a> for functions taking one argument.
getEnv1 :: (IOLike m, HasCallStack, HasHeader blk) => ChainDbHandle m blk -> (ChainDbEnv m blk -> a -> m r) -> a -> m r

-- | Variant 'of <a>getEnv</a> for functions taking two arguments.
getEnv2 :: (IOLike m, HasCallStack, HasHeader blk) => ChainDbHandle m blk -> (ChainDbEnv m blk -> a -> b -> m r) -> a -> b -> m r

-- | Variant of <a>getEnv</a> that works in <a>STM</a>.
getEnvSTM :: forall m blk r. (IOLike m, HasCallStack, HasHeader blk) => ChainDbHandle m blk -> (ChainDbEnv m blk -> STM m r) -> STM m r

-- | Variant of <a>getEnv1</a> that works in <a>STM</a>.
getEnvSTM1 :: forall m blk a r. (IOLike m, HasCallStack, HasHeader blk) => ChainDbHandle m blk -> (ChainDbEnv m blk -> a -> STM m r) -> a -> STM m r
data Internal m blk
Internal :: m (WithOrigin SlotNo) -> (SlotNo -> m ()) -> m () -> m Void -> StrictTVar m (m ()) -> Internal m blk

-- | Copy the blocks older than <tt>k</tt> from to the VolatileDB to the
--   ImmutableDB and update the in-memory chain fragment correspondingly.
--   
--   The <a>SlotNo</a> of the tip of the ImmutableDB after copying the
--   blocks is returned. This can be used for a garbage collection on the
--   VolatileDB.
[intCopyToImmutableDB] :: Internal m blk -> m (WithOrigin SlotNo)

-- | Perform garbage collection for blocks &lt;= the given <a>SlotNo</a>.
[intGarbageCollect] :: Internal m blk -> SlotNo -> m ()

-- | Write a new LedgerDB snapshot to disk and remove the oldest one(s).
[intUpdateLedgerSnapshots] :: Internal m blk -> m ()

-- | Start the loop that adds blocks to the ChainDB retrieved from the
--   queue populated by <a>addBlock</a>. Execute this loop in a separate
--   thread.
[intAddBlockRunner] :: Internal m blk -> m Void

-- | A handle to kill the background threads.
[intKillBgThreads] :: Internal m blk -> StrictTVar m (m ())

-- | We use this internally to track iterators in a map
--   (<a>cdbIterators</a>) in the ChainDB state so that we can remove them
--   from the map when the iterator is closed.
--   
--   We store them in the map so that the ChainDB can close all open
--   iterators when it is closed itself.
newtype IteratorKey
IteratorKey :: Word -> IteratorKey

-- | Internal handle to a <tt>Follower</tt> without an explicit <tt>b</tt>
--   (<tt>blk</tt>, <tt><a>Header</a> blk</tt>, etc.) parameter so
--   <tt>Follower</tt>s with different' <tt>b</tt>s can be stored together
--   in <a>cdbFollowers</a>.
data FollowerHandle m blk
FollowerHandle :: ChainType -> (Point blk -> Set (Point blk) -> STM m ()) -> m () -> FollowerHandle m blk

-- | Whether we follow the tentative chain.
[fhChainType] :: FollowerHandle m blk -> ChainType

-- | When we have switched to a fork, all open <tt>Follower</tt>s must be
--   notified.
[fhSwitchFork] :: FollowerHandle m blk -> Point blk -> Set (Point blk) -> STM m ()

-- | When closing the ChainDB, we must also close all open
--   <tt>Follower</tt>s, as they might be holding on to resources.
--   
--   Call <a>fhClose</a> will release the resources used by the
--   <tt>Follower</tt>.
--   
--   NOTE the <tt>Follower</tt> is not removed from <a>cdbFollowers</a>.
--   (That is done by <tt>closeAllFollowers</tt>).
[fhClose] :: FollowerHandle m blk -> m ()

-- | We use this internally to track follower in a map
--   (<a>cdbFollowers</a>) in the ChainDB state so that we can remove them
--   from the map when the follower is closed.
--   
--   We store them in the map so that the ChainDB can close all open
--   followers when it is closed itself and to update the followers in case
--   we switch to a different chain.
newtype FollowerKey
FollowerKey :: Word -> FollowerKey

-- | Similar to <a>FollowerState</a>.
data FollowerRollState blk

-- | We don't know at which point the user is, but the next message we'll
--   send is to roll back to this point.
RollBackTo :: !Point blk -> FollowerRollState blk

-- | We know that the follower is at this point and the next message we'll
--   send is to roll forward to the point <i>after</i> this point on our
--   chain.
RollForwardFrom :: !Point blk -> FollowerRollState blk

-- | <tt>b</tt> corresponds to the <tt>BlockComponent</tt> that is being
--   read.
data FollowerState m blk b

-- | The <tt>Follower</tt> is in its initial state. Its
--   <a>FollowerRollState</a> is <tt><a>RollBackTo</a>
--   <tt>genesisPoint</tt></tt>.
--   
--   This is equivalent to having a <a>FollowerInImmutableDB</a> with the
--   same <a>FollowerRollState</a> and an iterator streaming after genesis.
--   Opening such an iterator has a cost (index files will have to be
--   read). However, in most cases, right after opening a Follower, the
--   user of the Follower will try to move it forward, moving it from
--   genesis to a more recent point on the chain. So we incur the cost of
--   opening the iterator while not even using it.
--   
--   Therefore, we have this extra initial state, that avoids this cost.
--   When the user doesn't move the Follower forward, an iterator is
--   opened.
FollowerInit :: FollowerState m blk b

-- | The <tt>Follower</tt> is reading from the ImmutableDB.
--   
--   Note that the iterator includes 'Point blk' in addition to <tt>b</tt>,
--   as it is needed to keep track of where the iterator is.
--   
--   INVARIANT: for all <tt>FollowerInImmutableDB rollState immIt</tt>: the
--   predecessor of the next block streamed by <tt>immIt</tt> must be the
--   block identified by <tt>followerRollStatePoint rollState</tt>. In
--   other words: the iterator is positioned <i>on</i>
--   <tt>followerRollStatePoint rollState</tt>.
FollowerInImmutableDB :: !FollowerRollState blk -> !Iterator m blk (Point blk, b) -> FollowerState m blk b

-- | The <tt>Follower</tt> is reading from the in-memory current chain
--   fragment.
FollowerInMem :: !FollowerRollState blk -> FollowerState m blk b

-- | Get the point the <a>FollowerRollState</a> should roll back to or roll
--   forward from.
followerRollStatePoint :: FollowerRollState blk -> Point blk

-- | In addition to the reason why a block is invalid, the slot number of
--   the block is stored, so that whenever a garbage collection is
--   performed on the VolatileDB for some slot <tt>s</tt>, the hashes older
--   or equal to <tt>s</tt> can be removed from this map.
data InvalidBlockInfo blk
InvalidBlockInfo :: !InvalidBlockReason blk -> !SlotNo -> InvalidBlockInfo blk
[invalidBlockReason] :: InvalidBlockInfo blk -> !InvalidBlockReason blk
[invalidBlockSlotNo] :: InvalidBlockInfo blk -> !SlotNo

-- | Hashes corresponding to invalid blocks. This is used to ignore these
--   blocks during chain selection.
type InvalidBlocks blk = Map (HeaderHash blk) (InvalidBlockInfo blk)

-- | Blocks from the future for which we still need to trigger chain
--   selection.
--   
--   See <a>cdbFutureBlocks</a> for more info.
type FutureBlocks m blk = Map (HeaderHash blk) (Header blk, InvalidBlockPunishment m)

-- | Entry in the <a>BlocksToAdd</a> queue: a block together with the
--   <tt>TMVar</tt>s used to implement <a>AddBlockPromise</a>.
data BlockToAdd m blk
BlockToAdd :: !InvalidBlockPunishment m -> !blk -> !StrictTMVar m Bool -> !StrictTMVar m (AddBlockResult blk) -> BlockToAdd m blk

-- | Executed immediately upon determining this block or one from its
--   prefix is invalid.
[blockPunish] :: BlockToAdd m blk -> !InvalidBlockPunishment m
[blockToAdd] :: BlockToAdd m blk -> !blk

-- | Used for the <a>blockWrittenToDisk</a> field of
--   <a>AddBlockPromise</a>.
[varBlockWrittenToDisk] :: BlockToAdd m blk -> !StrictTMVar m Bool

-- | Used for the <a>blockProcessed</a> field of <a>AddBlockPromise</a>.
[varBlockProcessed] :: BlockToAdd m blk -> !StrictTMVar m (AddBlockResult blk)

-- | FIFO queue used to add blocks asynchronously to the ChainDB. Blocks
--   are read from this queue by a background thread, which processes the
--   blocks synchronously.
data BlocksToAdd m blk

-- | Add a block to the <a>BlocksToAdd</a> queue. Can block when the queue
--   is full.
addBlockToAdd :: (IOLike m, HasHeader blk) => Tracer m (TraceAddBlockEvent blk) -> BlocksToAdd m blk -> InvalidBlockPunishment m -> blk -> m (AddBlockPromise m blk)

-- | Flush the <a>BlocksToAdd</a> queue and notify the waiting threads.
closeBlocksToAdd :: IOLike m => BlocksToAdd m blk -> STM m ()

-- | Get the oldest block from the <a>BlocksToAdd</a> queue. Can block when
--   the queue is empty.
getBlockToAdd :: IOLike m => BlocksToAdd m blk -> m (BlockToAdd m blk)

-- | Create a new <a>BlocksToAdd</a> with the given size.
newBlocksToAdd :: IOLike m => Word -> m (BlocksToAdd m blk)

-- | Information on having changed our selection to a chain with a
--   (necessarily) new tip.
--   
--   NOTE: the fields of this record are intentionally lazy to prevent the
--   forcing of this information in case it doesn't have to be traced.
--   However, this means that the tracer processing this message <i>must
--   not</i> hold on to it, otherwise it leaks memory.
data SelectionChangedInfo blk
SelectionChangedInfo :: RealPoint blk -> EpochNo -> Word64 -> RealPoint blk -> SelectView (BlockProtocol blk) -> Maybe (SelectView (BlockProtocol blk)) -> SelectionChangedInfo blk

-- | The new tip of the current chain.
[newTipPoint] :: SelectionChangedInfo blk -> RealPoint blk

-- | The epoch of the new tip.
[newTipEpoch] :: SelectionChangedInfo blk -> EpochNo

-- | The slot in the epoch, i.e., the relative slot number, of the new tip.
[newTipSlotInEpoch] :: SelectionChangedInfo blk -> Word64

-- | The new tip of the current chain (<a>newTipPoint</a>) is the result of
--   performing chain selection for a <i>trigger</i> block
--   (<a>newTipTrigger</a>). In most cases, we add a new block to the tip
--   of the current chain, in which case the new tip <i>is</i> the trigger
--   block.
--   
--   However, this is not always the case. For example, with our current
--   chain being A and having a disconnected C lying around, adding B will
--   result in A -&gt; B -&gt; C as the new chain. The trigger B /= the new
--   tip C.
[newTipTrigger] :: SelectionChangedInfo blk -> RealPoint blk

-- | The <a>SelectView</a> of the new tip. It is guaranteed that
--   
--   <pre>
--   &gt;&gt;&gt; Just newTipSelectView &gt; oldTipSelectView
--   True
--   </pre>
[newTipSelectView] :: SelectionChangedInfo blk -> SelectView (BlockProtocol blk)

-- | The <a>SelectView</a> of the old, previous tip. This can be
--   <a>Nothing</a> when the previous chain/tip was Genesis.
[oldTipSelectView] :: SelectionChangedInfo blk -> Maybe (SelectView (BlockProtocol blk))

-- | Trace type for the various events that occur when adding a block.
data TraceAddBlockEvent blk

-- | A block with a <a>BlockNo</a> more than <tt>k</tt> back than the
--   current tip was ignored.
IgnoreBlockOlderThanK :: RealPoint blk -> TraceAddBlockEvent blk

-- | A block that is already in the Volatile DB was ignored.
IgnoreBlockAlreadyInVolatileDB :: RealPoint blk -> TraceAddBlockEvent blk

-- | A block that is know to be invalid was ignored.
IgnoreInvalidBlock :: RealPoint blk -> InvalidBlockReason blk -> TraceAddBlockEvent blk

-- | The block was added to the queue and will be added to the ChainDB by
--   the background thread. The size of the queue is included.
AddedBlockToQueue :: RealPoint blk -> Enclosing' Word -> TraceAddBlockEvent blk

-- | The block popped from the queue and will imminently be added to the
--   ChainDB.
PoppedBlockFromQueue :: Enclosing' (RealPoint blk) -> TraceAddBlockEvent blk

-- | The block is from the future, i.e., its slot number is greater than
--   the current slot (the second argument).
BlockInTheFuture :: RealPoint blk -> SlotNo -> TraceAddBlockEvent blk

-- | A block was added to the Volatile DB
AddedBlockToVolatileDB :: RealPoint blk -> BlockNo -> IsEBB -> Enclosing -> TraceAddBlockEvent blk

-- | The block fits onto the current chain, we'll try to use it to extend
--   our chain.
TryAddToCurrentChain :: RealPoint blk -> TraceAddBlockEvent blk

-- | The block fits onto some fork, we'll try to switch to that fork (if it
--   is preferable to our chain).
TrySwitchToAFork :: RealPoint blk -> ChainDiff (HeaderFields blk) -> TraceAddBlockEvent blk

-- | The block doesn't fit onto any other block, so we store it and ignore
--   it.
StoreButDontChange :: RealPoint blk -> TraceAddBlockEvent blk

-- | The new block fits onto the current chain (first fragment) and we have
--   successfully used it to extend our (new) current chain (second
--   fragment).
AddedToCurrentChain :: [LedgerEvent blk] -> SelectionChangedInfo blk -> AnchoredFragment (Header blk) -> AnchoredFragment (Header blk) -> TraceAddBlockEvent blk

-- | The new block fits onto some fork and we have switched to that fork
--   (second fragment), as it is preferable to our (previous) current chain
--   (first fragment).
SwitchedToAFork :: [LedgerEvent blk] -> SelectionChangedInfo blk -> AnchoredFragment (Header blk) -> AnchoredFragment (Header blk) -> TraceAddBlockEvent blk

-- | An event traced during validating performed while adding a block.
AddBlockValidation :: TraceValidationEvent blk -> TraceAddBlockEvent blk

-- | Run chain selection for a block that was previously from the future.
--   This is done for all blocks from the future each time a new block is
--   added.
ChainSelectionForFutureBlock :: RealPoint blk -> TraceAddBlockEvent blk

-- | The tentative header (in the context of diffusion pipelining) has been
--   updated.
PipeliningEvent :: TracePipeliningEvent blk -> TraceAddBlockEvent blk

-- | Herald of <a>AddedToCurrentChain</a> or <a>SwitchedToAFork</a>. Lists
--   the tip of the new chain.
ChangingSelection :: Point blk -> TraceAddBlockEvent blk
data TraceCopyToImmutableDBEvent blk

-- | A block was successfully copied to the ImmutableDB.
CopiedBlockToImmutableDB :: Point blk -> TraceCopyToImmutableDBEvent blk

-- | There are no block to copy to the ImmutableDB.
NoBlocksToCopyToImmutableDB :: TraceCopyToImmutableDBEvent blk

-- | Trace type for the various events of the ChainDB.
data TraceEvent blk
TraceAddBlockEvent :: TraceAddBlockEvent blk -> TraceEvent blk
TraceFollowerEvent :: TraceFollowerEvent blk -> TraceEvent blk
TraceCopyToImmutableDBEvent :: TraceCopyToImmutableDBEvent blk -> TraceEvent blk
TraceGCEvent :: TraceGCEvent blk -> TraceEvent blk
TraceInitChainSelEvent :: TraceInitChainSelEvent blk -> TraceEvent blk
TraceOpenEvent :: TraceOpenEvent blk -> TraceEvent blk
TraceIteratorEvent :: TraceIteratorEvent blk -> TraceEvent blk
TraceSnapshotEvent :: TraceSnapshotEvent blk -> TraceEvent blk
TraceLedgerReplayEvent :: TraceReplayEvent blk -> TraceEvent blk
TraceImmutableDBEvent :: TraceEvent blk -> TraceEvent blk
TraceVolatileDBEvent :: TraceEvent blk -> TraceEvent blk
data TraceFollowerEvent blk

-- | A new follower was created.
NewFollower :: TraceFollowerEvent blk

-- | The follower was in the <a>FollowerInMem</a> state but its point is no
--   longer on the in-memory chain fragment, so it has to switch to the
--   <a>FollowerInImmutableDB</a> state.
FollowerNoLongerInMem :: FollowerRollState blk -> TraceFollowerEvent blk

-- | The follower was in the <a>FollowerInImmutableDB</a> state and is
--   switched to the <a>FollowerInMem</a> state.
FollowerSwitchToMem :: Point blk -> WithOrigin SlotNo -> TraceFollowerEvent blk

-- | The follower is in the <a>FollowerInImmutableDB</a> state but the
--   iterator is exhausted while the ImmutableDB has grown, so we open a
--   new iterator to stream these blocks too.
FollowerNewImmIterator :: Point blk -> WithOrigin SlotNo -> TraceFollowerEvent blk
data TraceGCEvent blk

-- | A garbage collection for the given <a>SlotNo</a> was scheduled to
--   happen at the given time.
ScheduledGC :: SlotNo -> Time -> TraceGCEvent blk

-- | A garbage collection for the given <a>SlotNo</a> was performed.
PerformedGC :: SlotNo -> TraceGCEvent blk
data TraceInitChainSelEvent blk

-- | An event traced when inital chain selection has started during the
--   initialization of ChainDB
StartedInitChainSelection :: TraceInitChainSelEvent blk

-- | An event traced when inital chain has been selected
InitalChainSelected :: TraceInitChainSelEvent blk

-- | An event traced during validation performed while performing initial
--   chain selection.
InitChainSelValidation :: TraceValidationEvent blk -> TraceInitChainSelEvent blk
data TraceIteratorEvent blk

-- | An unknown range was requested, see <a>UnknownRange</a>.
UnknownRangeRequested :: UnknownRange blk -> TraceIteratorEvent blk

-- | Stream only from the VolatileDB.
StreamFromVolatileDB :: StreamFrom blk -> StreamTo blk -> [RealPoint blk] -> TraceIteratorEvent blk

-- | Stream only from the ImmutableDB.
StreamFromImmutableDB :: StreamFrom blk -> StreamTo blk -> TraceIteratorEvent blk

-- | Stream from both the VolatileDB and the ImmutableDB.
StreamFromBoth :: StreamFrom blk -> StreamTo blk -> [RealPoint blk] -> TraceIteratorEvent blk

-- | A block is no longer in the VolatileDB because it has been garbage
--   collected. It might now be in the ImmutableDB if it was part of the
--   current chain.
BlockMissingFromVolatileDB :: RealPoint blk -> TraceIteratorEvent blk

-- | A block that has been garbage collected from the VolatileDB is now
--   found and streamed from the ImmutableDB.
BlockWasCopiedToImmutableDB :: RealPoint blk -> TraceIteratorEvent blk

-- | A block is no longer in the VolatileDB and isn't in the ImmutableDB
--   either; it wasn't part of the current chain.
BlockGCedFromVolatileDB :: RealPoint blk -> TraceIteratorEvent blk

-- | We have streamed one or more blocks from the ImmutableDB that were
--   part of the VolatileDB when initialising the iterator. Now, we have to
--   look back in the VolatileDB again because the ImmutableDB doesn't have
--   the next block we're looking for.
SwitchBackToVolatileDB :: TraceIteratorEvent blk
data TraceOpenEvent blk

-- | The ChainDB started the process of opening.
StartedOpeningDB :: TraceOpenEvent blk

-- | The ChainDB was opened.
OpenedDB :: Point blk -> Point blk -> TraceOpenEvent blk

-- | The ChainDB was closed.
ClosedDB :: Point blk -> Point blk -> TraceOpenEvent blk

-- | The ImmutableDB started the process of opening.
StartedOpeningImmutableDB :: TraceOpenEvent blk

-- | The ImmutableDB was opened.
OpenedImmutableDB :: Point blk -> ChunkNo -> TraceOpenEvent blk

-- | The VolatileDB started opening.
StartedOpeningVolatileDB :: TraceOpenEvent blk

-- | The VolatileDB was opened.
OpenedVolatileDB :: TraceOpenEvent blk

-- | The LedgerDB started opening.
StartedOpeningLgrDB :: TraceOpenEvent blk

-- | The LedgerDB was opened.
OpenedLgrDB :: TraceOpenEvent blk
data TracePipeliningEvent blk

-- | A new tentative header got set.
SetTentativeHeader :: Header blk -> Enclosing -> TracePipeliningEvent blk

-- | The body of tentative block turned out to be invalid.
TrapTentativeHeader :: Header blk -> TracePipeliningEvent blk

-- | We selected a new (better) chain, which cleared the previous tentative
--   header.
OutdatedTentativeHeader :: Header blk -> TracePipeliningEvent blk
data TraceValidationEvent blk

-- | A point was found to be invalid.
InvalidBlock :: ExtValidationError blk -> RealPoint blk -> TraceValidationEvent blk

-- | A candidate chain was valid.
ValidCandidate :: AnchoredFragment (Header blk) -> TraceValidationEvent blk

-- | Candidate contains headers from the future which do no exceed the
--   clock skew.
CandidateContainsFutureBlocks :: AnchoredFragment (Header blk) -> [Header blk] -> TraceValidationEvent blk

-- | Candidate contains headers from the future which exceed the clock
--   skew, making them invalid.
CandidateContainsFutureBlocksExceedingClockSkew :: AnchoredFragment (Header blk) -> [Header blk] -> TraceValidationEvent blk
UpdateLedgerDbTraceEvent :: UpdateLedgerDbTraceEvent blk -> TraceValidationEvent blk
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ChainDB.Impl.Types.IteratorKey
instance GHC.Enum.Enum Ouroboros.Consensus.Storage.ChainDB.Impl.Types.IteratorKey
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.ChainDB.Impl.Types.IteratorKey
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ChainDB.Impl.Types.IteratorKey
instance GHC.Show.Show Ouroboros.Consensus.Storage.ChainDB.Impl.Types.IteratorKey
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerKey
instance GHC.Enum.Enum Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerKey
instance GHC.Classes.Ord Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerKey
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerKey
instance GHC.Show.Show Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerKey
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerHandle m blk)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerRollState blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerRollState blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerRollState blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerRollState blk)
instance Ouroboros.Network.Block.StandardHash blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerState m blk b)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.FollowerState m blk b)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.InvalidBlockInfo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.InvalidBlockInfo blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.InvalidBlockInfo blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.InvalidBlockInfo blk)
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.BlocksToAdd m blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceOpenEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceOpenEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceOpenEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.SelectionChangedInfo blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceValidationEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceAddBlockEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceInitChainSelEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceFollowerEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceFollowerEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceFollowerEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceCopyToImmutableDBEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceCopyToImmutableDBEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceCopyToImmutableDBEvent blk)
instance GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceGCEvent blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceGCEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceGCEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceIteratorEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceIteratorEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceIteratorEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceEvent blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.ChainDbEnv m blk)
instance (Ouroboros.Consensus.Util.IOLike.IOLike m, Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.ChainDbState m blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.ChainDbState m blk)
instance (GHC.Classes.Eq (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk, Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceEvent blk)
instance (GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk, Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceEvent blk)
instance (GHC.Show.Show (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)), Ouroboros.Network.Block.StandardHash blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.SelectionChangedInfo blk)
instance (GHC.Classes.Eq (Ouroboros.Consensus.Protocol.Abstract.SelectView (Ouroboros.Consensus.Block.Abstract.BlockProtocol blk)), Ouroboros.Network.Block.StandardHash blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.SelectionChangedInfo blk)
instance (GHC.Classes.Eq (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk, Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceAddBlockEvent blk)
instance (GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk, Ouroboros.Consensus.Ledger.Inspect.InspectLedger blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceAddBlockEvent blk)
instance (GHC.Classes.Eq (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceValidationEvent blk)
instance (GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceValidationEvent blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Block.Abstract.Header blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TracePipeliningEvent blk)
instance GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TracePipeliningEvent blk)
instance (GHC.Classes.Eq (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk) => GHC.Classes.Eq (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceInitChainSelEvent blk)
instance (GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header blk), Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk) => GHC.Show.Show (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.TraceInitChainSelEvent blk)
instance (Ouroboros.Consensus.Util.IOLike.IOLike m, Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Types.ChainDbEnv m blk)


-- | Queries
module Ouroboros.Consensus.Storage.ChainDB.Impl.Query
getBlockComponent :: forall m blk b. IOLike m => ChainDbEnv m blk -> BlockComponent blk b -> RealPoint blk -> m (Maybe b)

-- | Return the last <tt>k</tt> headers.
--   
--   While the in-memory fragment (<a>cdbChain</a>) might temporarily be
--   longer than <tt>k</tt> (until the background thread has copied those
--   blocks to the ImmutableDB), this function will never return a fragment
--   longer than <tt>k</tt>.
--   
--   The anchor point of the returned fragment will be the most recent
--   "immutable" block, i.e. a block that cannot be rolled back. In
--   ChainDB.md, we call this block <tt>i</tt>.
--   
--   Note that the returned fragment may be shorter than <tt>k</tt> in case
--   the whole chain itself is shorter than <tt>k</tt> or in case the
--   VolatileDB was corrupted. In the latter case, we don't take blocks
--   already in the ImmutableDB into account, as we know they <i>must</i>
--   have been "immutable" at some point, and, therefore, <i>must</i> still
--   be "immutable".
getCurrentChain :: forall m blk. (IOLike m, HasHeader (Header blk), ConsensusProtocol (BlockProtocol blk)) => ChainDbEnv m blk -> STM m (AnchoredFragment (Header blk))
getIsFetched :: forall m blk. IOLike m => ChainDbEnv m blk -> STM m (Point blk -> Bool)
getIsInvalidBlock :: forall m blk. (IOLike m, HasHeader blk) => ChainDbEnv m blk -> STM m (WithFingerprint (HeaderHash blk -> Maybe (InvalidBlockReason blk)))
getIsValid :: forall m blk. (IOLike m, HasHeader blk) => ChainDbEnv m blk -> STM m (RealPoint blk -> Maybe Bool)
getLedgerDB :: IOLike m => ChainDbEnv m blk -> STM m (LedgerDB' blk)
getMaxSlotNo :: forall m blk. (IOLike m, HasHeader (Header blk)) => ChainDbEnv m blk -> STM m MaxSlotNo
getTipBlock :: forall m blk. (IOLike m, HasHeader blk, HasHeader (Header blk)) => ChainDbEnv m blk -> m (Maybe blk)
getTipHeader :: forall m blk. (IOLike m, HasHeader blk, HasHeader (Header blk)) => ChainDbEnv m blk -> m (Maybe (Header blk))
getTipPoint :: forall m blk. (IOLike m, HasHeader (Header blk)) => ChainDbEnv m blk -> STM m (Point blk)

-- | Get a block component from either the immutable DB or volatile DB.
--   
--   Returns <a>Nothing</a> if the <a>Point</a> is unknown. Throws
--   <tt>NoGenesisBlockException</tt> if the <a>Point</a> refers to the
--   genesis block.
getAnyBlockComponent :: forall m blk b. IOLike m => ImmutableDB m blk -> VolatileDB m blk -> BlockComponent blk b -> RealPoint blk -> m (Maybe b)

-- | Variant of <a>getAnyBlockComponent</a> instantiated with
--   <a>GetBlock</a>.
getAnyKnownBlock :: forall m blk. (IOLike m, HasHeader blk) => ImmutableDB m blk -> VolatileDB m blk -> RealPoint blk -> m blk

-- | Wrapper around <a>getAnyBlockComponent</a> for blocks we know should
--   exist.
--   
--   If the block does not exist, this indicates disk failure.
getAnyKnownBlockComponent :: forall m blk b. (IOLike m, HasHeader blk) => ImmutableDB m blk -> VolatileDB m blk -> BlockComponent blk b -> RealPoint blk -> m b

module Ouroboros.Consensus.Storage.ChainDB.Impl.Args
data ChainDbArgs f m blk
ChainDbArgs :: SomeHasFS m -> SomeHasFS m -> SomeHasFS m -> ValidationPolicy -> BlockValidationPolicy -> BlocksPerFile -> DiskPolicy -> HKD f (TopLevelConfig blk) -> HKD f ChunkInfo -> HKD f (blk -> Bool) -> HKD f (m (ExtLedgerState blk)) -> HKD f (CheckInFuture m blk) -> CacheConfig -> Tracer m (TraceEvent blk) -> Tracer m (LedgerDB' blk) -> HKD f (ResourceRegistry m) -> DiffTime -> DiffTime -> Word -> ChainDbArgs f m blk
[cdbHasFSImmutableDB] :: ChainDbArgs f m blk -> SomeHasFS m
[cdbHasFSVolatileDB] :: ChainDbArgs f m blk -> SomeHasFS m
[cdbHasFSLgrDB] :: ChainDbArgs f m blk -> SomeHasFS m

-- | Which chunks of the ImmutableDB to validate on opening: all chunks, or
--   only the most recent chunk?
[cdbImmutableDbValidation] :: ChainDbArgs f m blk -> ValidationPolicy

-- | Should the parser for the VolatileDB fail when it encounters a
--   corrupt/invalid block?
[cdbVolatileDbValidation] :: ChainDbArgs f m blk -> BlockValidationPolicy
[cdbMaxBlocksPerFile] :: ChainDbArgs f m blk -> BlocksPerFile
[cdbDiskPolicy] :: ChainDbArgs f m blk -> DiskPolicy
[cdbTopLevelConfig] :: ChainDbArgs f m blk -> HKD f (TopLevelConfig blk)
[cdbChunkInfo] :: ChainDbArgs f m blk -> HKD f ChunkInfo

-- | Predicate to check for integrity of <a>GetVerifiedBlock</a> components
--   when extracting them from both the VolatileDB and the ImmutableDB.
[cdbCheckIntegrity] :: ChainDbArgs f m blk -> HKD f (blk -> Bool)
[cdbGenesis] :: ChainDbArgs f m blk -> HKD f (m (ExtLedgerState blk))
[cdbCheckInFuture] :: ChainDbArgs f m blk -> HKD f (CheckInFuture m blk)
[cdbImmutableDbCacheConfig] :: ChainDbArgs f m blk -> CacheConfig
[cdbTracer] :: ChainDbArgs f m blk -> Tracer m (TraceEvent blk)
[cdbTraceLedger] :: ChainDbArgs f m blk -> Tracer m (LedgerDB' blk)
[cdbRegistry] :: ChainDbArgs f m blk -> HKD f (ResourceRegistry m)
[cdbGcDelay] :: ChainDbArgs f m blk -> DiffTime
[cdbGcInterval] :: ChainDbArgs f m blk -> DiffTime

-- | Size of the queue used to store asynchronously added blocks. This is
--   the maximum number of blocks that could be kept in memory at the same
--   time when the background thread processing the blocks can't keep up.
[cdbBlocksToAddSize] :: ChainDbArgs f m blk -> Word

-- | Arguments specific to the ChainDB, not to the ImmutableDB, VolatileDB,
--   or LedgerDB.
data ChainDbSpecificArgs f m blk
ChainDbSpecificArgs :: Word -> HKD f (CheckInFuture m blk) -> DiffTime -> DiffTime -> HKD f (ResourceRegistry m) -> Tracer m (TraceEvent blk) -> ChainDbSpecificArgs f m blk
[cdbsBlocksToAddSize] :: ChainDbSpecificArgs f m blk -> Word
[cdbsCheckInFuture] :: ChainDbSpecificArgs f m blk -> HKD f (CheckInFuture m blk)

-- | Delay between copying a block to the ImmutableDB and triggering a
--   garbage collection for the corresponding slot on the VolatileDB.
--   
--   The goal of the delay is to ensure that the write to the ImmutableDB
--   has been flushed to disk before deleting the block from the
--   VolatileDB, so that a crash won't result in the loss of the block.
[cdbsGcDelay] :: ChainDbSpecificArgs f m blk -> DiffTime

-- | Batch all scheduled GCs so that at most one GC happens every
--   <a>cdbsGcInterval</a>.
[cdbsGcInterval] :: ChainDbSpecificArgs f m blk -> DiffTime
[cdbsRegistry] :: ChainDbSpecificArgs f m blk -> HKD f (ResourceRegistry m)
[cdbsTracer] :: ChainDbSpecificArgs f m blk -> Tracer m (TraceEvent blk)

-- | A relative path for a <a>MountPoint</a>
--   
--   The root is determined by context.
newtype RelativeMountPoint
RelativeMountPoint :: FilePath -> RelativeMountPoint

-- | Default arguments
--   
--   See <a>defaultArgs</a>, <a>defaultArgs</a>, <a>defaultArgs</a>, and
--   <a>defaultSpecificArgs</a> for a list of which fields are not given a
--   default and must therefore be set explicitly.
defaultArgs :: forall m blk. Monad m => (RelativeMountPoint -> SomeHasFS m) -> DiskPolicy -> ChainDbArgs Defaults m blk

-- | Internal: split <a>ChainDbArgs</a> into <tt>ImmutableDbArgs</tt>,
--   'VolatileDbArgs, <tt>LgrDbArgs</tt>, and <a>ChainDbSpecificArgs</a>.
fromChainDbArgs :: forall m blk f. MapHKD f => ChainDbArgs f m blk -> (ImmutableDbArgs f m blk, VolatileDbArgs f m blk, LgrDbArgs f m blk, ChainDbSpecificArgs f m blk)


-- | Iterators
module Ouroboros.Consensus.Storage.ChainDB.Impl.Iterator

-- | Close all open <a>Iterator</a>s.
--   
--   This <i>can</i> be called when the ChainDB is already closed.
closeAllIterators :: IOLike m => ChainDbEnv m blk -> m ()

-- | Stream blocks
--   
--   <h1>Start &amp; end point</h1>
--   
--   The start point can either be in the ImmutableDB (on our chain) or in
--   the VolatileDB (on our chain or on a recent fork). We first check
--   whether it is in the VolatileDB, if not, we check if it is in the
--   ImmutableDB (see "Garbage collection" for why this order is
--   important). Similarly for the end point.
--   
--   If a bound can't be found in the ChainDB, an <a>UnknownRange</a> error
--   is returned.
--   
--   When the bounds are nonsensical, e.g., &gt; StreamFromExclusive (Point
--   (SlotNo 3) _) &gt; StreamToInclusive (RealPoint (SlotNo 3) _) An
--   <a>InvalidIteratorRange</a> exception is thrown.
--   
--   <h1>Paths of blocks</h1>
--   
--   To stream blocks from the ImmutableDB we can simply use the iterators
--   offered by the ImmutableDB.
--   
--   To stream blocks from the VolatileDB we have to construct a path of
--   points backwards through the VolatileDB, starting from the end point
--   using <tt>getPredecessor</tt> until we get to the start point,
--   genesis, or we get to a block that is not in the VolatileDB. Then, for
--   each point in the path, we can ask the VolatileDB for the
--   corresponding block.
--   
--   If the path through the VolatileDB is incomplete, we will first have
--   to stream blocks from the ImmutableDB and then switch to the path
--   through the VolatileDB. We only allow the tip of the ImmutableDB to be
--   the switchover point between the two DBs. In other words, the
--   incomplete path through the VolatileDB must fit onto the tip of the
--   ImmutableDB. This must be true at the time of initialising the
--   iterator, but does not have to hold during the whole lifetime of the
--   iterator. If it doesn't fit on it, it means the path forked off more
--   than <tt>k</tt> blocks in the past and blocks belonging to it are more
--   likely to go missing because of garbage-collection (see the next
--   paragraph). In that case, we return <a>ForkTooOld</a>.
--   
--   <h1>Garbage collection</h1>
--   
--   We have to be careful about the following: as our chain grows, blocks
--   from our chain will be copied to the ImmutableDB in the background.
--   After a while, old blocks will be garbage-collected from the
--   VolatileDB. Blocks that were part of the current chain will be in the
--   ImmutableDB, but blocks that only lived on forks will be gone forever.
--   
--   This means that blocks that were part of the VolatileDB when the
--   iterator was initialised might no longer be part of the VolatileDB
--   when we come to the point that the iterator will try to read them.
--   When this is noticed, we will try to open an iterator from the
--   ImmutableDB to obtain the blocks that have moved over. However, this
--   will only work if they were and are part of the current chain,
--   otherwise they will have been deleted from the VolatileDB without
--   being copied to the ImmutableDB.
--   
--   This iterator is opened with an open upper bound and will be used to
--   stream blocks until the path has been fully streamed, the iterator is
--   exhausted, or a block doesn't match the expected point. In the latter
--   two cases, we switch back to the VolatileDB. If the block is missing
--   from the VolatileDB, we will switch back to streaming from the
--   ImmutableDB. If that fails, we switch back to the VolatileDB. To avoid
--   eternally switching between the two DBs, we only switch back to the
--   VolatileDB if the stream from the ImmutableDB has made progress, i.e.
--   streamed at least one block with the expected point. If no block was
--   streamed from the ImmutableDB, not even the first one, we know for
--   sure that that block isn't part of the VolatileDB (the reason we
--   switch to the ImmutableDB) and isn't part of the ImmutableDB (no block
--   was streamed). In that case, we return <a>IteratorBlockGCed</a> and
--   stop the stream.
--   
--   Note that the open upper bound doesn't allow us to include blocks in
--   the stream that are copied to the ImmutableDB after opening this
--   iterator, as the bound of the iterator is fixed upon initialisation.
--   These newly added blocks will be included in the stream because we
--   will repeatedly open new ImmutableDB iterators (as long as we make
--   progress).
--   
--   <h1>Bounds checking</h1>
--   
--   The VolatileDB is hash-based instead of point-based. While the bounds
--   of a stream are <i>point</i>s, we can simply check whether the hashes
--   of the bounds match the hashes stored in the points.
--   
--   The ImmutableDB is slot-based instead of point-based, which means that
--   before we know whether a block in the ImmutableDB matches a given
--   point, we must first read the block's hash corresponding to the
--   point's slot from the (cached) on-disk indices, after which we can
--   then verify whether it matches the hash of the point. This is
--   important for the start and end bounds (both points) of a stream in
--   case they are in the ImmutableDB (i.e., their slots are &lt;= the tip
--   of the ImmutableDB): we must first read the hashes corresponding to
--   the bounds from the (cached) on-disk indices to be sure the range is
--   valid. Note that these reads happen before the first call to
--   <a>iteratorNext</a>.
--   
--   Note that when streaming to an <i>exclusive</i> bound, the block
--   corresponding to that bound (<a>Point</a>) must exist in the ChainDB.
--   
--   The ImmutableDB will keep the on-disk indices of a chunk of blocks in
--   memory after the first read so that the next lookup doesn't have to
--   read from disk. When both bounds are in the same chunk, which will
--   typically be the case, only checking the first bound will require disk
--   reads, the second will be cached.
--   
--   <h1>Costs</h1>
--   
--   Opening an iterator has some costs:
--   
--   <ul>
--   <li>When blocks have to be streamed from the ImmutableDB: as discussed
--   in "Bounds checking", the hashes corresponding to the bounds have to
--   be read from the (cached) on-disk indices.</li>
--   <li>When blocks have to be streamed both from the ImmutableDB and the
--   VolatileDB, only the hash of the block corresponding to the lower
--   bound will have to be read from the ImmutableDB upfront, as described
--   in the previous bullet point. Note that the hash of the block
--   corresponding to the upper bound does not have to be read from disk,
--   since it will be in the VolatileDB, which means that we know its hash
--   already from the in-memory index.</li>
--   </ul>
--   
--   In summary:
--   
--   <ul>
--   <li>Only streaming from the VolatileDB: 0 (cached) reads from disk
--   upfront.</li>
--   <li>Only streaming from the ImmutableDB: 2 (cached) reads from disk
--   upfront.</li>
--   <li>Streaming from both the ImmutableDB and the VolatileDB: 1 (cached)
--   read from disk upfront.</li>
--   </ul>
--   
--   Additionally, when we notice during streaming that a block is no
--   longer in the VolatileDB, we try to see whether it can be streamed
--   from the ImmutableDB instead. Opening such an iterator costs 2
--   (cached) reads from disk upfront. This can happen multiple times.
stream :: forall m blk b. (IOLike m, HasHeader blk, HasCallStack) => ChainDbHandle m blk -> ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (UnknownRange blk) (Iterator m blk b))

-- | Environment containing everything needed to implement iterators.
--   
--   The main purpose of bundling these things in a separate record is to
--   make it easier to test this code: no need to set up a whole ChainDB,
--   just provide this record.
data IteratorEnv m blk
IteratorEnv :: ImmutableDB m blk -> VolatileDB m blk -> StrictTVar m (Map IteratorKey (m ())) -> StrictTVar m IteratorKey -> Tracer m (TraceIteratorEvent blk) -> IteratorEnv m blk
[itImmutableDB] :: IteratorEnv m blk -> ImmutableDB m blk
[itVolatileDB] :: IteratorEnv m blk -> VolatileDB m blk
[itIterators] :: IteratorEnv m blk -> StrictTVar m (Map IteratorKey (m ()))
[itNextIteratorKey] :: IteratorEnv m blk -> StrictTVar m IteratorKey
[itTracer] :: IteratorEnv m blk -> Tracer m (TraceIteratorEvent blk)

-- | See <a>stream</a>.
newIterator :: forall m blk b. (IOLike m, HasHeader blk, HasCallStack) => IteratorEnv m blk -> (forall r. (IteratorEnv m blk -> m r) -> m r) -> ResourceRegistry m -> BlockComponent blk b -> StreamFrom blk -> StreamTo blk -> m (Either (UnknownRange blk) (Iterator m blk b))
instance (Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Iterator.InImmutableDBEnd blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Iterator.InImmutableDBEnd blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Storage.ChainDB.Impl.Iterator.IteratorState m blk b)
instance (Data.Typeable.Internal.Typeable blk, Ouroboros.Network.Block.StandardHash blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Storage.ChainDB.Impl.Iterator.IteratorState m blk b)


-- | Followers
module Ouroboros.Consensus.Storage.ChainDB.Impl.Follower

-- | Close all open block and header <a>Follower</a>s.
closeAllFollowers :: IOLike m => ChainDbEnv m blk -> m ()
newFollower :: forall m blk b. (IOLike m, HasHeader blk, GetHeader blk, HasNestedContent Header blk, EncodeDiskDep (NestedCtxt Header) blk) => ChainDbHandle m blk -> ResourceRegistry m -> ChainType -> BlockComponent blk b -> m (Follower m blk b)

-- | Switches the follower to the new fork, by checking whether the
--   follower is following an old fork, and updating the follower state to
--   rollback to the intersection point if it is.
switchFork :: forall m blk b. HasHeader blk => Point blk -> Set (Point blk) -> FollowerState m blk b -> FollowerState m blk b


-- | Operations involving chain selection: the initial chain selection and
--   adding a block.
module Ouroboros.Consensus.Storage.ChainDB.Impl.ChainSel

-- | Add a block to the ChainDB, <i>asynchronously</i>.
--   
--   This adds a <a>BlockToAdd</a> corresponding to the given block to the
--   <a>cdbBlocksToAdd</a> queue. The entries in that queue are processed
--   using <a>addBlockSync</a>, see that function for more information.
--   
--   When the queue is full, this function will still block.
--   
--   An important advantage of this asynchronous approach over a
--   synchronous approach is that it doesn't have the following
--   disadvantage: when a thread adding a block to the ChainDB is killed,
--   which can happen when disconnecting from the corresponding node, we
--   might have written the block to disk, but not updated the
--   corresponding in-memory state (e.g., that of the VolatileDB), leaving
--   both out of sync.
--   
--   With this asynchronous approach, threads adding blocks asynchronously
--   can be killed without worries, the background thread processing the
--   blocks synchronously won't be killed. Only when the whole ChainDB
--   shuts down will that background thread get killed. But since there
--   will be no more in-memory state, it can't get out of sync with the
--   file system state. On the next startup, a correct in-memory state will
--   be reconstructed from the file system state.
addBlockAsync :: forall m blk. (IOLike m, HasHeader blk) => ChainDbEnv m blk -> InvalidBlockPunishment m -> blk -> m (AddBlockPromise m blk)

-- | Add a block to the ChainDB, <i>synchronously</i>.
--   
--   This is the only operation that actually changes the ChainDB. It will
--   store the block on disk and trigger chain selection, possibly
--   switching to a fork.
--   
--   When the slot of the block is &gt; the current slot, a chain selection
--   will be scheduled in the slot of the block.
addBlockSync :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, HasCallStack) => ChainDbEnv m blk -> BlockToAdd m blk -> m ()

-- | Trigger chain selection for the given block.
--   
--   PRECONDITION: the block is in the VolatileDB.
--   
--   PRECONDITION: the slot of the block &lt;= the current (wall) slot
--   
--   The new tip of the current chain is returned.
--   
--   <h1>Constructing candidate fragments</h1>
--   
--   The VolatileDB keeps a "successors" map in memory, telling us the
--   hashes of the known successors of any block, but it does not keep
--   <i>headers</i> in memory, which are needed to construct candidate
--   fargments. We try to reuse the headers from the current chain fragment
--   where possible, but it will not contain all needed headers. This means
--   that we will need to read some blocks from disk and extract their
--   headers. Under normal circumstances this does not matter too much;
--   although this will be done every time we add a block, the expected
--   number of headers to read from disk is very small:
--   
--   <ul>
--   <li>None if we stay on the current chain and this is just the next
--   block</li>
--   <li>A handful if we stay on the current chain and the block we just
--   received was a missing block and we already received some of its
--   successors</li>
--   <li>A handful if we switch to a short fork</li>
--   </ul>
--   
--   This is expensive only
--   
--   <ul>
--   <li>on startup: in this case we need to read at least <tt>k</tt>
--   blocks from the VolatileDB, and possibly more if there are some other
--   chains in the VolatileDB starting from the tip of the ImmutableDB</li>
--   <li>when we switch to a distant fork</li>
--   </ul>
--   
--   This cost is currently deemed acceptable.
chainSelectionForBlock :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, HasCallStack) => ChainDbEnv m blk -> BlockCache blk -> Header blk -> InvalidBlockPunishment m -> m (Point blk)

-- | Perform the initial chain selection based on the tip of the
--   ImmutableDB and the contents of the VolatileDB.
--   
--   Returns the chosen validated chain and corresponding ledger.
--   
--   See "## Initialization" in ChainDB.md.
initialChainSelection :: forall m blk. (IOLike m, LedgerSupportsProtocol blk) => ImmutableDB m blk -> VolatileDB m blk -> LgrDB m blk -> Tracer m (TraceInitChainSelEvent blk) -> TopLevelConfig blk -> StrictTVar m (WithFingerprint (InvalidBlocks blk)) -> StrictTVar m (FutureBlocks m blk) -> CheckInFuture m blk -> m (ChainAndLedger blk)

-- | Return <a>True</a> when the given header should be ignored when adding
--   it because it is too old, i.e., we wouldn't be able to switch to a
--   chain containing the corresponding block because its block number is
--   more than <tt>k</tt> blocks or exactly <tt>k</tt> blocks back.
--   
--   Special case: the header corresponds to an EBB which has the same
--   block number as the block <tt>k</tt> blocks back (the most recent
--   "immutable" block). As EBBs share their block number with the block
--   before them, the EBB is not too old in that case and can be adopted as
--   part of our chain.
--   
--   This special case can occur, for example, when the VolatileDB is empty
--   (because of corruption). The "immutable" block is then also the tip of
--   the chain. If we then try to add the EBB after it, it will have the
--   same block number, so we must allow it.
olderThanK :: HasHeader (Header blk) => Header blk -> IsEBB -> WithOrigin BlockNo -> Bool
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ChainDB.Impl.ChainSel.ChainSwitchType
instance GHC.Show.Show Ouroboros.Consensus.Storage.ChainDB.Impl.ChainSel.ChainSwitchType


-- | Background tasks:
--   
--   <ul>
--   <li>Copying blocks from the VolatileDB to the ImmutableDB</li>
--   <li>Performing and scheduling garbage collections on the
--   VolatileDB</li>
--   <li>Writing snapshots of the LedgerDB to disk and deleting old
--   ones</li>
--   <li>Executing scheduled chain selections</li>
--   </ul>
module Ouroboros.Consensus.Storage.ChainDB.Impl.Background
launchBgTasks :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, LgrDbSerialiseConstraints blk) => ChainDbEnv m blk -> Word64 -> m ()

-- | Copy blocks from the VolatileDB to ImmutableDB and take snapshots of
--   the LgrDB
--   
--   We watch the chain for changes. Whenever the chain is longer than
--   <tt>k</tt>, then the headers older than <tt>k</tt> are copied from the
--   VolatileDB to the ImmutableDB (using <a>copyToImmutableDB</a>). Once
--   that is complete,
--   
--   <ul>
--   <li>We periodically take a snapshot of the LgrDB (depending on its
--   config). When enough blocks (depending on its config) have been
--   replayed during startup, a snapshot of the replayed LgrDB will be
--   written to disk at the start of this function. NOTE: After this
--   initial snapshot we do not take a snapshot of the LgrDB until the
--   chain has changed again, irrespective of the LgrDB policy.</li>
--   <li>Schedule GC of the VolatileDB (<a>scheduleGC</a>) for the
--   <a>SlotNo</a> of the most recent block that was copied.</li>
--   </ul>
--   
--   It is important that we only take LgrDB snapshots when are are
--   <i>sure</i> they have been copied to the ImmutableDB, since the LgrDB
--   assumes that all snapshots correspond to immutable blocks. (Of course,
--   data corruption can occur and we can handle it by reverting to an
--   older LgrDB snapshot, but we should need this only in exceptional
--   circumstances.)
--   
--   We do not store any state of the VolatileDB GC. If the node shuts down
--   before GC can happen, when we restart the node and schedule the
--   <i>next</i> GC, it will <i>imply</i> any previously scheduled GC,
--   since GC is driven by slot number ("garbage collect anything older
--   than <tt>x</tt>").
copyAndSnapshotRunner :: forall m blk. (IOLike m, ConsensusProtocol (BlockProtocol blk), HasHeader blk, GetHeader blk, IsLedger (LedgerState blk), LgrDbSerialiseConstraints blk) => ChainDbEnv m blk -> GcSchedule m -> Word64 -> m Void

-- | Copy the blocks older than <tt>k</tt> from the VolatileDB to the
--   ImmutableDB.
--   
--   These headers of these blocks can be retrieved by dropping the
--   <tt>k</tt> most recent blocks from the fragment stored in
--   <a>cdbChain</a>.
--   
--   The copied blocks are removed from the fragment stored in
--   <a>cdbChain</a>.
--   
--   This function does not remove blocks from the VolatileDB.
--   
--   The <a>SlotNo</a> of the tip of the ImmutableDB after copying the
--   blocks is returned. This can be used for a garbage collection on the
--   VolatileDB.
--   
--   NOTE: this function would not be safe when called multiple times
--   concurrently. To enforce thread-safety, a lock is obtained at the
--   start of this function and released at the end. So in practice, this
--   function can be called multiple times concurrently, but the calls will
--   be serialised.
--   
--   NOTE: this function <i>can</i> run concurrently with all other
--   functions, just not with itself.
copyToImmutableDB :: forall m blk. (IOLike m, ConsensusProtocol (BlockProtocol blk), HasHeader blk, GetHeader blk, HasCallStack) => ChainDbEnv m blk -> m (WithOrigin SlotNo)

-- | Write a snapshot of the LedgerDB to disk and remove old snapshots
--   (typically one) so that only <tt>onDiskNumSnapshots</tt> snapshots are
--   on disk.
updateLedgerSnapshots :: (IOLike m, LgrDbSerialiseConstraints blk, HasHeader blk, IsLedger (LedgerState blk)) => ChainDbEnv m blk -> m ()

-- | Trigger a garbage collection for blocks older than the given
--   <a>SlotNo</a> on the VolatileDB.
--   
--   Also removes the corresponding cached "previously applied points" from
--   the LedgerDB.
--   
--   This is thread-safe as the VolatileDB locks itself while performing a
--   GC.
--   
--   When calling this function it is <b>critical</b> that the blocks that
--   will be garbage collected, which are determined by the <tt>slotNo</tt>
--   parameter, have already been copied to the immutable DB (if they are
--   part of the current selection).
--   
--   TODO will a long GC be a bottleneck? It will block any other calls to
--   <tt>putBlock</tt> and <tt>getBlock</tt>.
garbageCollect :: forall m blk. IOLike m => ChainDbEnv m blk -> SlotNo -> m ()
data GcParams
GcParams :: !DiffTime -> !DiffTime -> GcParams

-- | How long to wait until performing the GC. See <tt>cdbsGcDelay</tt>.
[gcDelay] :: GcParams -> !DiffTime

-- | The GC interval: the minimum time between two GCs. See
--   <tt>cdbsGcInterval</tt>.
[gcInterval] :: GcParams -> !DiffTime

-- | Scheduled garbage collections
--   
--   When a block has been copied to the ImmutableDB, we schedule a
--   VolatileDB garbage collection for the slot corresponding to the block
--   in the future. How far in the future is determined by the
--   <a>gcDelay</a> parameter. The goal is to allow some overlap so that
--   the write to the ImmutableDB will have been flushed to disk before the
--   block is removed from the VolatileDB.
--   
--   We store scheduled garbage collections in a LIFO queue. Since the
--   queue will be very short (see further down for why) and entries are
--   more often added (at the block sync speed by a single thread) than
--   removed (once every <a>gcInterval</a>), we simply use a
--   <a>StrictSeq</a> stored in a <tt>TVar</tt> to make reasoning and
--   testing easier. Entries are enqueued at the end (right) and dequeued
--   from the head (left).
--   
--   The <a>Time</a>s in the queue will be monotonically increasing. A
--   fictional example (with hh:mm:ss):
--   
--   <pre>
--   [(16:01:12, SlotNo 1012), (16:04:38, SlotNo 1045), ..]
--   </pre>
--   
--   Scheduling a garbage collection with <a>scheduleGC</a> will add an
--   entry to the end of the queue for the given slot at the time equal to
--   now (<a>getMonotonicTime</a>) + the <tt>gcDelay</tt> rounded to
--   <tt>gcInterval</tt>. Unless the last entry in the queue was scheduled
--   for the same rounded time, in that case the new entry replaces the
--   existing entry. The goal of this is to batch garbage collections so
--   that, when possible, at most one garbage collection happens every
--   <tt>gcInterval</tt>.
--   
--   For example, starting with an empty queue and <tt>gcDelay = 5min</tt>
--   and <tt>gcInterval = 10s</tt>:
--   
--   At 8:43:22, we schedule a GC for slot 10:
--   
--   <pre>
--   [(8:48:30, SlotNo 10)]
--   </pre>
--   
--   The scheduled time is rounded up to the next interval. Next, at
--   8:43:24, we schedule a GC for slot 11:
--   
--   <pre>
--   [(8:48:30, SlotNo 11)]
--   </pre>
--   
--   Note that the existing entry is replaced with the new one, as they map
--   to the same <tt>gcInterval</tt>. Instead of two GCs 2 seconds apart,
--   we will only schedule one GC.
--   
--   Next, at 8:44:02, we schedule a GC for slot 12:
--   
--   <pre>
--   [(8:48:30, SlotNo 11), (8:49:10, SlotNo 12)]
--   </pre>
--   
--   Now, a new entry was appended to the queue, as it doesn't map to the
--   same <tt>gcInterval</tt> as the last one.
--   
--   In other words, everything scheduled in the first 10s will be done
--   after 20s. The bounds are the open-closed interval:
--   
--   <pre>
--   (now + gcDelay, now + gcDelay + gcInterval]
--   </pre>
--   
--   Whether we're syncing at high speed or downloading blocks as they are
--   produced, the length of the queue will be at most <tt>⌈gcDelay /
--   gcInterval⌉ + 1</tt>, e.g., 5min / 10s = 31 entries. The <tt>+ 1</tt>
--   is needed because we might be somewhere in the middle of a
--   <tt>gcInterval</tt>.
--   
--   The background thread will look at head of the queue and wait until
--   that has <a>Time</a> passed. After the wait, it will pop off the head
--   of the queue and perform a garbage collection for the <a>SlotNo</a> in
--   the head. Note that the <a>SlotNo</a> before the wait can be different
--   from the one after the wait, precisely because of batching.
data GcSchedule m
computeTimeForGC :: GcParams -> Time -> Time
gcScheduleRunner :: forall m. IOLike m => GcSchedule m -> (SlotNo -> m ()) -> m Void
newGcSchedule :: IOLike m => m (GcSchedule m)
scheduleGC :: forall m blk. IOLike m => Tracer m (TraceGCEvent blk) -> SlotNo -> GcParams -> GcSchedule m -> m ()
data ScheduledGc
ScheduledGc :: !Time -> !SlotNo -> ScheduledGc

-- | Time at which to run the garbage collection
[scheduledGcTime] :: ScheduledGc -> !Time

-- | For which slot to run the garbage collection
[scheduledGcSlot] :: ScheduledGc -> !SlotNo

-- | Return the current contents of the <a>GcSchedule</a> queue without
--   modifying it.
--   
--   For testing purposes.
dumpGcSchedule :: IOLike m => GcSchedule m -> STM m [ScheduledGc]

-- | Read blocks from <a>cdbBlocksToAdd</a> and add them synchronously to
--   the ChainDB.
addBlockRunner :: (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, HasCallStack) => ChainDbEnv m blk -> m Void
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Storage.ChainDB.Impl.Background.ScheduledGc
instance GHC.Generics.Generic Ouroboros.Consensus.Storage.ChainDB.Impl.Background.ScheduledGc
instance GHC.Show.Show Ouroboros.Consensus.Storage.ChainDB.Impl.Background.ScheduledGc
instance GHC.Classes.Eq Ouroboros.Consensus.Storage.ChainDB.Impl.Background.ScheduledGc
instance GHC.Show.Show Ouroboros.Consensus.Storage.ChainDB.Impl.Background.GcParams
instance Ouroboros.Consensus.Util.Condense.Condense Ouroboros.Consensus.Storage.ChainDB.Impl.Background.ScheduledGc

module Ouroboros.Consensus.Storage.ChainDB.Impl
data ChainDbArgs f m blk
ChainDbArgs :: SomeHasFS m -> SomeHasFS m -> SomeHasFS m -> ValidationPolicy -> BlockValidationPolicy -> BlocksPerFile -> DiskPolicy -> HKD f (TopLevelConfig blk) -> HKD f ChunkInfo -> HKD f (blk -> Bool) -> HKD f (m (ExtLedgerState blk)) -> HKD f (CheckInFuture m blk) -> CacheConfig -> Tracer m (TraceEvent blk) -> Tracer m (LedgerDB' blk) -> HKD f (ResourceRegistry m) -> DiffTime -> DiffTime -> Word -> ChainDbArgs f m blk
[cdbHasFSImmutableDB] :: ChainDbArgs f m blk -> SomeHasFS m
[cdbHasFSVolatileDB] :: ChainDbArgs f m blk -> SomeHasFS m
[cdbHasFSLgrDB] :: ChainDbArgs f m blk -> SomeHasFS m

-- | Which chunks of the ImmutableDB to validate on opening: all chunks, or
--   only the most recent chunk?
[cdbImmutableDbValidation] :: ChainDbArgs f m blk -> ValidationPolicy

-- | Should the parser for the VolatileDB fail when it encounters a
--   corrupt/invalid block?
[cdbVolatileDbValidation] :: ChainDbArgs f m blk -> BlockValidationPolicy
[cdbMaxBlocksPerFile] :: ChainDbArgs f m blk -> BlocksPerFile
[cdbDiskPolicy] :: ChainDbArgs f m blk -> DiskPolicy
[cdbTopLevelConfig] :: ChainDbArgs f m blk -> HKD f (TopLevelConfig blk)
[cdbChunkInfo] :: ChainDbArgs f m blk -> HKD f ChunkInfo

-- | Predicate to check for integrity of <a>GetVerifiedBlock</a> components
--   when extracting them from both the VolatileDB and the ImmutableDB.
[cdbCheckIntegrity] :: ChainDbArgs f m blk -> HKD f (blk -> Bool)
[cdbGenesis] :: ChainDbArgs f m blk -> HKD f (m (ExtLedgerState blk))
[cdbCheckInFuture] :: ChainDbArgs f m blk -> HKD f (CheckInFuture m blk)
[cdbImmutableDbCacheConfig] :: ChainDbArgs f m blk -> CacheConfig
[cdbTracer] :: ChainDbArgs f m blk -> Tracer m (TraceEvent blk)
[cdbTraceLedger] :: ChainDbArgs f m blk -> Tracer m (LedgerDB' blk)
[cdbRegistry] :: ChainDbArgs f m blk -> HKD f (ResourceRegistry m)
[cdbGcDelay] :: ChainDbArgs f m blk -> DiffTime
[cdbGcInterval] :: ChainDbArgs f m blk -> DiffTime

-- | Size of the queue used to store asynchronously added blocks. This is
--   the maximum number of blocks that could be kept in memory at the same
--   time when the background thread processing the blocks can't keep up.
[cdbBlocksToAddSize] :: ChainDbArgs f m blk -> Word

-- | All the serialisation related constraints needed by the ChainDB.
class (ImmutableDbSerialiseConstraints blk, LgrDbSerialiseConstraints blk, VolatileDbSerialiseConstraints blk, EncodeDiskDep (NestedCtxt Header) blk) => SerialiseDiskConstraints blk

-- | Default arguments
--   
--   See <a>defaultArgs</a>, <a>defaultArgs</a>, <a>defaultArgs</a>, and
--   <a>defaultSpecificArgs</a> for a list of which fields are not given a
--   default and must therefore be set explicitly.
defaultArgs :: forall m blk. Monad m => (RelativeMountPoint -> SomeHasFS m) -> DiskPolicy -> ChainDbArgs Defaults m blk
openDB :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, ConvertRawHash blk, SerialiseDiskConstraints blk) => ChainDbArgs Identity m blk -> m (ChainDB m blk)
withDB :: forall m blk a. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, ConvertRawHash blk, SerialiseDiskConstraints blk) => ChainDbArgs Identity m blk -> (ChainDB m blk -> m a) -> m a

-- | Events traced while replaying blocks against the ledger to bring it up
--   to date w.r.t. the tip of the ImmutableDB during initialisation. As
--   this process takes a while, we trace events to inform higher layers of
--   our progress.
data TraceReplayEvent blk

-- | Information on having changed our selection to a chain with a
--   (necessarily) new tip.
--   
--   NOTE: the fields of this record are intentionally lazy to prevent the
--   forcing of this information in case it doesn't have to be traced.
--   However, this means that the tracer processing this message <i>must
--   not</i> hold on to it, otherwise it leaks memory.
data SelectionChangedInfo blk
SelectionChangedInfo :: RealPoint blk -> EpochNo -> Word64 -> RealPoint blk -> SelectView (BlockProtocol blk) -> Maybe (SelectView (BlockProtocol blk)) -> SelectionChangedInfo blk

-- | The new tip of the current chain.
[newTipPoint] :: SelectionChangedInfo blk -> RealPoint blk

-- | The epoch of the new tip.
[newTipEpoch] :: SelectionChangedInfo blk -> EpochNo

-- | The slot in the epoch, i.e., the relative slot number, of the new tip.
[newTipSlotInEpoch] :: SelectionChangedInfo blk -> Word64

-- | The new tip of the current chain (<a>newTipPoint</a>) is the result of
--   performing chain selection for a <i>trigger</i> block
--   (<a>newTipTrigger</a>). In most cases, we add a new block to the tip
--   of the current chain, in which case the new tip <i>is</i> the trigger
--   block.
--   
--   However, this is not always the case. For example, with our current
--   chain being A and having a disconnected C lying around, adding B will
--   result in A -&gt; B -&gt; C as the new chain. The trigger B /= the new
--   tip C.
[newTipTrigger] :: SelectionChangedInfo blk -> RealPoint blk

-- | The <a>SelectView</a> of the new tip. It is guaranteed that
--   
--   <pre>
--   &gt;&gt;&gt; Just newTipSelectView &gt; oldTipSelectView
--   True
--   </pre>
[newTipSelectView] :: SelectionChangedInfo blk -> SelectView (BlockProtocol blk)

-- | The <a>SelectView</a> of the old, previous tip. This can be
--   <a>Nothing</a> when the previous chain/tip was Genesis.
[oldTipSelectView] :: SelectionChangedInfo blk -> Maybe (SelectView (BlockProtocol blk))

-- | Trace type for the various events that occur when adding a block.
data TraceAddBlockEvent blk

-- | A block with a <a>BlockNo</a> more than <tt>k</tt> back than the
--   current tip was ignored.
IgnoreBlockOlderThanK :: RealPoint blk -> TraceAddBlockEvent blk

-- | A block that is already in the Volatile DB was ignored.
IgnoreBlockAlreadyInVolatileDB :: RealPoint blk -> TraceAddBlockEvent blk

-- | A block that is know to be invalid was ignored.
IgnoreInvalidBlock :: RealPoint blk -> InvalidBlockReason blk -> TraceAddBlockEvent blk

-- | The block was added to the queue and will be added to the ChainDB by
--   the background thread. The size of the queue is included.
AddedBlockToQueue :: RealPoint blk -> Enclosing' Word -> TraceAddBlockEvent blk

-- | The block popped from the queue and will imminently be added to the
--   ChainDB.
PoppedBlockFromQueue :: Enclosing' (RealPoint blk) -> TraceAddBlockEvent blk

-- | The block is from the future, i.e., its slot number is greater than
--   the current slot (the second argument).
BlockInTheFuture :: RealPoint blk -> SlotNo -> TraceAddBlockEvent blk

-- | A block was added to the Volatile DB
AddedBlockToVolatileDB :: RealPoint blk -> BlockNo -> IsEBB -> Enclosing -> TraceAddBlockEvent blk

-- | The block fits onto the current chain, we'll try to use it to extend
--   our chain.
TryAddToCurrentChain :: RealPoint blk -> TraceAddBlockEvent blk

-- | The block fits onto some fork, we'll try to switch to that fork (if it
--   is preferable to our chain).
TrySwitchToAFork :: RealPoint blk -> ChainDiff (HeaderFields blk) -> TraceAddBlockEvent blk

-- | The block doesn't fit onto any other block, so we store it and ignore
--   it.
StoreButDontChange :: RealPoint blk -> TraceAddBlockEvent blk

-- | The new block fits onto the current chain (first fragment) and we have
--   successfully used it to extend our (new) current chain (second
--   fragment).
AddedToCurrentChain :: [LedgerEvent blk] -> SelectionChangedInfo blk -> AnchoredFragment (Header blk) -> AnchoredFragment (Header blk) -> TraceAddBlockEvent blk

-- | The new block fits onto some fork and we have switched to that fork
--   (second fragment), as it is preferable to our (previous) current chain
--   (first fragment).
SwitchedToAFork :: [LedgerEvent blk] -> SelectionChangedInfo blk -> AnchoredFragment (Header blk) -> AnchoredFragment (Header blk) -> TraceAddBlockEvent blk

-- | An event traced during validating performed while adding a block.
AddBlockValidation :: TraceValidationEvent blk -> TraceAddBlockEvent blk

-- | Run chain selection for a block that was previously from the future.
--   This is done for all blocks from the future each time a new block is
--   added.
ChainSelectionForFutureBlock :: RealPoint blk -> TraceAddBlockEvent blk

-- | The tentative header (in the context of diffusion pipelining) has been
--   updated.
PipeliningEvent :: TracePipeliningEvent blk -> TraceAddBlockEvent blk

-- | Herald of <a>AddedToCurrentChain</a> or <a>SwitchedToAFork</a>. Lists
--   the tip of the new chain.
ChangingSelection :: Point blk -> TraceAddBlockEvent blk
data TraceCopyToImmutableDBEvent blk

-- | A block was successfully copied to the ImmutableDB.
CopiedBlockToImmutableDB :: Point blk -> TraceCopyToImmutableDBEvent blk

-- | There are no block to copy to the ImmutableDB.
NoBlocksToCopyToImmutableDB :: TraceCopyToImmutableDBEvent blk

-- | Trace type for the various events of the ChainDB.
data TraceEvent blk
TraceAddBlockEvent :: TraceAddBlockEvent blk -> TraceEvent blk
TraceFollowerEvent :: TraceFollowerEvent blk -> TraceEvent blk
TraceCopyToImmutableDBEvent :: TraceCopyToImmutableDBEvent blk -> TraceEvent blk
TraceGCEvent :: TraceGCEvent blk -> TraceEvent blk
TraceInitChainSelEvent :: TraceInitChainSelEvent blk -> TraceEvent blk
TraceOpenEvent :: TraceOpenEvent blk -> TraceEvent blk
TraceIteratorEvent :: TraceIteratorEvent blk -> TraceEvent blk
TraceSnapshotEvent :: TraceSnapshotEvent blk -> TraceEvent blk
TraceLedgerReplayEvent :: TraceReplayEvent blk -> TraceEvent blk
TraceImmutableDBEvent :: TraceEvent blk -> TraceEvent blk
TraceVolatileDBEvent :: TraceEvent blk -> TraceEvent blk
data TraceFollowerEvent blk

-- | A new follower was created.
NewFollower :: TraceFollowerEvent blk

-- | The follower was in the <a>FollowerInMem</a> state but its point is no
--   longer on the in-memory chain fragment, so it has to switch to the
--   <a>FollowerInImmutableDB</a> state.
FollowerNoLongerInMem :: FollowerRollState blk -> TraceFollowerEvent blk

-- | The follower was in the <a>FollowerInImmutableDB</a> state and is
--   switched to the <a>FollowerInMem</a> state.
FollowerSwitchToMem :: Point blk -> WithOrigin SlotNo -> TraceFollowerEvent blk

-- | The follower is in the <a>FollowerInImmutableDB</a> state but the
--   iterator is exhausted while the ImmutableDB has grown, so we open a
--   new iterator to stream these blocks too.
FollowerNewImmIterator :: Point blk -> WithOrigin SlotNo -> TraceFollowerEvent blk
data TraceGCEvent blk

-- | A garbage collection for the given <a>SlotNo</a> was scheduled to
--   happen at the given time.
ScheduledGC :: SlotNo -> Time -> TraceGCEvent blk

-- | A garbage collection for the given <a>SlotNo</a> was performed.
PerformedGC :: SlotNo -> TraceGCEvent blk
data TraceInitChainSelEvent blk

-- | An event traced when inital chain selection has started during the
--   initialization of ChainDB
StartedInitChainSelection :: TraceInitChainSelEvent blk

-- | An event traced when inital chain has been selected
InitalChainSelected :: TraceInitChainSelEvent blk

-- | An event traced during validation performed while performing initial
--   chain selection.
InitChainSelValidation :: TraceValidationEvent blk -> TraceInitChainSelEvent blk
data TraceIteratorEvent blk

-- | An unknown range was requested, see <a>UnknownRange</a>.
UnknownRangeRequested :: UnknownRange blk -> TraceIteratorEvent blk

-- | Stream only from the VolatileDB.
StreamFromVolatileDB :: StreamFrom blk -> StreamTo blk -> [RealPoint blk] -> TraceIteratorEvent blk

-- | Stream only from the ImmutableDB.
StreamFromImmutableDB :: StreamFrom blk -> StreamTo blk -> TraceIteratorEvent blk

-- | Stream from both the VolatileDB and the ImmutableDB.
StreamFromBoth :: StreamFrom blk -> StreamTo blk -> [RealPoint blk] -> TraceIteratorEvent blk

-- | A block is no longer in the VolatileDB because it has been garbage
--   collected. It might now be in the ImmutableDB if it was part of the
--   current chain.
BlockMissingFromVolatileDB :: RealPoint blk -> TraceIteratorEvent blk

-- | A block that has been garbage collected from the VolatileDB is now
--   found and streamed from the ImmutableDB.
BlockWasCopiedToImmutableDB :: RealPoint blk -> TraceIteratorEvent blk

-- | A block is no longer in the VolatileDB and isn't in the ImmutableDB
--   either; it wasn't part of the current chain.
BlockGCedFromVolatileDB :: RealPoint blk -> TraceIteratorEvent blk

-- | We have streamed one or more blocks from the ImmutableDB that were
--   part of the VolatileDB when initialising the iterator. Now, we have to
--   look back in the VolatileDB again because the ImmutableDB doesn't have
--   the next block we're looking for.
SwitchBackToVolatileDB :: TraceIteratorEvent blk
data TraceOpenEvent blk

-- | The ChainDB started the process of opening.
StartedOpeningDB :: TraceOpenEvent blk

-- | The ChainDB was opened.
OpenedDB :: Point blk -> Point blk -> TraceOpenEvent blk

-- | The ChainDB was closed.
ClosedDB :: Point blk -> Point blk -> TraceOpenEvent blk

-- | The ImmutableDB started the process of opening.
StartedOpeningImmutableDB :: TraceOpenEvent blk

-- | The ImmutableDB was opened.
OpenedImmutableDB :: Point blk -> ChunkNo -> TraceOpenEvent blk

-- | The VolatileDB started opening.
StartedOpeningVolatileDB :: TraceOpenEvent blk

-- | The VolatileDB was opened.
OpenedVolatileDB :: TraceOpenEvent blk

-- | The LedgerDB started opening.
StartedOpeningLgrDB :: TraceOpenEvent blk

-- | The LedgerDB was opened.
OpenedLgrDB :: TraceOpenEvent blk
data TracePipeliningEvent blk

-- | A new tentative header got set.
SetTentativeHeader :: Header blk -> Enclosing -> TracePipeliningEvent blk

-- | The body of tentative block turned out to be invalid.
TrapTentativeHeader :: Header blk -> TracePipeliningEvent blk

-- | We selected a new (better) chain, which cleared the previous tentative
--   header.
OutdatedTentativeHeader :: Header blk -> TracePipeliningEvent blk
data TraceValidationEvent blk

-- | A point was found to be invalid.
InvalidBlock :: ExtValidationError blk -> RealPoint blk -> TraceValidationEvent blk

-- | A candidate chain was valid.
ValidCandidate :: AnchoredFragment (Header blk) -> TraceValidationEvent blk

-- | Candidate contains headers from the future which do no exceed the
--   clock skew.
CandidateContainsFutureBlocks :: AnchoredFragment (Header blk) -> [Header blk] -> TraceValidationEvent blk

-- | Candidate contains headers from the future which exceed the clock
--   skew, making them invalid.
CandidateContainsFutureBlocksExceedingClockSkew :: AnchoredFragment (Header blk) -> [Header blk] -> TraceValidationEvent blk
UpdateLedgerDbTraceEvent :: UpdateLedgerDbTraceEvent blk -> TraceValidationEvent blk

-- | A relative path for a <a>MountPoint</a>
--   
--   The root is determined by context.
newtype RelativeMountPoint
RelativeMountPoint :: FilePath -> RelativeMountPoint

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   ImmutableDB.
type ImmutableDbSerialiseConstraints blk = (EncodeDisk blk blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, ReconstructNestedCtxt Header blk, HasBinaryBlockInfo blk)

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   LgrDB.
type LgrDbSerialiseConstraints blk = (Serialise (HeaderHash blk), EncodeDisk blk (LedgerState blk), DecodeDisk blk (LedgerState blk), EncodeDisk blk (AnnTip blk), DecodeDisk blk (AnnTip blk), EncodeDisk blk (ChainDepState (BlockProtocol blk)), DecodeDisk blk (ChainDepState (BlockProtocol blk)))

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   VolatileDB.
type VolatileDbSerialiseConstraints blk = (EncodeDisk blk blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, HasNestedContent Header blk, HasBinaryBlockInfo blk)
data Internal m blk
Internal :: m (WithOrigin SlotNo) -> (SlotNo -> m ()) -> m () -> m Void -> StrictTVar m (m ()) -> Internal m blk

-- | Copy the blocks older than <tt>k</tt> from to the VolatileDB to the
--   ImmutableDB and update the in-memory chain fragment correspondingly.
--   
--   The <a>SlotNo</a> of the tip of the ImmutableDB after copying the
--   blocks is returned. This can be used for a garbage collection on the
--   VolatileDB.
[intCopyToImmutableDB] :: Internal m blk -> m (WithOrigin SlotNo)

-- | Perform garbage collection for blocks &lt;= the given <a>SlotNo</a>.
[intGarbageCollect] :: Internal m blk -> SlotNo -> m ()

-- | Write a new LedgerDB snapshot to disk and remove the oldest one(s).
[intUpdateLedgerSnapshots] :: Internal m blk -> m ()

-- | Start the loop that adds blocks to the ChainDB retrieved from the
--   queue populated by <a>addBlock</a>. Execute this loop in a separate
--   thread.
[intAddBlockRunner] :: Internal m blk -> m Void

-- | A handle to kill the background threads.
[intKillBgThreads] :: Internal m blk -> StrictTVar m (m ())
openDBInternal :: forall m blk. (IOLike m, LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, ConvertRawHash blk, SerialiseDiskConstraints blk) => ChainDbArgs Identity m blk -> Bool -> m (ChainDB m blk, Internal m blk)


-- | The storage layer is a highly specialized database for storing the
--   blockchain. It consists of five subcomponents:
--   
--   <ul>
--   <li>An abstract file system API, <a>HasFS</a>, that smooths out over
--   some differences between the file systems of different operating
--   systems and, more importantly, allows us to simulate all kinds of
--   failures. This is then used for stress-testing the other components
--   below.</li>
--   <li>The <b><a>Immutable DB</a></b>, stores the part of the chain that
--   is immutable, that is, no longer subject to rollback. It is an
--   append-only database, providing efficient access to the chain.
--   <a>ImmutableDB</a> defines the immutable DB API.</li>
--   <li>The <b><a>Volatile DB</a></b>, stores the part of the chain near
--   its tip. This doesn't really store a <b>chain</b> as such, but rather
--   simply a collection of blocks from which we might <b>construct</b> a
--   chain. <a>VolatileDB</a> defines the volatile DB API.</li>
--   <li>The ledger DB, stores the state of the ledger. The <b><a>on
--   disk</a></b> part only stores snapshots of the ledger state that
--   correspond to immutable blocks. The <b><a>in memory</a></b> part
--   stores various snapshots of the ledger state corresponding to blocks
--   near the current tip of the chain, and provides an efficient way of
--   computing any ledger state for the last <tt>k</tt> blocks of the
--   chain.</li>
--   <li>The Chain DB finally combines all of these components. It makes
--   decisions about which chains to adopt (chain selection), switches to
--   forks when needed, deals with clock skew, and provides various
--   interfaces to the rest of the consensus layer for things like finding
--   out which blocks were invalid (so we can disconnect from the clients
--   who sent them), cursors that follow the tip of the chain (so that we
--   can inform our downstream peers of how our chain evolves), etc. In
--   many ways, the chain DB is the component that is responsible for
--   "consensus": deciding which chain is the one true chain.
--   <a>ChainDB</a> defines the chain DB API.</li>
--   </ul>
module Ouroboros.Consensus.Storage.ChainDB


-- | Infrastructure required to run a node
--   
--   The definitions in this module are independent from any specific
--   protocol.
module Ouroboros.Consensus.Node.Run

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   ImmutableDB.
type ImmutableDbSerialiseConstraints blk = (EncodeDisk blk blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, ReconstructNestedCtxt Header blk, HasBinaryBlockInfo blk)

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   LgrDB.
type LgrDbSerialiseConstraints blk = (Serialise (HeaderHash blk), EncodeDisk blk (LedgerState blk), DecodeDisk blk (LedgerState blk), EncodeDisk blk (AnnTip blk), DecodeDisk blk (AnnTip blk), EncodeDisk blk (ChainDepState (BlockProtocol blk)), DecodeDisk blk (ChainDepState (BlockProtocol blk)))

-- | All the serialisation related constraints needed by the ChainDB.
class (ImmutableDbSerialiseConstraints blk, LgrDbSerialiseConstraints blk, VolatileDbSerialiseConstraints blk, EncodeDiskDep (NestedCtxt Header) blk) => SerialiseDiskConstraints blk

-- | <a>EncodeDisk</a> and <a>DecodeDisk</a> constraints needed for the
--   VolatileDB.
type VolatileDbSerialiseConstraints blk = (EncodeDisk blk blk, DecodeDisk blk (ByteString -> blk), DecodeDiskDep (NestedCtxt Header) blk, HasNestedContent Header blk, HasBinaryBlockInfo blk)

-- | Serialisation constraints needed by the node-to-node protocols
class (ConvertRawHash blk, SerialiseNodeToNode blk blk, SerialiseNodeToNode blk (Header blk), SerialiseNodeToNode blk (Serialised blk), SerialiseNodeToNode blk (SerialisedHeader blk), SerialiseNodeToNode blk (GenTx blk), SerialiseNodeToNode blk (GenTxId blk)) => SerialiseNodeToNodeConstraints blk

-- | An upper bound on the size in bytes of the block corresponding to the
--   header. This can be an overestimate, but not an underestimate.
--   
--   The block fetch client uses this to estimate how bytes will be in
--   flight. This is also used to limit the number of bytes accepted when
--   downloading a block.
--   
--   This is part of this class as it depends on the node-to-node
--   serialisation format used for blocks.
estimateBlockSize :: SerialiseNodeToNodeConstraints blk => Header blk -> SizeInBytes

-- | Serialisation constraints needed by the node-to-client protocols
class (Typeable blk, ConvertRawHash blk, SerialiseNodeToClient blk blk, SerialiseNodeToClient blk (Serialised blk), SerialiseNodeToClient blk (GenTx blk), SerialiseNodeToClient blk (GenTxId blk), SerialiseNodeToClient blk SlotNo, SerialiseNodeToClient blk (ApplyTxErr blk), SerialiseNodeToClient blk (SomeSecond BlockQuery blk), SerialiseResult blk (BlockQuery blk)) => SerialiseNodeToClientConstraints blk
class (LedgerSupportsProtocol blk, InspectLedger blk, HasHardForkHistory blk, LedgerSupportsMempool blk, HasTxId (GenTx blk), QueryLedger blk, SupportedNetworkProtocolVersion blk, ConfigSupportsNode blk, ConvertRawHash blk, CommonProtocolParams blk, HasBinaryBlockInfo blk, SerialiseDiskConstraints blk, SerialiseNodeToNodeConstraints blk, SerialiseNodeToClientConstraints blk, LedgerSupportsPeerSelection blk, NodeInitStorage blk, BlockSupportsMetrics blk, Show (CannotForge blk), Show (ForgeStateInfo blk), Show (ForgeStateUpdateError blk), ShowProxy blk, ShowProxy (ApplyTxErr blk), ShowProxy (GenTx blk), ShowProxy (Header blk), ShowProxy (BlockQuery blk), ShowProxy (TxId (GenTx blk))) => RunNode blk

module Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common

-- | Exception thrown in the HFC encoders
data HardForkEncoderException

-- | HFC disabled, but we saw a value from an era other than the first
[HardForkEncoderFutureEra] :: SingleEraInfo blk -> HardForkEncoderException

-- | HFC enabled, but we saw a value from a disabled era
--   
--   This is only thrown by the Node-to-Client codec. Two nodes' negotiated
--   version does not constrain how the <i>distributed</i> chain will
--   evolve, so the Node-to-Node communication does not need this. The
--   <a>MaxMajorProtVer</a> check will enforce it appropriately and incur
--   explanatory log messages on the node that needs to be updated in order
--   to handle the latest hard fork.
--   
--   See <a>HardForkNodeToClientEnabled</a> for the use case.
[HardForkEncoderDisabledEra] :: SingleEraInfo blk -> HardForkEncoderException

-- | HFC disabled, but we saw a query that is only supported by the HFC
[HardForkEncoderQueryHfcDisabled] :: HardForkEncoderException

-- | HFC enabled, but we saw a HFC query that is not supported by the
--   HFC-specific version used
[HardForkEncoderQueryWrongVersion] :: HardForkEncoderException
class (SingleEraBlock blk, SerialiseDiskConstraints blk, SerialiseNodeToNodeConstraints blk, SerialiseNodeToClientConstraints blk, HasNetworkProtocolVersion blk) => SerialiseConstraintsHFC blk

-- | Conditions required by the HFC to provide serialisation
--   
--   NOTE: Compatibility between HFC enabled and disabled:
--   
--   <ol>
--   <li>Node-to-node and node-to-client communication is versioned. When
--   the HFC is disabled, we default to the instances for the first era,
--   and so compatibility is preserved by construction.</li>
--   <li>On-disk storage is <i>not</i> versioned, and here we make no
--   attempt to be compatible between non-HFC and HFC deployments,
--   <i>except</i> for blocks: we define two methods
--   <a>encodeDiskHfcBlock</a> and <a>decodeDiskHfcBlock</a> which are used
--   for on-disk serialisation of blocks. These methods have defaults which
--   can and probably should be used for deployments that use the HFC from
--   the get-go, but for deployments that only later change to use the HFC
--   these functions can be overriden to provide an on-disk storage format
--   for HFC blocks that is compatible with the on-disk storage of blocks
--   from the first era.</li>
--   <li>The converse is NOT supported. Deployments that use the HFC from
--   the start should not use <a>HardForkNodeToNodeDisabled</a> and/or
--   <a>HardForkNodeToClientDisabled</a>. Doing so would result in opposite
--   compatibility problems: the on-disk block would include the HFC tag,
--   but sending blocks with the HFC disabled suggests that that tag is
--   unexpected. This would then lead to problems with binary streaming,
--   and we do not currently provide any provisions to resolve these.</li>
--   </ol>
class (CanHardFork xs, All SerialiseConstraintsHFC xs, All (Compose Show EraNodeToClientVersion) xs, All (Compose Eq EraNodeToClientVersion) xs, All (Compose Show WrapNodeToNodeVersion) xs, All (Compose Eq WrapNodeToNodeVersion) xs, All (EncodeDiskDepIx (NestedCtxt Header)) xs, All (DecodeDiskDepIx (NestedCtxt Header)) xs, All HasBinaryBlockInfo xs) => SerialiseHFC xs
encodeDiskHfcBlock :: SerialiseHFC xs => CodecConfig (HardForkBlock xs) -> HardForkBlock xs -> Encoding
decodeDiskHfcBlock :: SerialiseHFC xs => CodecConfig (HardForkBlock xs) -> forall s. Decoder s (ByteString -> HardForkBlock xs)

-- | Used as the implementation of <a>reconstructPrefixLen</a> for
--   <a>HardForkBlock</a>.
reconstructHfcPrefixLen :: SerialiseHFC xs => proxy (Header (HardForkBlock xs)) -> PrefixLen

-- | Used as the implementation of <a>reconstructNestedCtxt</a> for
--   <a>HardForkBlock</a>.
reconstructHfcNestedCtxt :: SerialiseHFC xs => proxy (Header (HardForkBlock xs)) -> ShortByteString -> SizeInBytes -> SomeSecond (NestedCtxt Header) (HardForkBlock xs)

-- | Used as the implementation of <a>getBinaryBlockInfo</a> for
--   <a>HardForkBlock</a>.
getHfcBinaryBlockInfo :: SerialiseHFC xs => HardForkBlock xs -> BinaryBlockInfo

-- | Used as the implementation of <a>estimateBlockSize</a> for
--   <a>HardForkBlock</a>.
estimateHfcBlockSize :: SerialiseHFC xs => Header (HardForkBlock xs) -> SizeInBytes
disabledEraException :: forall blk. SingleEraBlock blk => Proxy blk -> HardForkEncoderException
futureEraException :: SListI xs => NS SingleEraInfo xs -> HardForkEncoderException
pSHFC :: Proxy SerialiseConstraintsHFC
type family FirstEra (xs :: [Type])
type family LaterEra (xs :: [Type])
isFirstEra :: forall f xs. All SingleEraBlock xs => NS f xs -> Either (NS SingleEraInfo (LaterEra xs)) (f (FirstEra xs))

-- | Used to construct <tt>FutureEraException</tt>
notFirstEra :: All SingleEraBlock xs => NS f xs -> NS SingleEraInfo xs
data EraNodeToClientVersion blk
EraNodeToClientEnabled :: !BlockNodeToClientVersion blk -> EraNodeToClientVersion blk
EraNodeToClientDisabled :: EraNodeToClientVersion blk
data HardForkNodeToClientVersion xs

-- | Disable the HFC
--   
--   See <a>HardForkNodeToNodeDisabled</a>
[HardForkNodeToClientDisabled] :: BlockNodeToClientVersion x -> HardForkNodeToClientVersion (x ': xs)

-- | Enable the HFC
--   
--   See <a>HardForkNodeToNodeEnabled</a>
[HardForkNodeToClientEnabled] :: HardForkSpecificNodeToClientVersion -> NP EraNodeToClientVersion xs -> HardForkNodeToClientVersion xs
data HardForkNodeToNodeVersion xs

-- | Disable the HFC
--   
--   This means that only the first era (<tt>x</tt>) is supported, and
--   moreover, is compatible with serialisation used if the HFC would not
--   be present at all.
[HardForkNodeToNodeDisabled] :: BlockNodeToNodeVersion x -> HardForkNodeToNodeVersion (x ': xs)

-- | Enable the HFC
--   
--   Serialised values will always include tags inserted by the HFC to
--   distinguish one era from another. We version the hard-fork specific
--   parts with <a>HardForkSpecificNodeToNodeVersion</a>.
[HardForkNodeToNodeEnabled] :: HardForkSpecificNodeToNodeVersion -> NP WrapNodeToNodeVersion xs -> HardForkNodeToNodeVersion xs

-- | Versioning of the specific additions made by the HFC to the
--   <tt>NodeToClient</tt> protocols, e.g., the era tag or the hard-fork
--   specific queries.
data HardForkSpecificNodeToClientVersion
HardForkSpecificNodeToClientVersion1 :: HardForkSpecificNodeToClientVersion

-- | Enable the <a>GetCurrentEra</a> query in <a>QueryHardFork</a>.
HardForkSpecificNodeToClientVersion2 :: HardForkSpecificNodeToClientVersion

-- | Versioning of the specific additions made by the HFC to the
--   <tt>NodeToNode</tt> protocols, e.g., the era tag.
data HardForkSpecificNodeToNodeVersion
HardForkSpecificNodeToNodeVersion1 :: HardForkSpecificNodeToNodeVersion
isHardForkNodeToClientEnabled :: HardForkNodeToClientVersion xs -> Bool
isHardForkNodeToNodeEnabled :: HardForkNodeToNodeVersion xs -> Bool
data AnnDecoder f blk
AnnDecoder :: (forall s. Decoder s (ByteString -> f blk)) -> AnnDecoder f blk
[annDecoder] :: AnnDecoder f blk -> forall s. Decoder s (ByteString -> f blk)
decodeTelescope :: NP (Decoder s :.: f) xs -> Decoder s (HardForkState f xs)
encodeTelescope :: SListI xs => NP (f -.-> K Encoding) xs -> HardForkState f xs -> Encoding
decodeAnnNS :: SListI xs => NP (AnnDecoder f) xs -> forall s. Decoder s (ByteString -> NS f xs)
decodeNS :: SListI xs => NP (Decoder s :.: f) xs -> Decoder s (NS f xs)
encodeNS :: SListI xs => NP (f -.-> K Encoding) xs -> NS f xs -> Encoding
decodeNested :: All (DecodeDiskDep (NestedCtxt f)) xs => CodecConfig (HardForkBlock xs) -> NestedCtxt f (HardForkBlock xs) a -> forall s. Decoder s (ByteString -> a)
decodeNestedCtxt :: All (DecodeDiskDepIx (NestedCtxt f)) xs => CodecConfig (HardForkBlock xs) -> forall s. Decoder s (SomeSecond (NestedCtxt f) (HardForkBlock xs))
encodeNested :: All (EncodeDiskDep (NestedCtxt f)) xs => CodecConfig (HardForkBlock xs) -> NestedCtxt f (HardForkBlock xs) a -> a -> Encoding
encodeNestedCtxt :: All (EncodeDiskDepIx (NestedCtxt f)) xs => CodecConfig (HardForkBlock xs) -> SomeSecond (NestedCtxt f) (HardForkBlock xs) -> Encoding
decodeEitherMismatch :: SListI xs => BlockNodeToClientVersion (HardForkBlock xs) -> Decoder s a -> Decoder s (Either (MismatchEraInfo xs) a)
encodeEitherMismatch :: forall xs a. SListI xs => BlockNodeToClientVersion (HardForkBlock xs) -> (a -> Encoding) -> Either (MismatchEraInfo xs) a -> Encoding
distribAnnTip :: SListI xs => AnnTip (HardForkBlock xs) -> NS AnnTip xs
distribQueryIfCurrent :: Some (QueryIfCurrent xs) -> NS (SomeSecond BlockQuery) xs
distribSerialisedHeader :: SerialisedHeader (HardForkBlock xs) -> NS SerialisedHeader xs
undistribAnnTip :: SListI xs => NS AnnTip xs -> AnnTip (HardForkBlock xs)
undistribQueryIfCurrent :: NS (SomeSecond BlockQuery) xs -> Some (QueryIfCurrent xs)
undistribSerialisedHeader :: NS SerialisedHeader xs -> SerialisedHeader (HardForkBlock xs)

-- | Used for deriving via
--   
--   Example
--   
--   <pre>
--   deriving via SerialiseNS Header SomeEras
--            instance Serialise (Header SomeSecond)
--   </pre>
newtype SerialiseNS f xs
SerialiseNS :: NS f xs -> SerialiseNS f xs
[getSerialiseNS] :: SerialiseNS f xs -> NS f xs
instance GHC.Enum.Bounded Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToNodeVersion
instance GHC.Enum.Enum Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToNodeVersion
instance GHC.Show.Show Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToNodeVersion
instance GHC.Classes.Ord Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToNodeVersion
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToNodeVersion
instance GHC.Enum.Bounded Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToClientVersion
instance GHC.Enum.Enum Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToClientVersion
instance GHC.Show.Show Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToClientVersion
instance GHC.Classes.Ord Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToClientVersion
instance GHC.Classes.Eq Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkSpecificNodeToClientVersion
instance GHC.Show.Show (Ouroboros.Consensus.Node.NetworkProtocolVersion.BlockNodeToClientVersion blk) => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.EraNodeToClientVersion blk)
instance GHC.Classes.Eq (Ouroboros.Consensus.Node.NetworkProtocolVersion.BlockNodeToClientVersion blk) => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.EraNodeToClientVersion blk)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkNodeToNodeVersion xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => GHC.Show.Show (Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkNodeToClientVersion xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkNodeToNodeVersion xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => GHC.Classes.Eq (Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkNodeToClientVersion xs)
instance GHC.Show.Show Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkEncoderException
instance Data.SOP.Constraint.All (Data.SOP.Constraint.Compose Codec.Serialise.Class.Serialise f) xs => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseNS f xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.NetworkProtocolVersion.HasNetworkProtocolVersion (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance GHC.Exception.Type.Exception Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.HardForkEncoderException


-- | The ChainSync client logic
--   
--   Its core specification is found in "The Shelley Networking Protocol",
--   currently found at
--   <a>https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-spec/network-spec.pdf</a>.
--   
--   It would be difficult to maintain or extrend this module without
--   understanding the <tt>typed-protocols</tt> architecture; eg see
--   <a>https://github.com/input-output-hk/typed-protocols</a>.
--   
--   This module is intended for qualified import, aliased as either CSC,
--   CSClient, or CsClient.
module Ouroboros.Consensus.MiniProtocol.ChainSync.Client
bracketChainSyncClient :: (IOLike m, Ord peer, LedgerSupportsProtocol blk) => Tracer m (TraceChainSyncClientEvent blk) -> ChainDbView m blk -> StrictTVar m (Map peer (StrictTVar m (AnchoredFragment (Header blk)))) -> peer -> NodeToNodeVersion -> (StrictTVar m (AnchoredFragment (Header blk)) -> m a) -> m a

-- | Chain sync client
--   
--   This never terminates. In case of a failure, a
--   <a>ChainSyncClientException</a> is thrown. The network layer
--   classifies exception such that the corresponding peer will never be
--   chosen again.
chainSyncClient :: forall m blk. (IOLike m, LedgerSupportsProtocol blk) => ConfigEnv m blk -> DynamicEnv m blk -> Consensus ChainSyncClientPipelined blk m

-- | Abstract over the ChainDB
data ChainDbView m blk
ChainDbView :: STM m (AnchoredFragment (Header blk)) -> STM m (HeaderStateHistory blk) -> (Point blk -> STM m (Maybe (ExtLedgerState blk))) -> STM m (WithFingerprint (HeaderHash blk -> Maybe (InvalidBlockReason blk))) -> ChainDbView m blk
[$sel:getCurrentChain:ChainDbView] :: ChainDbView m blk -> STM m (AnchoredFragment (Header blk))
[$sel:getHeaderStateHistory:ChainDbView] :: ChainDbView m blk -> STM m (HeaderStateHistory blk)
[$sel:getPastLedger:ChainDbView] :: ChainDbView m blk -> Point blk -> STM m (Maybe (ExtLedgerState blk))
[$sel:getIsInvalidBlock:ChainDbView] :: ChainDbView m blk -> STM m (WithFingerprint (HeaderHash blk -> Maybe (InvalidBlockReason blk)))

-- | Arguments determined by configuration
--   
--   These are available before the diffusion layer is online.
data ConfigEnv m blk
ConfigEnv :: MkPipelineDecision -> Tracer m (TraceChainSyncClientEvent blk) -> TopLevelConfig blk -> SomeHeaderInFutureCheck m blk -> ChainDbView m blk -> ConfigEnv m blk

-- | The pipelining decider to use after <tt>MsgFoundIntersect</tt> arrives
[$sel:mkPipelineDecision0:ConfigEnv] :: ConfigEnv m blk -> MkPipelineDecision
[$sel:tracer:ConfigEnv] :: ConfigEnv m blk -> Tracer m (TraceChainSyncClientEvent blk)
[$sel:cfg:ConfigEnv] :: ConfigEnv m blk -> TopLevelConfig blk
[$sel:someHeaderInFutureCheck:ConfigEnv] :: ConfigEnv m blk -> SomeHeaderInFutureCheck m blk
[$sel:chainDbView:ConfigEnv] :: ConfigEnv m blk -> ChainDbView m blk

-- | Arguments determined dynamically
--   
--   These are available only after the diffusion layer is online and/or on
--   per client basis.
data DynamicEnv m blk
DynamicEnv :: NodeToNodeVersion -> ControlMessageSTM m -> HeaderMetricsTracer m -> StrictTVar m (AnchoredFragment (Header blk)) -> DynamicEnv m blk
[$sel:version:DynamicEnv] :: DynamicEnv m blk -> NodeToNodeVersion
[$sel:controlMessageSTM:DynamicEnv] :: DynamicEnv m blk -> ControlMessageSTM m
[$sel:headerMetricsTracer:DynamicEnv] :: DynamicEnv m blk -> HeaderMetricsTracer m
[$sel:varCandidate:DynamicEnv] :: DynamicEnv m blk -> StrictTVar m (AnchoredFragment (Header blk))

-- | General values collectively needed by the top-level entry points
data InternalEnv m blk arrival judgment
InternalEnv :: (forall s n. NoThunks s => Nat n -> Stateful m blk s (ClientPipelinedStIdle 'Z) -> Stateful m blk s (ClientPipelinedStIdle n)) -> (forall m' a. MonadThrow m' => ChainSyncClientException -> m' a) -> HeaderInFutureCheck m blk arrival judgment -> (KnownIntersectionState blk -> STM m (UpdatedIntersectionState blk ())) -> (ChainSyncClientResult -> m (Consensus (ClientPipelinedStIdle 'Z) blk m)) -> (forall n. Nat n -> ChainSyncClientResult -> m (Consensus (ClientPipelinedStIdle n) blk m)) -> (forall a. m a -> m a) -> InternalEnv m blk arrival judgment

-- | "Drain the pipe": collect and discard all in-flight responses and
--   finally execute the given action.
[$sel:drainThePipe:InternalEnv] :: InternalEnv m blk arrival judgment -> forall s n. NoThunks s => Nat n -> Stateful m blk s (ClientPipelinedStIdle 'Z) -> Stateful m blk s (ClientPipelinedStIdle n)

-- | Disconnect from the upstream node by throwing the given exception. The
--   cleanup is handled in <a>bracketChainSyncClient</a>.
[$sel:disconnect:InternalEnv] :: InternalEnv m blk arrival judgment -> forall m' a. MonadThrow m' => ChainSyncClientException -> m' a
[$sel:headerInFutureCheck:InternalEnv] :: InternalEnv m blk arrival judgment -> HeaderInFutureCheck m blk arrival judgment

-- | A combinator necessary whenever relying on a
--   <a>KnownIntersectionState</a>, since it's always possible that that
--   intersection will go stale.
--   
--   Look at the current chain fragment that may have been updated in the
--   background. Check whether the candidate fragment still intersects with
--   it. If so, update the <a>KnownIntersectionState</a> and trim the
--   candidate fragment to the new current chain fragment's anchor point.
--   If not, return <a>Nothing</a>.
--   
--   INVARIANT: This a read-only STM transaction.
[$sel:intersectsWithCurrentChain:InternalEnv] :: InternalEnv m blk arrival judgment -> KnownIntersectionState blk -> STM m (UpdatedIntersectionState blk ())

-- | Gracefully terminate the connection with the upstream node with the
--   given result.
[$sel:terminate:InternalEnv] :: InternalEnv m blk arrival judgment -> ChainSyncClientResult -> m (Consensus (ClientPipelinedStIdle 'Z) blk m)

-- | Same as <a>$sel:terminate:InternalEnv</a>, but first
--   <a>$sel:drainThePipe:InternalEnv</a>.
[$sel:terminateAfterDrain:InternalEnv] :: InternalEnv m blk arrival judgment -> forall n. Nat n -> ChainSyncClientResult -> m (Consensus (ClientPipelinedStIdle n) blk m)

-- | Trace any <a>ChainSyncClientException</a> if thrown.
[$sel:traceException:InternalEnv] :: InternalEnv m blk arrival judgment -> forall a. m a -> m a
defaultChainDbView :: (IOLike m, LedgerSupportsProtocol blk) => ChainDB m blk -> ChainDbView m blk

-- | When the upstream node violates the protocol or exhibits malicious
--   behaviour, e.g., serving an invalid header or a header corresponding
--   to a known invalid block, we throw an exception to disconnect. This
--   will bring down all miniprotocols in both directions with that node.
data ChainSyncClientException
HeaderError :: Point blk -> HeaderError blk -> Our (Tip blk) -> Their (Tip blk) -> ChainSyncClientException
InvalidIntersection :: Point blk -> Our (Tip blk) -> Their (Tip blk) -> ChainSyncClientException
InvalidBlock :: Point blk -> HeaderHash blk -> InvalidBlockReason blk -> ChainSyncClientException

-- | A header arrived from the far future.
InFutureHeaderExceedsClockSkew :: !HeaderArrivalException -> ChainSyncClientException

-- | The Chain sync client only _gracefully_ terminates when the upstream
--   node's chain is not interesting (e.g., forked off too far in the
--   past). By gracefully terminating, the network layer can keep the other
--   mini-protocols connect to the same upstream node running.
--   
--   For example, a relay node will often receive connections from nodes
--   syncing from scratch or an old chain. Since these nodes have a chain
--   that is shorter than the relay node's chain, it's useless for the
--   relay node to run the client-side of the chain sync protocol. However,
--   the other direction of the protocol, and, e.g., the transaction
--   submission protocol, should keep running.
data ChainSyncClientResult
ForkTooDeep :: Point blk -> Our (Tip blk) -> Their (Tip blk) -> ChainSyncClientResult

-- | Our chain changed such that it no longer intersects with the
--   candidate's fragment, and asking for a new intersection did not yield
--   one.
NoMoreIntersection :: Our (Tip blk) -> Their (Tip blk) -> ChainSyncClientResult
RolledBackPastIntersection :: Point blk -> Our (Tip blk) -> Their (Tip blk) -> ChainSyncClientResult

-- | We were asked to terminate via the <a>ControlMessageSTM</a>
AskedToTerminate :: ChainSyncClientResult

-- | Merely a helpful abbreviation
type Consensus (client :: Type -> Type -> Type -> (Type -> Type) -> Type -> Type) blk m = client (Header blk) (Point blk) (Tip blk) m ChainSyncClientResult

-- | A newtype wrapper to avoid confusing our tip with their tip.
newtype Our a
Our :: a -> Our a
[$sel:unOur:Our] :: Our a -> a

-- | A newtype wrapper to avoid confusing our tip with their tip.
newtype Their a
Their :: a -> Their a
[$sel:unTheir:Their] :: Their a -> a

-- | The reason why a block is invalid.
data InvalidBlockReason blk

-- | Events traced by the Chain Sync Client.
data TraceChainSyncClientEvent blk

-- | While following a candidate chain, we rolled forward by downloading a
--   header.
TraceDownloadedHeader :: Header blk -> TraceChainSyncClientEvent blk

-- | While following a candidate chain, we rolled back to the given point.
TraceRolledBack :: Point blk -> TraceChainSyncClientEvent blk

-- | We found an intersection between our chain fragment and the
--   candidate's chain.
TraceFoundIntersection :: Point blk -> Our (Tip blk) -> Their (Tip blk) -> TraceChainSyncClientEvent blk

-- | An exception was thrown by the Chain Sync Client.
TraceException :: ChainSyncClientException -> TraceChainSyncClientEvent blk

-- | The client has terminated.
TraceTermination :: ChainSyncClientResult -> TraceChainSyncClientEvent blk
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.Their a)
instance GHC.Show.Show a => GHC.Show.Show (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.Their a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.Their a)
instance NoThunks.Class.NoThunks a => NoThunks.Class.NoThunks (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.Our a)
instance GHC.Show.Show a => GHC.Show.Show (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.Our a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.Our a)
instance GHC.Generics.Generic (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.UnknownIntersectionState blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.KnownIntersectionState blk)
instance GHC.Show.Show Ouroboros.Consensus.MiniProtocol.ChainSync.Client.ChainSyncClientResult
instance GHC.Show.Show Ouroboros.Consensus.MiniProtocol.ChainSync.Client.ChainSyncClientException
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, GHC.Classes.Eq (Ouroboros.Consensus.Block.Abstract.Header blk)) => GHC.Classes.Eq (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.TraceChainSyncClientEvent blk)
instance (Ouroboros.Consensus.Block.SupportsProtocol.BlockSupportsProtocol blk, GHC.Show.Show (Ouroboros.Consensus.Block.Abstract.Header blk)) => GHC.Show.Show (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.TraceChainSyncClientEvent blk)
instance GHC.Classes.Eq Ouroboros.Consensus.MiniProtocol.ChainSync.Client.ChainSyncClientException
instance GHC.Exception.Type.Exception Ouroboros.Consensus.MiniProtocol.ChainSync.Client.ChainSyncClientException
instance GHC.Classes.Eq Ouroboros.Consensus.MiniProtocol.ChainSync.Client.ChainSyncClientResult
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.KnownIntersectionState blk)
instance Ouroboros.Consensus.Ledger.SupportsProtocol.LedgerSupportsProtocol blk => NoThunks.Class.NoThunks (Ouroboros.Consensus.MiniProtocol.ChainSync.Client.UnknownIntersectionState blk)

module Ouroboros.Consensus.MiniProtocol.BlockFetch.Server

-- | Block fetch server based on <a>mockBlockFetchServer1</a>, but using
--   the <a>ChainDB</a>.
blockFetchServer :: forall m blk. (IOLike m, StandardHash blk, Typeable blk) => Tracer m (TraceBlockFetchServerEvent blk) -> ChainDB m blk -> NodeToNodeVersion -> ResourceRegistry m -> BlockFetchServer (Serialised blk) (Point blk) m ()

-- | Events traced by the Block Fetch Server.
data TraceBlockFetchServerEvent blk

-- | The server sent a block to the peer. This traces the start, not the
--   end, of block sending.
TraceBlockFetchServerSendBlock :: !Point blk -> TraceBlockFetchServerEvent blk
data BlockFetchServerException
blockFetchServer' :: forall m blk a. (IOLike m, StandardHash blk, Typeable blk) => Tracer m (TraceBlockFetchServerEvent blk) -> (StreamFrom blk -> StreamTo blk -> m (Either (UnknownRange blk) (Iterator m blk (WithPoint blk a)))) -> BlockFetchServer a (Point blk) m ()
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.MiniProtocol.BlockFetch.Server.TraceBlockFetchServerEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.MiniProtocol.BlockFetch.Server.TraceBlockFetchServerEvent blk)
instance GHC.Show.Show Ouroboros.Consensus.MiniProtocol.BlockFetch.Server.BlockFetchServerException
instance GHC.Exception.Type.Exception Ouroboros.Consensus.MiniProtocol.BlockFetch.Server.BlockFetchServerException

module Ouroboros.Consensus.HardFork.Combinator.Serialisation.SerialiseDisk
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.ChainDB.Impl.Types.SerialiseDiskConstraints (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.ReconstructNestedCtxt Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.HasBinaryBlockInfo (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Data.ByteString.Lazy.Internal.ByteString -> Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDepIx (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.DecodeDiskDepIx (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.EncodeDiskDep (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.DecodeDiskDep (Ouroboros.Consensus.Block.NestedContent.NestedCtxt Ouroboros.Consensus.Block.Abstract.Header) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HeaderValidation.AnnTip (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HeaderValidation.AnnTip (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkChainDepState xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HardFork.Combinator.Protocol.HardForkChainDepState xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.EncodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Storage.Serialisation.DecodeDisk (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Ledger.Basics.LedgerState (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))

module Ouroboros.Consensus.HardFork.Combinator.Serialisation.SerialiseNodeToNode
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Run.SerialiseNodeToNodeConstraints (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Network.Block.Serialised (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))

module Ouroboros.Consensus.HardFork.Combinator.Serialisation.SerialiseNodeToClient
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Run.SerialiseNodeToClientConstraints (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Network.Block.Serialised (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) Cardano.Slotting.Slot.SlotNo
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.HardFork.Combinator.Mempool.HardForkApplyTxErr xs)
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseNodeToClient (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs => Ouroboros.Consensus.Node.Serialisation.SerialiseResult (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs) (Ouroboros.Consensus.Ledger.Query.BlockQuery (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))


-- | Serialisation support for the HFC
module Ouroboros.Consensus.HardFork.Combinator.Serialisation
data EraNodeToClientVersion blk
EraNodeToClientEnabled :: !BlockNodeToClientVersion blk -> EraNodeToClientVersion blk
EraNodeToClientDisabled :: EraNodeToClientVersion blk
data HardForkNodeToClientVersion xs

-- | Disable the HFC
--   
--   See <a>HardForkNodeToNodeDisabled</a>
[HardForkNodeToClientDisabled] :: BlockNodeToClientVersion x -> HardForkNodeToClientVersion (x ': xs)

-- | Enable the HFC
--   
--   See <a>HardForkNodeToNodeEnabled</a>
[HardForkNodeToClientEnabled] :: HardForkSpecificNodeToClientVersion -> NP EraNodeToClientVersion xs -> HardForkNodeToClientVersion xs
data HardForkNodeToNodeVersion xs

-- | Disable the HFC
--   
--   This means that only the first era (<tt>x</tt>) is supported, and
--   moreover, is compatible with serialisation used if the HFC would not
--   be present at all.
[HardForkNodeToNodeDisabled] :: BlockNodeToNodeVersion x -> HardForkNodeToNodeVersion (x ': xs)

-- | Enable the HFC
--   
--   Serialised values will always include tags inserted by the HFC to
--   distinguish one era from another. We version the hard-fork specific
--   parts with <a>HardForkSpecificNodeToNodeVersion</a>.
[HardForkNodeToNodeEnabled] :: HardForkSpecificNodeToNodeVersion -> NP WrapNodeToNodeVersion xs -> HardForkNodeToNodeVersion xs

-- | Versioning of the specific additions made by the HFC to the
--   <tt>NodeToClient</tt> protocols, e.g., the era tag or the hard-fork
--   specific queries.
data HardForkSpecificNodeToClientVersion
HardForkSpecificNodeToClientVersion1 :: HardForkSpecificNodeToClientVersion

-- | Enable the <a>GetCurrentEra</a> query in <a>QueryHardFork</a>.
HardForkSpecificNodeToClientVersion2 :: HardForkSpecificNodeToClientVersion

-- | Versioning of the specific additions made by the HFC to the
--   <tt>NodeToNode</tt> protocols, e.g., the era tag.
data HardForkSpecificNodeToNodeVersion
HardForkSpecificNodeToNodeVersion1 :: HardForkSpecificNodeToNodeVersion
class (SingleEraBlock blk, SerialiseDiskConstraints blk, SerialiseNodeToNodeConstraints blk, SerialiseNodeToClientConstraints blk, HasNetworkProtocolVersion blk) => SerialiseConstraintsHFC blk

-- | Conditions required by the HFC to provide serialisation
--   
--   NOTE: Compatibility between HFC enabled and disabled:
--   
--   <ol>
--   <li>Node-to-node and node-to-client communication is versioned. When
--   the HFC is disabled, we default to the instances for the first era,
--   and so compatibility is preserved by construction.</li>
--   <li>On-disk storage is <i>not</i> versioned, and here we make no
--   attempt to be compatible between non-HFC and HFC deployments,
--   <i>except</i> for blocks: we define two methods
--   <a>encodeDiskHfcBlock</a> and <a>decodeDiskHfcBlock</a> which are used
--   for on-disk serialisation of blocks. These methods have defaults which
--   can and probably should be used for deployments that use the HFC from
--   the get-go, but for deployments that only later change to use the HFC
--   these functions can be overriden to provide an on-disk storage format
--   for HFC blocks that is compatible with the on-disk storage of blocks
--   from the first era.</li>
--   <li>The converse is NOT supported. Deployments that use the HFC from
--   the start should not use <a>HardForkNodeToNodeDisabled</a> and/or
--   <a>HardForkNodeToClientDisabled</a>. Doing so would result in opposite
--   compatibility problems: the on-disk block would include the HFC tag,
--   but sending blocks with the HFC disabled suggests that that tag is
--   unexpected. This would then lead to problems with binary streaming,
--   and we do not currently provide any provisions to resolve these.</li>
--   </ol>
class (CanHardFork xs, All SerialiseConstraintsHFC xs, All (Compose Show EraNodeToClientVersion) xs, All (Compose Eq EraNodeToClientVersion) xs, All (Compose Show WrapNodeToNodeVersion) xs, All (Compose Eq WrapNodeToNodeVersion) xs, All (EncodeDiskDepIx (NestedCtxt Header)) xs, All (DecodeDiskDepIx (NestedCtxt Header)) xs, All HasBinaryBlockInfo xs) => SerialiseHFC xs
encodeDiskHfcBlock :: SerialiseHFC xs => CodecConfig (HardForkBlock xs) -> HardForkBlock xs -> Encoding
decodeDiskHfcBlock :: SerialiseHFC xs => CodecConfig (HardForkBlock xs) -> forall s. Decoder s (ByteString -> HardForkBlock xs)

-- | Used as the implementation of <a>reconstructPrefixLen</a> for
--   <a>HardForkBlock</a>.
reconstructHfcPrefixLen :: SerialiseHFC xs => proxy (Header (HardForkBlock xs)) -> PrefixLen

-- | Used as the implementation of <a>reconstructNestedCtxt</a> for
--   <a>HardForkBlock</a>.
reconstructHfcNestedCtxt :: SerialiseHFC xs => proxy (Header (HardForkBlock xs)) -> ShortByteString -> SizeInBytes -> SomeSecond (NestedCtxt Header) (HardForkBlock xs)

-- | Used as the implementation of <a>getBinaryBlockInfo</a> for
--   <a>HardForkBlock</a>.
getHfcBinaryBlockInfo :: SerialiseHFC xs => HardForkBlock xs -> BinaryBlockInfo

-- | Used as the implementation of <a>estimateBlockSize</a> for
--   <a>HardForkBlock</a>.
estimateHfcBlockSize :: SerialiseHFC xs => Header (HardForkBlock xs) -> SizeInBytes
isHardForkNodeToClientEnabled :: HardForkNodeToClientVersion xs -> Bool
isHardForkNodeToNodeEnabled :: HardForkNodeToNodeVersion xs -> Bool

module Ouroboros.Consensus.HardFork.Combinator.Node
instance Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs => Ouroboros.Consensus.Config.SupportsNode.ConfigSupportsNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance (Ouroboros.Consensus.HardFork.Combinator.Abstract.CanHardFork.CanHardFork xs, Ouroboros.Consensus.Node.NetworkProtocolVersion.SupportedNetworkProtocolVersion (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs), Ouroboros.Consensus.HardFork.Combinator.Serialisation.Common.SerialiseHFC xs) => Ouroboros.Consensus.Node.Run.RunNode (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)


-- | The hard fork combinator
--   
--   Intended for unqualified import
module Ouroboros.Consensus.HardFork.Combinator

-- | The parameterizable exception monad.
--   
--   Computations are either exceptions or normal values.
--   
--   The <a>return</a> function returns a normal value, while
--   <tt>&gt;&gt;=</tt> exits on the first exception. For a variant that
--   continues after an error and collects all the errors, see
--   <a>Errors</a>.
type Except e = ExceptT e Identity

-- | Static configuration required to work with this type of blocks
data family BlockConfig blk :: Type

-- | Static configuration required for serialisation and deserialisation of
--   types pertaining to this type of block.
--   
--   Data family instead of type family to get better type inference.
data family CodecConfig blk :: Type

-- | Config needed for the <a>NodeInitStorage</a> class. Defined here to
--   avoid circular dependencies.
data family StorageConfig blk :: Type
data family Header blk :: Type

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type

-- | Context identifying what kind of block we have
--   
--   In almost all places we will use <a>NestedCtxt</a> rather than
--   <a>NestedCtxt_</a>.
data family NestedCtxt_ blk :: (Type -> Type) -> (Type -> Type)

-- | We tried to convert something that is past the horizon
--   
--   That is, we tried to convert something that is past the point in time
--   beyond which we lack information due to uncertainty about the next
--   hard fork.
data PastHorizonException

-- | " Validated " transaction or block
--   
--   The ledger defines how to validate transactions and blocks. It's
--   possible the type before and after validation may be distinct (eg
--   Alonzo transactions), which originally motivated this family.
--   
--   We also gain the related benefit that certain interface functions,
--   such as those that <i>reapply</i> blocks, can have a more precise type
--   now. TODO
--   
--   Similarly, the Node-to-Client mini protocols can explicitly indicate
--   that the client trusts the blocks from the local server, by having the
--   server send <a>Validated</a> blocks to the client. TODO
--   
--   Note that validation has different implications for a transaction than
--   for a block. In particular, a validated transaction can be " reapplied
--   " to different ledger states, whereas a validated block must only be "
--   reapplied " to the exact same ledger state (eg as part of rebuilding
--   from an on-disk ledger snapshot).
--   
--   Since the ledger defines validation, see the ledger details for
--   concrete examples of what determines the validity (wrt to a
--   <a>LedgerState</a>) of a transaction and/or block. Example properties
--   include: a transaction's claimed inputs exist and are still unspent, a
--   block carries a sufficient cryptographic signature, etc.
data family Validated x :: Type
data () => Product2 (f :: Type -> Type -> Type) (g :: Type -> Type -> Type) x y
Pair2 :: f x y -> g x y -> Product2 (f :: Type -> Type -> Type) (g :: Type -> Type -> Type) x y
data () => InPairs (f :: k -> k -> Type) (xs :: [k])
[PNil] :: forall {k} (f :: k -> k -> Type) (x :: k). InPairs f '[x]
[PCons] :: forall {k} (f :: k -> k -> Type) (x :: k) (y :: k) (zs :: [k]). f x y -> InPairs f (y ': zs) -> InPairs f (x ': (y ': zs))
data () => Mismatch (f :: k -> Type) (g :: k -> Type) (xs :: [k])
[ML] :: forall {k} (f :: k -> Type) (x :: k) (g :: k -> Type) (xs1 :: [k]). f x -> NS g xs1 -> Mismatch f g (x ': xs1)
[MR] :: forall {k} (f :: k -> Type) (xs1 :: [k]) (g :: k -> Type) (x :: k). NS f xs1 -> g x -> Mismatch f g (x ': xs1)
[MS] :: forall {k} (f :: k -> Type) (g :: k -> Type) (xs1 :: [k]) (x :: k). Mismatch f g xs1 -> Mismatch f g (x ': xs1)
data () => Telescope (g :: k -> Type) (f :: k -> Type) (xs :: [k])
[TZ] :: forall {k} (f :: k -> Type) (x :: k) (g :: k -> Type) (xs1 :: [k]). !f x -> Telescope g f (x ': xs1)
[TS] :: forall {k} (g :: k -> Type) (x :: k) (f :: k -> Type) (xs1 :: [k]). !g x -> !Telescope g f xs1 -> Telescope g f (x ': xs1)
newtype MismatchEraInfo xs
MismatchEraInfo :: Mismatch SingleEraInfo LedgerEraInfo xs -> MismatchEraInfo xs

-- | Era mismatch
--   
--   We have an era mismatch between the era of a
--   block<i>header</i>tx/query and the era of the current ledger.
[getMismatchEraInfo] :: MismatchEraInfo xs -> Mismatch SingleEraInfo LedgerEraInfo xs
newtype OneEraApplyTxErr xs
OneEraApplyTxErr :: NS WrapApplyTxErr xs -> OneEraApplyTxErr xs
[getOneEraApplyTxErr] :: OneEraApplyTxErr xs -> NS WrapApplyTxErr xs
newtype OneEraBlock xs
OneEraBlock :: NS I xs -> OneEraBlock xs
[getOneEraBlock] :: OneEraBlock xs -> NS I xs
newtype OneEraGenTx xs
OneEraGenTx :: NS GenTx xs -> OneEraGenTx xs
[getOneEraGenTx] :: OneEraGenTx xs -> NS GenTx xs
newtype OneEraGenTxId xs
OneEraGenTxId :: NS WrapGenTxId xs -> OneEraGenTxId xs
[getOneEraGenTxId] :: OneEraGenTxId xs -> NS WrapGenTxId xs

-- | The hash for an era
--   
--   This type is special: we don't use an NS here, because the hash by
--   itself should not allow us to differentiate between eras. If it did,
--   the <i>size</i> of the hash would necessarily have to increase, and
--   that leads to trouble. So, the type parameter <tt>xs</tt> here is
--   merely a phantom one, and we just store the underlying raw hash.
newtype OneEraHash (xs :: [k])
OneEraHash :: ShortByteString -> OneEraHash (xs :: [k])
[getOneEraHash] :: OneEraHash (xs :: [k]) -> ShortByteString
newtype OneEraHeader xs
OneEraHeader :: NS Header xs -> OneEraHeader xs
[getOneEraHeader] :: OneEraHeader xs -> NS Header xs
newtype OneEraTipInfo xs
OneEraTipInfo :: NS WrapTipInfo xs -> OneEraTipInfo xs
[getOneEraTipInfo] :: OneEraTipInfo xs -> NS WrapTipInfo xs
newtype PerEraBlockConfig xs
PerEraBlockConfig :: NP BlockConfig xs -> PerEraBlockConfig xs
[getPerEraBlockConfig] :: PerEraBlockConfig xs -> NP BlockConfig xs
newtype PerEraCodecConfig xs
PerEraCodecConfig :: NP CodecConfig xs -> PerEraCodecConfig xs
[getPerEraCodecConfig] :: PerEraCodecConfig xs -> NP CodecConfig xs
newtype PerEraConsensusConfig xs
PerEraConsensusConfig :: NP WrapPartialConsensusConfig xs -> PerEraConsensusConfig xs
[getPerEraConsensusConfig] :: PerEraConsensusConfig xs -> NP WrapPartialConsensusConfig xs
newtype PerEraLedgerConfig xs
PerEraLedgerConfig :: NP WrapPartialLedgerConfig xs -> PerEraLedgerConfig xs
[getPerEraLedgerConfig] :: PerEraLedgerConfig xs -> NP WrapPartialLedgerConfig xs
newtype PerEraStorageConfig xs
PerEraStorageConfig :: NP StorageConfig xs -> PerEraStorageConfig xs
[getPerEraStorageConfig] :: PerEraStorageConfig xs -> NP StorageConfig xs

-- | For each era in which we want to forge blocks, we have a
--   <a>BlockForging</a>, and thus <a>ForgeStateInfo</a>.
--   
--   When we update the hard fork forge state, we only update the forge
--   state of the current era. However, the current era <i>might not</i>
--   have a forge state as it lacks a <a>BlockForging</a>.
--   
--   TODO #2766: expire past <tt>ForgeState</tt>
data HardForkForgeStateInfo xs

-- | There is no <a>BlockForging</a> record for the current era.
[CurrentEraLacksBlockForging] :: EraIndex (x ': (y ': xs)) -> HardForkForgeStateInfo (x ': (y ': xs))

-- | The <tt>ForgeState</tt> of the current era was updated.
[CurrentEraForgeStateUpdated] :: OneEraForgeStateInfo xs -> HardForkForgeStateInfo xs
type InjectTx = InjectPolyTx GenTx
type InjectValidatedTx = InjectPolyTx WrapValidatedGenTx

-- | Generic hard fork state
--   
--   This is used both for the consensus state and the ledger state.
--   
--   By using a telescope with <tt>f ~ LedgerState</tt>, we will keep track
--   of <a>Past</a> information for eras before the current one:
--   
--   <pre>
--   TZ currentByronState
--   TZ pastByronState $ TZ currentShelleyState
--   TZ pastByronState $ TS pastShelleyState $ TZ currentAllegraState
--   ...
--   </pre>
--   
--   These are some intuitions on how the Telescope operations behave for
--   this type:
--   
--   <h1><tt>extend</tt></h1>
--   
--   Suppose we have a telescope containing the ledger state. The "how to
--   extend" argument would take, say, the final Byron state to the initial
--   Shelley state; and "where to extend from" argument would indicate when
--   we want to extend: when the current slot number has gone past the end
--   of the Byron era.
--   
--   <h1><tt>retract</tt></h1>
--   
--   Suppose we have a telescope containing the consensus state. When we
--   rewind the consensus state, we might cross a hard fork transition
--   point. So we first <i>retract</i> the telescope <i>to</i> the era
--   containing the slot number that we want to rewind to, and only then
--   call <tt>rewindChainDepState</tt> on that era. Of course, retraction
--   may fail (we might not <i>have</i> past consensus state to rewind to
--   anymore); this failure would require a choice for a particular monad
--   <tt>m</tt>.
--   
--   <h1><tt>align</tt></h1>
--   
--   Suppose we have one telescope containing the already-ticked ledger
--   state, and another telescope containing the consensus state. Since the
--   ledger state has already been ticked, it might have been advanced to
--   the next era. If this happens, we should then align the consensus
--   state with the ledger state, moving <i>it</i> also to the next era,
--   before we can do the consensus header validation check. Note that in
--   this particular example, the ledger state will always be ahead of the
--   consensus state, never behind; <tt>alignExtend</tt> can be used in
--   this case.
newtype HardForkState f xs
HardForkState :: Telescope (K Past) (Current f) xs -> HardForkState f xs
[getHardForkState] :: HardForkState f xs -> Telescope (K Past) (Current f) xs
class () => IsNonEmpty (xs :: [a])
isNonEmpty :: IsNonEmpty xs => proxy xs -> ProofNonEmpty xs
data () => ProofNonEmpty (xs :: [a])
[ProofNonEmpty] :: forall {a} (x :: a) (xs1 :: [a]). Proxy x -> Proxy xs1 -> ProofNonEmpty (x ': xs1)
class (All SingleEraBlock xs, Typeable xs, IsNonEmpty xs) => CanHardFork xs
hardForkEraTranslation :: CanHardFork xs => EraTranslation xs
hardForkChainSel :: CanHardFork xs => Tails AcrossEraSelection xs
hardForkInjectTxs :: CanHardFork xs => InPairs (RequiringBoth WrapLedgerConfig (Product2 InjectTx InjectValidatedTx)) xs
class SingleEraBlock blk => NoHardForks blk

-- | Extract <a>EraParams</a> from the top-level config
--   
--   The HFC itself does not care about this, as it must be given the full
--   shape across <i>all</i> eras.
getEraParams :: NoHardForks blk => TopLevelConfig blk -> EraParams

-- | Construct partial ledger config from full ledger config
--   
--   See also <a>toPartialConsensusConfig</a>
toPartialLedgerConfig :: NoHardForks blk => proxy blk -> LedgerConfig blk -> PartialLedgerConfig blk

-- | Blocks from which we can assemble a hard fork
class (LedgerSupportsProtocol blk, InspectLedger blk, LedgerSupportsMempool blk, HasTxId (GenTx blk), QueryLedger blk, HasPartialConsensusConfig (BlockProtocol blk), HasPartialLedgerConfig blk, ConvertRawHash blk, ReconstructNestedCtxt Header blk, CommonProtocolParams blk, LedgerSupportsPeerSelection blk, ConfigSupportsNode blk, NodeInitStorage blk, BlockSupportsMetrics blk, Eq (GenTx blk), Eq (Validated (GenTx blk)), Eq (ApplyTxErr blk), Show blk, Show (Header blk), Show (CannotForge blk), Show (ForgeStateInfo blk), Show (ForgeStateUpdateError blk), Show (LedgerState blk), Eq (LedgerState blk), NoThunks (LedgerState blk)) => SingleEraBlock blk

-- | Era transition
--   
--   This should only report the transition point once it is stable
--   (rollback cannot affect it anymore).
--   
--   Since we need this to construct the <tt>HardForkSummary</tt> (and
--   hence the <a>EpochInfo</a>), this takes the <i>partial</i> config, not
--   the full config (or we'd end up with a catch-22).
singleEraTransition :: SingleEraBlock blk => PartialLedgerConfig blk -> EraParams -> Bound -> LedgerState blk -> Maybe EpochNo

-- | Era information (for use in error messages)
singleEraInfo :: SingleEraBlock blk => proxy blk -> SingleEraInfo blk
data () => EpochInfo (m :: Type -> Type)
EpochInfo :: (HasCallStack => EpochNo -> m EpochSize) -> (HasCallStack => EpochNo -> m SlotNo) -> (HasCallStack => SlotNo -> m EpochNo) -> (HasCallStack => SlotNo -> m RelativeTime) -> (HasCallStack => SlotNo -> m SlotLength) -> EpochInfo (m :: Type -> Type)
[epochInfoSize_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => EpochNo -> m EpochSize
[epochInfoFirst_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => EpochNo -> m SlotNo
[epochInfoEpoch_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => SlotNo -> m EpochNo
[epochInfoSlotToRelativeTime_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => SlotNo -> m RelativeTime
[epochInfoSlotLength_] :: EpochInfo (m :: Type -> Type) -> HasCallStack => SlotNo -> m SlotLength
newtype EraIndex xs
EraIndex :: NS (K ()) xs -> EraIndex xs
[getEraIndex] :: EraIndex xs -> NS (K ()) xs
newtype HardForkBlock xs
HardForkBlock :: OneEraBlock xs -> HardForkBlock xs
[getHardForkBlock] :: HardForkBlock xs -> OneEraBlock xs
data HardForkProtocol (xs :: [Type])

-- | Ledger state associated with a block
data family LedgerState blk :: Type

-- | Static configuration required to run the consensus protocol
--   
--   Every method in the <a>ConsensusProtocol</a> class takes the consensus
--   configuration as a parameter, so having this as a data family rather
--   than a type family resolves most ambiguity.
--   
--   Defined out of the class so that protocols can define this type
--   without having to define the entire protocol at the same time (or
--   indeed in the same module).
data family ConsensusConfig p :: Type
data HardForkLedgerConfig xs
HardForkLedgerConfig :: !Shape xs -> !PerEraLedgerConfig xs -> HardForkLedgerConfig xs
[hardForkLedgerConfigShape] :: HardForkLedgerConfig xs -> !Shape xs
[hardForkLedgerConfigPerEra] :: HardForkLedgerConfig xs -> !PerEraLedgerConfig xs

-- | Generalized transaction
--   
--   The mempool (and, accordingly, blocks) consist of "generalized
--   transactions"; this could be "proper" transactions (transferring
--   funds) but also other kinds of things such as update proposals,
--   delegations, etc.
data family GenTx blk :: Type

-- | Different queries supported by the ledger, indexed by the result type.
data family BlockQuery blk :: Type -> Type
data HardForkApplyTxErr xs

-- | Validation error from one of the eras
HardForkApplyTxErrFromEra :: !OneEraApplyTxErr xs -> HardForkApplyTxErr xs

-- | We tried to apply a block from the wrong era
HardForkApplyTxErrWrongEra :: !MismatchEraInfo xs -> HardForkApplyTxErr xs
data HardForkEnvelopeErr xs

-- | Validation error from one of the eras
HardForkEnvelopeErrFromEra :: OneEraEnvelopeErr xs -> HardForkEnvelopeErr xs

-- | We tried to apply a block from the wrong era
HardForkEnvelopeErrWrongEra :: MismatchEraInfo xs -> HardForkEnvelopeErr xs
data HardForkLedgerError xs

-- | Validation error from one of the eras
HardForkLedgerErrorFromEra :: OneEraLedgerError xs -> HardForkLedgerError xs

-- | We tried to apply a block from the wrong era
HardForkLedgerErrorWrongEra :: MismatchEraInfo xs -> HardForkLedgerError xs

-- | A generalized transaction, <a>GenTx</a>, identifier.
data family TxId tx :: Type

-- | Additional newtype wrapper around <a>SingleEraInfo</a>
--   
--   This is primarily useful for use in error messages: it marks which era
--   info came from the ledger, and which came from a
--   tx<i>block</i>header/etc.
newtype LedgerEraInfo blk
LedgerEraInfo :: SingleEraInfo blk -> LedgerEraInfo blk
[getLedgerEraInfo] :: LedgerEraInfo blk -> SingleEraInfo blk

-- | Information about an era (mostly for type errors)
data SingleEraInfo blk
SingleEraInfo :: !Text -> SingleEraInfo blk
[singleEraName] :: SingleEraInfo blk -> !Text
data HardForkLedgerUpdate xs
HardForkUpdateInEra :: OneEraLedgerUpdate xs -> HardForkLedgerUpdate xs

-- | Hard fork transition got confirmed
HardForkUpdateTransitionConfirmed :: EraIndex xs -> EraIndex xs -> EpochNo -> HardForkLedgerUpdate xs

-- | Hard fork transition happened
--   
--   We record the <a>EpochNo</a> at the start of the era after the
--   transition
HardForkUpdateTransitionDone :: EraIndex xs -> EraIndex xs -> EpochNo -> HardForkLedgerUpdate xs

-- | The hard fork transition rolled back
HardForkUpdateTransitionRolledBack :: EraIndex xs -> EraIndex xs -> HardForkLedgerUpdate xs
data HardForkLedgerWarning xs

-- | Warning from the underlying era
HardForkWarningInEra :: OneEraLedgerWarning xs -> HardForkLedgerWarning xs

-- | The transition to the next era does not match the <a>EraParams</a>
--   
--   The <a>EraParams</a> can specify a lower bound on when the transition
--   to the next era will happen. If the actual transition, when confirmed,
--   is <i>before</i> this lower bound, the node is misconfigured and will
--   likely not work correctly. This should be taken care of as soon as
--   possible (before the transition happens).
HardForkWarningTransitionMismatch :: EraIndex xs -> EraParams -> EpochNo -> HardForkLedgerWarning xs

-- | Transition in the final era
--   
--   The final era should never confirm any transitions. For clarity, we
--   also record the index of that final era.
HardForkWarningTransitionInFinalEra :: EraIndex xs -> EpochNo -> HardForkLedgerWarning xs

-- | An already-confirmed transition got un-confirmed
HardForkWarningTransitionUnconfirmed :: EraIndex xs -> HardForkLedgerWarning xs

-- | An already-confirmed transition got changed
--   
--   We record the indices of the era we are transitioning from and to, as
--   well as the old and new <a>EpochNo</a> of that transition, in that
--   order.
HardForkWarningTransitionReconfirmed :: EraIndex xs -> EraIndex xs -> EpochNo -> EpochNo -> HardForkLedgerWarning xs

-- | Forecast annotated with details about the ledger it was derived from
data AnnForecast state view blk
AnnForecast :: Forecast (view blk) -> state blk -> WithOrigin SlotNo -> Maybe Bound -> AnnForecast state view blk
[annForecast] :: AnnForecast state view blk -> Forecast (view blk)
[annForecastState] :: AnnForecast state view blk -> state blk
[annForecastTip] :: AnnForecast state view blk -> WithOrigin SlotNo
[annForecastEnd] :: AnnForecast state view blk -> Maybe Bound
type HardForkQueryResult xs = Either (MismatchEraInfo xs)
data QueryAnytime result
[GetEraStart] :: QueryAnytime (Maybe Bound)
data QueryHardFork xs result
[GetInterpreter] :: QueryHardFork xs (Interpreter xs)
[GetCurrentEra] :: QueryHardFork xs (EraIndex xs)
data QueryIfCurrent :: [Type] -> Type -> Type
[QZ] :: BlockQuery x result -> QueryIfCurrent (x ': xs) result
[QS] :: QueryIfCurrent xs result -> QueryIfCurrent (x ': xs) result

-- | Partial consensus config
class (ConsensusProtocol p, NoThunks (PartialConsensusConfig p)) => HasPartialConsensusConfig p where {
    type PartialConsensusConfig p :: Type;
    type PartialConsensusConfig p = ConsensusConfig p;
}

-- | Construct <a>ConsensusConfig</a> from <a>PartialConsensusConfig</a>
--   
--   See comments for <a>completeLedgerConfig</a> for some details about
--   the <a>EpochInfo</a>.
completeConsensusConfig :: HasPartialConsensusConfig p => proxy p -> EpochInfo (Except PastHorizonException) -> PartialConsensusConfig p -> ConsensusConfig p

-- | Construct <a>ConsensusConfig</a> from <a>PartialConsensusConfig</a>
--   
--   See comments for <a>completeLedgerConfig</a> for some details about
--   the <a>EpochInfo</a>.
completeConsensusConfig :: (HasPartialConsensusConfig p, PartialConsensusConfig p ~ ConsensusConfig p) => proxy p -> EpochInfo (Except PastHorizonException) -> PartialConsensusConfig p -> ConsensusConfig p

-- | Construct partial consensus config from full consensus config
--   
--   NOTE: This is basically just losing <a>EpochInfo</a>, but that is
--   constant anyway when we are dealing with a single era.
toPartialConsensusConfig :: HasPartialConsensusConfig p => proxy p -> ConsensusConfig p -> PartialConsensusConfig p

-- | Construct partial consensus config from full consensus config
--   
--   NOTE: This is basically just losing <a>EpochInfo</a>, but that is
--   constant anyway when we are dealing with a single era.
toPartialConsensusConfig :: (HasPartialConsensusConfig p, PartialConsensusConfig p ~ ConsensusConfig p) => proxy p -> ConsensusConfig p -> PartialConsensusConfig p

-- | Partial ledger config
class (UpdateLedger blk, NoThunks (PartialLedgerConfig blk)) => HasPartialLedgerConfig blk where {
    type PartialLedgerConfig blk :: Type;
    type PartialLedgerConfig blk = LedgerConfig blk;
}

-- | Construct <a>LedgerConfig</a> from <tt>PartialLedgerCfg</tt>
--   
--   NOTE: The <a>EpochInfo</a> provided will have limited range, any
--   attempt to look past its horizon will result in a pure
--   <a>PastHorizonException</a>. The horizon is determined by the tip of
--   the ledger <i>state</i> (not view) from which the <a>EpochInfo</a> is
--   derived.
completeLedgerConfig :: HasPartialLedgerConfig blk => proxy blk -> EpochInfo (Except PastHorizonException) -> PartialLedgerConfig blk -> LedgerConfig blk

-- | Construct <a>LedgerConfig</a> from <tt>PartialLedgerCfg</tt>
--   
--   NOTE: The <a>EpochInfo</a> provided will have limited range, any
--   attempt to look past its horizon will result in a pure
--   <a>PastHorizonException</a>. The horizon is determined by the tip of
--   the ledger <i>state</i> (not view) from which the <a>EpochInfo</a> is
--   derived.
completeLedgerConfig :: (HasPartialLedgerConfig blk, PartialLedgerConfig blk ~ LedgerConfig blk) => proxy blk -> EpochInfo (Except PastHorizonException) -> PartialLedgerConfig blk -> LedgerConfig blk
newtype WrapPartialConsensusConfig blk
WrapPartialConsensusConfig :: PartialConsensusConfig (BlockProtocol blk) -> WrapPartialConsensusConfig blk
[unwrapPartialConsensusConfig] :: WrapPartialConsensusConfig blk -> PartialConsensusConfig (BlockProtocol blk)
newtype WrapPartialLedgerConfig blk
WrapPartialLedgerConfig :: PartialLedgerConfig blk -> WrapPartialLedgerConfig blk
[unwrapPartialLedgerConfig] :: WrapPartialLedgerConfig blk -> PartialLedgerConfig blk
newtype HardForkSelectView xs
HardForkSelectView :: WithBlockNo OneEraSelectView xs -> HardForkSelectView xs
[getHardForkSelectView] :: HardForkSelectView xs -> WithBlockNo OneEraSelectView xs

-- | We have one or more <a>BlockForging</a>s, and thus <a>CanBeLeader</a>
--   proofs, for each era in which we can forge blocks.
type HardForkCanBeLeader xs = SomeErasCanBeLeader xs
type HardForkChainDepState xs = HardForkState WrapChainDepState xs

-- | We are a leader if we have a proof from one of the eras
type HardForkIsLeader xs = OneEraIsLeader xs
data HardForkValidationErr xs

-- | Validation error from one of the eras
HardForkValidationErrFromEra :: OneEraValidationErr xs -> HardForkValidationErr xs

-- | We tried to apply a block from the wrong era
HardForkValidationErrWrongEra :: MismatchEraInfo xs -> HardForkValidationErr xs
type HardForkLedgerView = HardForkLedgerView_ WrapLedgerView
data HardForkLedgerView_ f xs
HardForkLedgerView :: !TransitionInfo -> !HardForkState f xs -> HardForkLedgerView_ f xs

-- | Information about the transition to the next era, if known
[hardForkLedgerViewTransition] :: HardForkLedgerView_ f xs -> !TransitionInfo

-- | The underlying ledger view
[hardForkLedgerViewPerEra] :: HardForkLedgerView_ f xs -> !HardForkState f xs
data AcrossEraSelection :: Type -> Type -> Type

-- | Just compare block numbers
--   
--   This is a useful default when two eras run totally different consensus
--   protocols, and we just want to choose the longer chain.
[CompareBlockNo] :: AcrossEraSelection x y

-- | Two eras using the same <a>SelectView</a>. In this case, we can just
--   compare chains even across eras, as the chain ordering is fully
--   captured by <a>SelectView</a> and its <a>Ord</a> instance.
[CompareSameSelectView] :: SelectView (BlockProtocol x) ~ SelectView (BlockProtocol y) => AcrossEraSelection x y
data WithBlockNo (f :: k -> Type) (a :: k)
WithBlockNo :: BlockNo -> f a -> WithBlockNo (f :: k -> Type) (a :: k)
[getBlockNo] :: WithBlockNo (f :: k -> Type) (a :: k) -> BlockNo
[dropBlockNo] :: WithBlockNo (f :: k -> Type) (a :: k) -> f a
data EraTranslation xs
EraTranslation :: InPairs (RequiringBoth WrapLedgerConfig (Translate LedgerState)) xs -> InPairs (RequiringBoth WrapConsensusConfig (Translate WrapChainDepState)) xs -> InPairs (RequiringBoth WrapLedgerConfig (CrossEraForecaster LedgerState WrapLedgerView)) xs -> EraTranslation xs
[translateLedgerState] :: EraTranslation xs -> InPairs (RequiringBoth WrapLedgerConfig (Translate LedgerState)) xs
[translateChainDepState] :: EraTranslation xs -> InPairs (RequiringBoth WrapConsensusConfig (Translate WrapChainDepState)) xs
[crossEraForecast] :: EraTranslation xs -> InPairs (RequiringBoth WrapLedgerConfig (CrossEraForecaster LedgerState WrapLedgerView)) xs

-- | <a>InjectPolyTx</a> at type <a>InjectTx</a>
pattern InjectTx :: (GenTx blk -> Maybe (GenTx blk')) -> InjectTx blk blk'

-- | <a>InjectPolyTx</a> at type <a>InjectValidatedTx</a>
pattern InjectValidatedTx :: (WrapValidatedGenTx blk -> Maybe (WrapValidatedGenTx blk')) -> InjectValidatedTx blk blk'
hardForkBlockForging :: forall m xs. (CanHardFork xs, Monad m) => Text -> NonEmptyOptNP (BlockForging m) xs -> BlockForging m (HardForkBlock xs)

-- | <a>cannotInjectPolyTx</a> at type <a>InjectTx</a>
cannotInjectTx :: InjectTx blk blk'

-- | <a>cannotInjectPolyTx</a> at type <a>InjectValidatedTx</a>
cannotInjectValidatedTx :: InjectValidatedTx blk blk'
initHardForkState :: f x -> HardForkState f (x ': xs)
noHardForksEpochInfo :: (Monad m, NoHardForks blk) => TopLevelConfig blk -> EpochInfo m
proxySingle :: Proxy SingleEraBlock
singleEraTransition' :: SingleEraBlock blk => WrapPartialLedgerConfig blk -> EraParams -> Bound -> LedgerState blk -> Maybe EpochNo
eraIndexEmpty :: EraIndex '[] -> Void
eraIndexFromIndex :: Index xs blk -> EraIndex xs
eraIndexFromNS :: SListI xs => NS f xs -> EraIndex xs
eraIndexSucc :: EraIndex xs -> EraIndex (x ': xs)
eraIndexToInt :: EraIndex xs -> Int
eraIndexZero :: EraIndex (x ': xs)
completeConsensusConfig' :: forall blk. HasPartialConsensusConfig (BlockProtocol blk) => EpochInfo (Except PastHorizonException) -> WrapPartialConsensusConfig blk -> ConsensusConfig (BlockProtocol blk)
completeConsensusConfig'' :: forall blk. HasPartialConsensusConfig (BlockProtocol blk) => EpochInfo (Except PastHorizonException) -> WrapPartialConsensusConfig blk -> WrapConsensusConfig blk
completeLedgerConfig' :: forall blk. HasPartialLedgerConfig blk => EpochInfo (Except PastHorizonException) -> WrapPartialLedgerConfig blk -> LedgerConfig blk
completeLedgerConfig'' :: forall blk. HasPartialLedgerConfig blk => EpochInfo (Except PastHorizonException) -> WrapPartialLedgerConfig blk -> WrapLedgerConfig blk
distribLedgerConfig :: CanHardFork xs => EpochInfo (Except PastHorizonException) -> LedgerConfig (HardForkBlock xs) -> NP WrapLedgerConfig xs
distribTopLevelConfig :: All SingleEraBlock xs => EpochInfo (Except PastHorizonException) -> TopLevelConfig (HardForkBlock xs) -> NP TopLevelConfig xs
distribAnnTip :: SListI xs => AnnTip (HardForkBlock xs) -> NS AnnTip xs
undistribAnnTip :: SListI xs => NS AnnTip xs -> AnnTip (HardForkBlock xs)

-- | Change a telescope of a forecast into a forecast of a telescope
mkHardForkForecast :: forall state view xs. SListI xs => InPairs (CrossEraForecaster state view) xs -> HardForkState (AnnForecast state view) xs -> Forecast (HardForkLedgerView_ view xs)
decodeQueryAnytimeResult :: QueryAnytime result -> forall s. Decoder s result
decodeQueryHardForkResult :: SListI xs => QueryHardFork xs result -> forall s. Decoder s result
encodeQueryAnytimeResult :: QueryAnytime result -> result -> Encoding
encodeQueryHardForkResult :: SListI xs => QueryHardFork xs result -> result -> Encoding
getHardForkQuery :: BlockQuery (HardForkBlock xs) result -> (forall result'. (result :~: HardForkQueryResult xs result') -> QueryIfCurrent xs result' -> r) -> (forall x' xs'. (xs :~: (x' ': xs')) -> ProofNonEmpty xs' -> QueryAnytime result -> EraIndex xs -> r) -> (forall x' xs'. (xs :~: (x' ': xs')) -> ProofNonEmpty xs' -> QueryHardFork xs result -> r) -> r
hardForkQueryInfo :: All SingleEraBlock xs => QueryIfCurrent xs result -> NS SingleEraInfo xs
hardForkApplyTxErrFromEither :: Either (MismatchEraInfo xs) (OneEraApplyTxErr xs) -> HardForkApplyTxErr xs
hardForkApplyTxErrToEither :: HardForkApplyTxErr xs -> Either (MismatchEraInfo xs) (OneEraApplyTxErr xs)
acrossEraSelection :: All SingleEraBlock xs => Tails AcrossEraSelection xs -> WithBlockNo (NS WrapSelectView) xs -> WithBlockNo (NS WrapSelectView) xs -> Ordering
mapWithBlockNo :: (f x -> g y) -> WithBlockNo f x -> WithBlockNo g y
trivialEraTranslation :: EraTranslation '[blk]

module Ouroboros.Consensus.HardFork.Combinator.Embed.Nary
class Inject f
inject :: forall x xs. (Inject f, CanHardFork xs) => Exactly xs Bound -> Index xs x -> f x -> f (HardForkBlock xs)
inject' :: forall f a b x xs. (Inject f, CanHardFork xs, Coercible a (f x), Coercible b (f (HardForkBlock xs))) => Proxy f -> Exactly xs Bound -> Index xs x -> a -> b
injectHardForkState :: forall f x xs. Exactly xs Bound -> Index xs x -> f x -> HardForkState f xs
injectNestedCtxt_ :: forall f x xs a. Index xs x -> NestedCtxt_ x f a -> NestedCtxt_ (HardForkBlock xs) f a
injectQuery :: forall x xs result. Index xs x -> BlockQuery x result -> QueryIfCurrent xs result

-- | Inject the first era's initial <a>ExtLedgerState</a> and trigger any
--   translations that should take place in the very first slot.
--   
--   Performs any hard forks scheduled via <tt>TriggerHardForkAtEpoch</tt>.
--   
--   Note: we can translate across multiple eras when computing the initial
--   ledger state, but we do not support translation across multiple eras
--   in general; extending <tt>applyChainTick</tt> to translate across more
--   than one era is not problematic, but extending
--   <tt>ledgerViewForecastAt</tt> is a lot more subtle; see
--   <tt>forecastNotFinal</tt>.
injectInitialExtLedgerState :: forall x xs. CanHardFork (x ': xs) => TopLevelConfig (HardForkBlock (x ': xs)) -> ExtLedgerState x -> ExtLedgerState (HardForkBlock (x ': xs))
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Data.SOP.BasicFunctors.I
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.Block.Abstract.Header
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.Storage.Serialisation.SerialisedHeader
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.TypeFamilyWrappers.WrapHeaderHash
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.Ledger.SupportsMempool.GenTx
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.TypeFamilyWrappers.WrapApplyTxErr
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject (Ouroboros.Consensus.Util.SomeSecond Ouroboros.Consensus.Ledger.Query.BlockQuery)
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.HeaderValidation.AnnTip
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.Ledger.Basics.LedgerState
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.TypeFamilyWrappers.WrapChainDepState
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.HeaderValidation.HeaderState
instance Ouroboros.Consensus.HardFork.Combinator.Embed.Nary.Inject Ouroboros.Consensus.Ledger.Extended.ExtLedgerState

module Ouroboros.Consensus.HardFork.Combinator.Embed.Binary
protocolInfoBinary :: forall m blk1 blk2. (CanHardFork '[blk1, blk2], Monad m) => ProtocolInfo blk1 -> m [BlockForging m blk1] -> EraParams -> (ConsensusConfig (BlockProtocol blk1) -> PartialConsensusConfig (BlockProtocol blk1)) -> (LedgerConfig blk1 -> PartialLedgerConfig blk1) -> ProtocolInfo blk2 -> m [BlockForging m blk2] -> EraParams -> (ConsensusConfig (BlockProtocol blk2) -> PartialConsensusConfig (BlockProtocol blk2)) -> (LedgerConfig blk2 -> PartialLedgerConfig blk2) -> (ProtocolInfo (HardForkBlock '[blk1, blk2]), m [BlockForging m (HardForkBlock '[blk1, blk2])])


-- | Condense instances
--   
--   These are for the benefit of integration and tests. We do not rely on
--   them within consensus.
--   
--   NOTE: No guarantees are made about what these condense instances look
--   like.
module Ouroboros.Consensus.HardFork.Combinator.Condense
class (Condense blk, Condense (Header blk), Condense (GenTx blk), Condense (GenTxId blk)) => CondenseConstraints blk
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Condense.CondenseConstraints xs => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Condense.CondenseConstraints xs => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Block.Abstract.Header (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Condense.CondenseConstraints xs => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs))
instance Data.SOP.Constraint.All Ouroboros.Consensus.HardFork.Combinator.Condense.CondenseConstraints xs => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Ledger.SupportsMempool.TxId (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx (Ouroboros.Consensus.HardFork.Combinator.Basics.HardForkBlock xs)))
instance Ouroboros.Consensus.Util.Condense.Condense a => Ouroboros.Consensus.Util.Condense.Condense (Data.SOP.BasicFunctors.I a)
instance Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk) => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.TypeFamilyWrappers.WrapGenTxId blk)

module Ouroboros.Consensus.HardFork.Combinator.Degenerate

-- | Static configuration required to work with this type of blocks
data family BlockConfig blk :: Type
pattern DegenBlockConfig :: NoHardForks b => BlockConfig b -> BlockConfig (HardForkBlock '[b])

-- | Different queries supported by the ledger, indexed by the result type.
data family BlockQuery blk :: Type -> Type
pattern DegenQuery :: () => HardForkQueryResult '[b] result ~ a => BlockQuery b result -> BlockQuery (HardForkBlock '[b]) a

-- | Static configuration required for serialisation and deserialisation of
--   types pertaining to this type of block.
--   
--   Data family instead of type family to get better type inference.
data family CodecConfig blk :: Type
pattern DegenCodecConfig :: NoHardForks b => CodecConfig b -> CodecConfig (HardForkBlock '[b])

-- | Static configuration required to run the consensus protocol
--   
--   Every method in the <a>ConsensusProtocol</a> class takes the consensus
--   configuration as a parameter, so having this as a data family rather
--   than a type family resolves most ambiguity.
--   
--   Defined out of the class so that protocols can define this type
--   without having to define the entire protocol at the same time (or
--   indeed in the same module).
data family ConsensusConfig p :: Type
pattern DegenConsensusConfig :: PartialConsensusConfig (BlockProtocol b) -> ConsensusConfig (BlockProtocol (HardForkBlock '[b]))

-- | The <a>Either</a> type represents values with two possibilities: a
--   value of type <tt><a>Either</a> a b</tt> is either <tt><a>Left</a>
--   a</tt> or <tt><a>Right</a> b</tt>.
--   
--   The <a>Either</a> type is sometimes used to represent a value which is
--   either correct or an error; by convention, the <a>Left</a> constructor
--   is used to hold an error value and the <a>Right</a> constructor is
--   used to hold a correct value (mnemonic: "right" also means "correct").
--   
--   <h4><b>Examples</b></h4>
--   
--   The type <tt><a>Either</a> <a>String</a> <a>Int</a></tt> is the type
--   of values which can be either a <a>String</a> or an <a>Int</a>. The
--   <a>Left</a> constructor can be used only on <a>String</a>s, and the
--   <a>Right</a> constructor can be used only on <a>Int</a>s:
--   
--   <pre>
--   &gt;&gt;&gt; let s = Left "foo" :: Either String Int
--   
--   &gt;&gt;&gt; s
--   Left "foo"
--   
--   &gt;&gt;&gt; let n = Right 3 :: Either String Int
--   
--   &gt;&gt;&gt; n
--   Right 3
--   
--   &gt;&gt;&gt; :type s
--   s :: Either String Int
--   
--   &gt;&gt;&gt; :type n
--   n :: Either String Int
--   </pre>
--   
--   The <a>fmap</a> from our <a>Functor</a> instance will ignore
--   <a>Left</a> values, but will apply the supplied function to values
--   contained in a <a>Right</a>:
--   
--   <pre>
--   &gt;&gt;&gt; let s = Left "foo" :: Either String Int
--   
--   &gt;&gt;&gt; let n = Right 3 :: Either String Int
--   
--   &gt;&gt;&gt; fmap (*2) s
--   Left "foo"
--   
--   &gt;&gt;&gt; fmap (*2) n
--   Right 6
--   </pre>
--   
--   The <a>Monad</a> instance for <a>Either</a> allows us to chain
--   together multiple actions which may fail, and fail overall if any of
--   the individual steps failed. First we'll write a function that can
--   either parse an <a>Int</a> from a <a>Char</a>, or fail.
--   
--   <pre>
--   &gt;&gt;&gt; import Data.Char ( digitToInt, isDigit )
--   
--   &gt;&gt;&gt; :{
--       let parseEither :: Char -&gt; Either String Int
--           parseEither c
--             | isDigit c = Right (digitToInt c)
--             | otherwise = Left "parse error"
--   
--   &gt;&gt;&gt; :}
--   </pre>
--   
--   The following should work, since both <tt>'1'</tt> and <tt>'2'</tt>
--   can be parsed as <a>Int</a>s.
--   
--   <pre>
--   &gt;&gt;&gt; :{
--       let parseMultiple :: Either String Int
--           parseMultiple = do
--             x &lt;- parseEither '1'
--             y &lt;- parseEither '2'
--             return (x + y)
--   
--   &gt;&gt;&gt; :}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; parseMultiple
--   Right 3
--   </pre>
--   
--   But the following should fail overall, since the first operation where
--   we attempt to parse <tt>'m'</tt> as an <a>Int</a> will fail:
--   
--   <pre>
--   &gt;&gt;&gt; :{
--       let parseMultiple :: Either String Int
--           parseMultiple = do
--             x &lt;- parseEither 'm'
--             y &lt;- parseEither '2'
--             return (x + y)
--   
--   &gt;&gt;&gt; :}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; parseMultiple
--   Left "parse error"
--   </pre>
data () => Either a b
pattern DegenQueryResult :: result -> HardForkQueryResult '[b] result

-- | Generalized transaction
--   
--   The mempool (and, accordingly, blocks) consist of "generalized
--   transactions"; this could be "proper" transactions (transferring
--   funds) but also other kinds of things such as update proposals,
--   delegations, etc.
data family GenTx blk :: Type
pattern DegenGenTx :: NoHardForks b => GenTx b -> GenTx (HardForkBlock '[b])
data HardForkApplyTxErr xs
pattern DegenApplyTxErr :: forall b. NoHardForks b => ApplyTxErr b -> HardForkApplyTxErr '[b]
data HardForkBlock xs
pattern DegenBlock :: forall b. NoHardForks b => b -> HardForkBlock '[b]
data HardForkEnvelopeErr xs
pattern DegenOtherHeaderEnvelopeError :: forall b. NoHardForks b => OtherHeaderEnvelopeError b -> HardForkEnvelopeErr '[b]
data HardForkLedgerConfig xs
pattern DegenLedgerConfig :: PartialLedgerConfig b -> HardForkLedgerConfig '[b]
data HardForkLedgerError xs
pattern DegenLedgerError :: forall b. NoHardForks b => LedgerError b -> HardForkLedgerError '[b]
data family Header blk :: Type
pattern DegenHeader :: NoHardForks b => Header b -> Header (HardForkBlock '[b])

-- | Ledger state associated with a block
data family LedgerState blk :: Type
pattern DegenLedgerState :: NoHardForks b => LedgerState b -> LedgerState (HardForkBlock '[b])
data OneEraTipInfo xs
pattern DegenTipInfo :: forall b. NoHardForks b => TipInfo b -> OneEraTipInfo '[b]

-- | The top-level node configuration
data TopLevelConfig blk
pattern DegenTopLevelConfig :: NoHardForks b => TopLevelConfig b -> TopLevelConfig (HardForkBlock '[b])

-- | A generalized transaction, <a>GenTx</a>, identifier.
data family TxId tx :: Type
pattern DegenGenTxId :: forall b. NoHardForks b => GenTxId b -> GenTxId (HardForkBlock '[b])

module Ouroboros.Consensus.MiniProtocol.ChainSync.Server
data () => Tip (b :: k)
chainSyncBlockServerFollower :: ChainDB m blk -> ResourceRegistry m -> m (Follower m blk (WithPoint blk (Serialised blk)))

-- | Chain Sync Server for blocks for a given a <a>ChainDB</a>.
--   
--   The local node-to-client protocol uses the chain sync mini-protocol
--   with chains of full blocks (rather than a header / body split).
chainSyncBlocksServer :: forall m blk. (IOLike m, HasHeader (Header blk)) => Tracer m (TraceChainSyncServerEvent blk) -> ChainDB m blk -> Follower m blk (WithPoint blk (Serialised blk)) -> ChainSyncServer (Serialised blk) (Point blk) (Tip blk) m ()
chainSyncHeaderServerFollower :: ChainDB m blk -> ChainType -> ResourceRegistry m -> m (Follower m blk (WithPoint blk (SerialisedHeader blk)))

-- | Chain Sync Server for block headers for a given a <a>ChainDB</a>.
--   
--   The node-to-node protocol uses the chain sync mini-protocol with chain
--   headers (and fetches blocks separately with the block fetch
--   mini-protocol).
chainSyncHeadersServer :: forall m blk. (IOLike m, HasHeader (Header blk)) => Tracer m (TraceChainSyncServerEvent blk) -> ChainDB m blk -> Follower m blk (WithPoint blk (SerialisedHeader blk)) -> ChainSyncServer (SerialisedHeader blk) (Point blk) (Tip blk) m ()

-- | Whether reading a ChainSync server update instruction was blocking or
--   non-blocking.
data BlockingType
Blocking :: BlockingType
NonBlocking :: BlockingType

-- | Events traced by the Chain Sync Server.
data TraceChainSyncServerEvent blk

-- | Send a <a>ChainUpdate</a> message.
TraceChainSyncServerUpdate :: Tip blk -> ChainUpdate blk (Point blk) -> BlockingType -> Enclosing -> TraceChainSyncServerEvent blk

-- | A chain sync server.
--   
--   This is a version of <a>chainSyncServerExample</a> that uses an action
--   to get the current <a>Tip</a> and a <a>Follower</a> instead of
--   <a>ChainProducerState</a>.
--   
--   All the hard work is done by the <a>Follower</a>s provided by the
--   <a>ChainDB</a>.
chainSyncServerForFollower :: forall m blk b. IOLike m => Tracer m (TraceChainSyncServerEvent blk) -> STM m (Tip blk) -> Follower m blk (WithPoint blk b) -> ChainSyncServer b (Point blk) (Tip blk) m ()
instance GHC.Show.Show Ouroboros.Consensus.MiniProtocol.ChainSync.Server.BlockingType
instance GHC.Classes.Ord Ouroboros.Consensus.MiniProtocol.ChainSync.Server.BlockingType
instance GHC.Classes.Eq Ouroboros.Consensus.MiniProtocol.ChainSync.Server.BlockingType
instance Ouroboros.Network.Block.StandardHash blk => GHC.Show.Show (Ouroboros.Consensus.MiniProtocol.ChainSync.Server.TraceChainSyncServerEvent blk)
instance Ouroboros.Network.Block.StandardHash blk => GHC.Classes.Eq (Ouroboros.Consensus.MiniProtocol.ChainSync.Server.TraceChainSyncServerEvent blk)


-- | Initialization of the <a>BlockFetchConsensusInterface</a>
module Ouroboros.Consensus.MiniProtocol.BlockFetch.ClientInterface

-- | Abstract over the ChainDB
data ChainDbView m blk
ChainDbView :: STM m (AnchoredFragment (Header blk)) -> STM m (Point blk -> Bool) -> STM m MaxSlotNo -> (InvalidBlockPunishment m -> blk -> m Bool) -> ChainDbView m blk
[getCurrentChain] :: ChainDbView m blk -> STM m (AnchoredFragment (Header blk))
[getIsFetched] :: ChainDbView m blk -> STM m (Point blk -> Bool)
[getMaxSlotNo] :: ChainDbView m blk -> STM m MaxSlotNo
[addBlockWaitWrittenToDisk] :: ChainDbView m blk -> InvalidBlockPunishment m -> blk -> m Bool

-- | How to get the wall-clock time of a slot. Note that this is a very
--   non-trivial operation in the context of the HFC, cf.
--   <a>headerForgeUTCTime</a>.
type SlotForgeTimeOracle m blk = RealPoint blk -> STM m UTCTime
defaultChainDbView :: IOLike m => ChainDB m blk -> ChainDbView m blk

-- | Create a HFC-enabled <a>SlotForgeTimeOracle</a>. Note that its
--   semantics are rather tricky, cf. <a>headerForgeUTCTime</a>.
initSlotForgeTimeOracle :: forall m blk. (IOLike m, BlockSupportsProtocol blk, HasHardForkHistory blk, ConfigSupportsNode blk, IsLedger (LedgerState blk)) => TopLevelConfig blk -> ChainDB m blk -> m (SlotForgeTimeOracle m blk)
mkBlockFetchConsensusInterface :: forall m peer blk. (IOLike m, BlockSupportsProtocol blk) => BlockConfig blk -> ChainDbView m blk -> STM m (Map peer (AnchoredFragment (Header blk))) -> (Header blk -> SizeInBytes) -> SlotForgeTimeOracle m blk -> STM m FetchMode -> BlockFetchConsensusInterface peer (Header blk) blk m
readFetchModeDefault :: (MonadSTM m, HasHeader blk) => BlockchainTime m -> STM m (AnchoredFragment blk) -> STM m FetchMode


-- | Definition of common types used in
--   <a>Ouroboros.Consensus.Mempool.Init</a>,
--   <a>Ouroboros.Consensus.Mempool.Update</a> and
--   <a>Ouroboros.Consensus.Mempool.Query</a>.
module Ouroboros.Consensus.Mempool.Impl.Common

-- | Internal state in the mempool
data InternalState blk
IS :: !TxSeq (Validated (GenTx blk)) -> !Set (GenTxId blk) -> !TickedLedgerState blk -> !ChainHash blk -> !SlotNo -> !TicketNo -> !MempoolCapacityBytes -> InternalState blk

-- | Transactions currently in the mempool
--   
--   NOTE: the total size of the transactions in <a>isTxs</a> may exceed
--   the current capacity (<a>isCapacity</a>). When the capacity computed
--   from the ledger has shrunk, we don't remove transactions from the
--   Mempool to satisfy the new lower limit. We let the transactions get
--   removed in the normal way: by becoming invalid w.r.t. the updated
--   ledger state. We treat a Mempool <i>over</i> capacity in the same way
--   as a Mempool <i>at</i> capacity.
[isTxs] :: InternalState blk -> !TxSeq (Validated (GenTx blk))

-- | The cached IDs of transactions currently in the mempool.
--   
--   This allows one to more quickly lookup transactions by ID from a
--   <a>MempoolSnapshot</a> (see <a>snapshotHasTx</a>).
--   
--   This should always be in-sync with the transactions in <a>isTxs</a>.
[isTxIds] :: InternalState blk -> !Set (GenTxId blk)

-- | The cached ledger state after applying the transactions in the Mempool
--   against the chain's ledger state. New transactions will be validated
--   against this ledger.
--   
--   INVARIANT: <a>isLedgerState</a> is the ledger resulting from applying
--   the transactions in <a>isTxs</a> against the ledger identified
--   <a>isTip</a> as tip.
[isLedgerState] :: InternalState blk -> !TickedLedgerState blk

-- | The tip of the chain that <a>isTxs</a> was validated against
--   
--   This comes from the underlying ledger state
--   (<tt>tickedLedgerState</tt>)
[isTip] :: InternalState blk -> !ChainHash blk

-- | The most recent <a>SlotNo</a> that <a>isTxs</a> was validated against
--   
--   This comes from <a>applyChainTick</a> (<tt>tickedSlotNo</tt>).
[isSlotNo] :: InternalState blk -> !SlotNo

-- | The mempool <a>TicketNo</a> counter.
--   
--   See <a>vrLastTicketNo</a> for more information.
[isLastTicketNo] :: InternalState blk -> !TicketNo

-- | Current maximum capacity of the Mempool. Result of
--   <a>computeMempoolCapacity</a> using the current chain's
--   <a>TickedLedgerState</a>.
--   
--   NOTE: this does not correspond to <a>isLedgerState</a>, which is the
--   <a>TickedLedgerState</a> <i>after</i> applying the transactions in the
--   Mempool. There might be a transaction in the Mempool triggering a
--   change in the maximum transaction capacity of a block, which would
--   change the Mempool's capacity (unless overridden). We don't want the
--   Mempool's capacity to depend on its contents. The mempool is assuming
--   <i>all</i> its transactions will be in the next block. So any changes
--   caused by that block will take effect after applying it and will only
--   affect the next block.
[isCapacity] :: InternalState blk -> !MempoolCapacityBytes

-- | &lt;math&gt;. Return the number of transactions in the internal state
--   of the Mempool paired with their total size in bytes.
isMempoolSize :: InternalState blk -> MempoolSize

-- | The mempool environment captures all the associated variables wrt the
--   Mempool and is accessed by the Mempool interface on demand to perform
--   the different operations.
data MempoolEnv m blk
MempoolEnv :: LedgerInterface m blk -> LedgerConfig blk -> StrictTVar m (InternalState blk) -> MVar m () -> MVar m () -> Tracer m (TraceEventMempool blk) -> (GenTx blk -> TxSizeInBytes) -> MempoolCapacityBytesOverride -> MempoolEnv m blk
[mpEnvLedger] :: MempoolEnv m blk -> LedgerInterface m blk
[mpEnvLedgerCfg] :: MempoolEnv m blk -> LedgerConfig blk
[mpEnvStateVar] :: MempoolEnv m blk -> StrictTVar m (InternalState blk)
[mpEnvAddTxsRemoteFifo] :: MempoolEnv m blk -> MVar m ()
[mpEnvAddTxsAllFifo] :: MempoolEnv m blk -> MVar m ()
[mpEnvTracer] :: MempoolEnv m blk -> Tracer m (TraceEventMempool blk)
[mpEnvTxSize] :: MempoolEnv m blk -> GenTx blk -> TxSizeInBytes
[mpEnvCapacityOverride] :: MempoolEnv m blk -> MempoolCapacityBytesOverride
initMempoolEnv :: (IOLike m, NoThunks (GenTxId blk), LedgerSupportsMempool blk, ValidateEnvelope blk) => LedgerInterface m blk -> LedgerConfig blk -> MempoolCapacityBytesOverride -> Tracer m (TraceEventMempool blk) -> (GenTx blk -> TxSizeInBytes) -> m (MempoolEnv m blk)

-- | Abstract interface needed to run a Mempool.
data LedgerInterface m blk
LedgerInterface :: STM m (LedgerState blk) -> LedgerInterface m blk
[getCurrentLedgerState] :: LedgerInterface m blk -> STM m (LedgerState blk)

-- | Create a <a>LedgerInterface</a> from a <a>ChainDB</a>.
chainDBLedgerInterface :: (IOLike m, IsLedger (LedgerState blk)) => ChainDB m blk -> LedgerInterface m blk
data ValidationResult invalidTx blk
ValidationResult :: ChainHash blk -> SlotNo -> MempoolCapacityBytes -> TxSeq (Validated (GenTx blk)) -> Set (GenTxId blk) -> Maybe (Validated (GenTx blk)) -> TickedLedgerState blk -> [(invalidTx, ApplyTxErr blk)] -> TicketNo -> ValidationResult invalidTx blk

-- | The tip of the chain before applying these transactions
[vrBeforeTip] :: ValidationResult invalidTx blk -> ChainHash blk

-- | The slot number of the (imaginary) block the txs will be placed in
[vrSlotNo] :: ValidationResult invalidTx blk -> SlotNo

-- | Capacity of the Mempool. Corresponds to <a>vrBeforeTip</a> and
--   <tt>vrBeforeSlotNo</tt>, <i>not</i> <a>vrAfter</a>.
[vrBeforeCapacity] :: ValidationResult invalidTx blk -> MempoolCapacityBytes

-- | The transactions that were found to be valid (oldest to newest)
[vrValid] :: ValidationResult invalidTx blk -> TxSeq (Validated (GenTx blk))

-- | The cached IDs of transactions that were found to be valid (oldest to
--   newest)
[vrValidTxIds] :: ValidationResult invalidTx blk -> Set (GenTxId blk)

-- | A new transaction (not previously known) which was found to be valid.
--   
--   n.b. This will only contain a valid transaction that was <i>newly</i>
--   added to the mempool (not a previously known valid transaction).
[vrNewValid] :: ValidationResult invalidTx blk -> Maybe (Validated (GenTx blk))

-- | The state of the ledger after applying <a>vrValid</a> against the
--   ledger state identifeid by <a>vrBeforeTip</a>.
[vrAfter] :: ValidationResult invalidTx blk -> TickedLedgerState blk

-- | The transactions that were invalid, along with their errors
--   
--   From oldest to newest.
[vrInvalid] :: ValidationResult invalidTx blk -> [(invalidTx, ApplyTxErr blk)]

-- | The mempool <a>TicketNo</a> counter.
--   
--   When validating new transactions, this should be incremented, starting
--   from <a>isLastTicketNo</a> of the <a>InternalState</a>. When
--   validating previously applied transactions, this field should not be
--   affected.
[vrLastTicketNo] :: ValidationResult invalidTx blk -> TicketNo

-- | Extend <a>ValidationResult</a> with a new transaction (one which we
--   have not previously validated) that may or may not be valid in this
--   ledger state.
--   
--   PRECONDITION: <a>vrNewValid</a> is <a>Nothing</a>. In other words: new
--   transactions should be validated one-by-one, not by calling
--   <a>extendVRNew</a> on its result again.
extendVRNew :: (LedgerSupportsMempool blk, HasTxId (GenTx blk)) => LedgerConfig blk -> (GenTx blk -> TxSizeInBytes) -> WhetherToIntervene -> GenTx blk -> ValidationResult (GenTx blk) blk -> (Either (ApplyTxErr blk) (Validated (GenTx blk)), ValidationResult (GenTx blk) blk)

-- | Extend <a>ValidationResult</a> with a previously validated transaction
--   that may or may not be valid in this ledger state
--   
--   n.b. Even previously validated transactions may not be valid in a
--   different ledger state; it is <i>still</i> useful to indicate whether
--   we have previously validated this transaction because, if we have, we
--   can utilize <a>reapplyTx</a> rather than <a>applyTx</a> and,
--   therefore, skip things like cryptographic signatures.
extendVRPrevApplied :: (LedgerSupportsMempool blk, HasTxId (GenTx blk)) => LedgerConfig blk -> TxTicket (Validated (GenTx blk)) -> ValidationResult (Validated (GenTx blk)) blk -> ValidationResult (Validated (GenTx blk)) blk

-- | Revalidate the given transactions (<tt>[<a>TxTicket</a> (<a>GenTx</a>
--   blk)]</tt>), which are <i>all</i> the transactions in the Mempool
--   against the given ticked ledger state, which corresponds to the
--   chain's ledger state.
revalidateTxsFor :: (LedgerSupportsMempool blk, HasTxId (GenTx blk)) => MempoolCapacityBytesOverride -> LedgerConfig blk -> SlotNo -> TickedLedgerState blk -> TicketNo -> [TxTicket (Validated (GenTx blk))] -> ValidationResult (Validated (GenTx blk)) blk

-- | Given a (valid) internal state, validate it against the given ledger
--   state and <tt>BlockSlot</tt>.
--   
--   When these match the internal state's <a>isTip</a> and
--   <a>isSlotNo</a>, this is very cheap, as the given internal state will
--   already be valid against the given inputs.
--   
--   When these don't match, the transaction in the internal state will be
--   revalidated (<a>revalidateTxsFor</a>).
validateStateFor :: (LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => MempoolCapacityBytesOverride -> LedgerConfig blk -> ForgeLedgerState blk -> InternalState blk -> ValidationResult (Validated (GenTx blk)) blk

-- | Events traced by the Mempool.
data TraceEventMempool blk
TraceMempoolAddedTx :: Validated (GenTx blk) -> MempoolSize -> MempoolSize -> TraceEventMempool blk
TraceMempoolRejectedTx :: GenTx blk -> ApplyTxErr blk -> MempoolSize -> TraceEventMempool blk
TraceMempoolRemoveTxs :: [(Validated (GenTx blk), ApplyTxErr blk)] -> MempoolSize -> TraceEventMempool blk
TraceMempoolManuallyRemovedTxs :: [GenTxId blk] -> [Validated (GenTx blk)] -> MempoolSize -> TraceEventMempool blk

-- | Construct internal state from <a>ValidationResult</a>
--   
--   Discards information about invalid and newly valid transactions
internalStateFromVR :: ValidationResult invalidTx blk -> InternalState blk

-- | Create a Mempool Snapshot from a given Internal State of the mempool.
snapshotFromIS :: HasTxId (GenTx blk) => InternalState blk -> MempoolSnapshot blk

-- | Construct a <a>ValidationResult</a> from internal state.
validationResultFromIS :: InternalState blk -> ValidationResult invalidTx blk

-- | Tick the <a>LedgerState</a> using the given <tt>BlockSlot</tt>.
tickLedgerState :: forall blk. (UpdateLedger blk, ValidateEnvelope blk) => LedgerConfig blk -> ForgeLedgerState blk -> (SlotNo, TickedLedgerState blk)
instance GHC.Generics.Generic (Ouroboros.Consensus.Mempool.Impl.Common.InternalState blk)
instance (NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)), NoThunks.Class.NoThunks (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk), NoThunks.Class.NoThunks (Ouroboros.Consensus.Ticked.Ticked (Ouroboros.Consensus.Ledger.Basics.LedgerState blk)), Ouroboros.Network.Block.StandardHash blk, Data.Typeable.Internal.Typeable blk) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Mempool.Impl.Common.InternalState blk)
instance (GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk), GHC.Classes.Eq (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)), GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk), GHC.Classes.Eq (Ouroboros.Consensus.Ledger.SupportsMempool.ApplyTxErr blk)) => GHC.Classes.Eq (Ouroboros.Consensus.Mempool.Impl.Common.TraceEventMempool blk)
instance (GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk), GHC.Show.Show (Ouroboros.Consensus.Ledger.Abstract.Validated (Ouroboros.Consensus.Ledger.SupportsMempool.GenTx blk)), GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.GenTxId blk), GHC.Show.Show (Ouroboros.Consensus.Ledger.SupportsMempool.ApplyTxErr blk)) => GHC.Show.Show (Ouroboros.Consensus.Mempool.Impl.Common.TraceEventMempool blk)


-- | Operations that update the mempool. They are internally divided in the
--   pure and impure sides of the operation.
module Ouroboros.Consensus.Mempool.Update

-- | Add a single transaction to the mempool, blocking if there is no
--   space.
implAddTx :: (MonadSTM m, MonadMVar m, LedgerSupportsMempool blk, HasTxId (GenTx blk)) => StrictTVar m (InternalState blk) -> MVar m () -> MVar m () -> LedgerConfig blk -> (GenTx blk -> TxSizeInBytes) -> Tracer m (TraceEventMempool blk) -> AddTxOnBehalfOf -> GenTx blk -> m (MempoolAddTxResult blk)

-- | See <a>removeTxs</a>.
implRemoveTxs :: (IOLike m, LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => MempoolEnv m blk -> [GenTxId blk] -> m ()

-- | See <a>syncWithLedger</a>.
implSyncWithLedger :: (IOLike m, LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => MempoolEnv m blk -> m (MempoolSnapshot blk)


-- | Queries to the mempool
module Ouroboros.Consensus.Mempool.Query

-- | Get a snapshot of the mempool state that is valid with respect to the
--   given ledger state
pureGetSnapshotFor :: (LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => LedgerConfig blk -> ForgeLedgerState blk -> MempoolCapacityBytesOverride -> InternalState blk -> MempoolSnapshot blk


-- | Creating a mempool
module Ouroboros.Consensus.Mempool.Init

-- | Create a <tt>Mempool m blk</tt> in <tt>m</tt> to manipulate the
--   mempool. It will also fork a thread that syncs the mempool and the
--   ledger when the ledger changes.
openMempool :: (IOLike m, LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => ResourceRegistry m -> LedgerInterface m blk -> LedgerConfig blk -> MempoolCapacityBytesOverride -> Tracer m (TraceEventMempool blk) -> (GenTx blk -> TxSizeInBytes) -> m (Mempool m blk)

-- | Unlike <a>openMempool</a>, this function does not fork a background
--   thread that synchronises with the ledger state whenever the later
--   changes.
--   
--   Intended for testing purposes.
openMempoolWithoutSyncThread :: (IOLike m, LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => LedgerInterface m blk -> LedgerConfig blk -> MempoolCapacityBytesOverride -> Tracer m (TraceEventMempool blk) -> (GenTx blk -> TxSizeInBytes) -> m (Mempool m blk)


-- | The mempool API and implementation.
module Ouroboros.Consensus.Mempool

-- | Mempool
--   
--   The mempool is the set of transactions that should be included in the
--   next block. In principle this is a <i>set</i> of all the transactions
--   that we receive from our peers. In order to avoid flooding the network
--   with invalid transactions, however, we only want to keep <i>valid</i>
--   transactions in the mempool. That raises the question: valid with
--   respect to which ledger state?
--   
--   We opt for a very simple answer to this: the mempool will be
--   interpreted as a <i>list</i> of transactions; which are validated
--   strictly in order, starting from the current ledger state. This has a
--   number of advantages:
--   
--   <ul>
--   <li>It's simple to implement and it's efficient. In particular, no
--   search for a valid subset is ever required.</li>
--   <li>When producing a block, we can simply take the longest possible
--   prefix of transactions that fits in a block.</li>
--   <li>It supports wallets that submit dependent transactions (where
--   later transaction depends on outputs from earlier ones).</li>
--   </ul>
--   
--   The mempool provides fairness guarantees for the case of multiple
--   threads performing <a>addTx</a> concurrently. Implementations of this
--   interface must provide this guarantee, and users of this interface may
--   rely on it. Specifically, multiple threads that continuously use
--   <a>addTx</a> will, over time, get a share of the mempool resource
--   (measured by the number of txs only, not their sizes) roughly
--   proportional to their "weight". The weight depends on the
--   <a>AddTxOnBehalfOf</a>: either acting on behalf of remote peers
--   (<a>AddTxForRemotePeer</a>) or on behalf of a local client
--   (<a>AddTxForLocalClient</a>). The weighting for threads acting on
--   behalf of remote peers is the same for all remote peers, so all remote
--   peers will get a roughly equal share of the resource. The weighting
--   for local clients is the same for all local clients but <i>may</i> be
--   higher than the weighting for remote peers. The weighting is not
--   unboundedly higher however, so there is still (weighted) fairness
--   between remote peers and local clients. Thus local clients will also
--   get a roughly equal share of the resource, but that share may be
--   strictly greater than the share for each remote peer. Furthermore,
--   this implies local clients cannot starve remote peers, despite their
--   higher weighting.
--   
--   This fairness specification in terms of weighting is deliberately
--   non-specific, which allows multiple strategies. The existing default
--   strategy (for the implementation in
--   <a>Ouroboros.Consensus.Mempool</a>) is as follows. The design uses two
--   FIFOs, to give strictly in-order behaviour. All remote peers get equal
--   weight and all local clients get equal weight. The relative weight
--   between remote and local is that if there are N remote peers and M
--   local clients, each local client gets weight 1/(M+1), while all of the
--   N remote peers together also get total weight 1/(M+1). This means
--   individual remote peers get weight 1/(N * (M+1)). Intuitively: a
--   single local client has the same weight as all the remote peers put
--   together.
data Mempool m blk
Mempool :: (AddTxOnBehalfOf -> GenTx blk -> m (MempoolAddTxResult blk)) -> ([GenTxId blk] -> m ()) -> m (MempoolSnapshot blk) -> STM m (MempoolSnapshot blk) -> (ForgeLedgerState blk -> STM m (MempoolSnapshot blk)) -> STM m MempoolCapacityBytes -> (GenTx blk -> TxSizeInBytes) -> Mempool m blk

-- | Add a single transaction to the mempool.
--   
--   The new transaction provided will be validated, <i>in order</i>,
--   against the ledger state obtained by applying all the transactions
--   already in the Mempool to it. Transactions which are found to be
--   invalid, with respect to the ledger state, are dropped, whereas valid
--   transactions are added to the mempool.
--   
--   Note that transactions that are invalid, with respect to the ledger
--   state, will <i>never</i> be added to the mempool. However, it is
--   possible that, at a given point in time, transactions which were once
--   valid but are now invalid, with respect to the current ledger state,
--   could exist within the mempool until they are revalidated and dropped
--   from the mempool via a call to <a>syncWithLedger</a> or by the
--   background thread that watches the ledger for changes.
--   
--   This action returns one of two results
--   
--   <ul>
--   <li>A <a>MempoolTxAdded</a> value if the transaction provided was
--   found to be valid. This transactions is now in the Mempool.</li>
--   <li>A <a>MempoolTxRejected</a> value if the transaction provided was
--   found to be invalid, along with its accompanying validation errors.
--   This transactions is not in the Mempool.</li>
--   </ul>
--   
--   Note that this is a blocking action. It will block until the
--   transaction fits into the mempool. This includes transactions that
--   turn out to be invalid: the action waits for there to be space for the
--   transaction before it gets validated.
--   
--   Note that it is safe to use this from multiple threads concurrently.
--   
--   POSTCONDITION: &gt; let prj = case &gt; MempoolTxAdded vtx -&gt;
--   txForgetValidated vtx &gt; MempoolTxRejected tx _err -&gt; tx &gt;
--   processed &lt;- addTx wti txs &gt; prj processed == tx
--   
--   Note that previously valid transaction that are now invalid with
--   respect to the current ledger state are dropped from the mempool, but
--   are not part of the first returned list (nor the second).
--   
--   In principle it is possible that validation errors are transient; for
--   example, it is possible that a transaction is rejected because one of
--   its inputs is not <i>yet</i> available in the UTxO (the transaction it
--   depends on is not yet in the chain, nor in the mempool). In practice
--   however it is likely that rejected transactions will still be rejected
--   later, and should just be dropped.
--   
--   It is important to note one important special case of transactions
--   being "invalid": a transaction will <i>also</i> be considered invalid
--   if <i>that very same transaction</i> is already included on the
--   blockchain (after all, by definition that must mean its inputs have
--   been used). Rejected transactions are therefore not necessarily a sign
--   of malicious behaviour. Indeed, we would expect <i>most</i>
--   transactions that are reported as invalid by <tt>tryAddTxs</tt> to be
--   invalid precisely because they have already been included.
--   Distinguishing between these two cases can be done in theory, but it
--   is expensive unless we have an index of transaction hashes that have
--   been included on the blockchain.
--   
--   As long as we keep the mempool entirely in-memory this could live in
--   <tt>STM m</tt>; we keep it in <tt>m</tt> instead to leave open the
--   possibility of persistence.
[addTx] :: Mempool m blk -> AddTxOnBehalfOf -> GenTx blk -> m (MempoolAddTxResult blk)

-- | Manually remove the given transactions from the mempool.
[removeTxs] :: Mempool m blk -> [GenTxId blk] -> m ()

-- | Sync the transactions in the mempool with the current ledger state of
--   the <tt>ChainDB</tt>.
--   
--   The transactions that exist within the mempool will be revalidated
--   against the current ledger state. Transactions which are found to be
--   invalid with respect to the current ledger state, will be dropped from
--   the mempool, whereas valid transactions will remain.
--   
--   We keep this in <tt>m</tt> instead of <tt>STM m</tt> to leave open the
--   possibility of persistence. Additionally, this makes it possible to
--   trace the removal of invalid transactions.
--   
--   n.b. in our current implementation, when one opens a mempool, we spawn
--   a thread which performs this action whenever the <tt>ChainDB</tt> tip
--   point changes.
[syncWithLedger] :: Mempool m blk -> m (MempoolSnapshot blk)

-- | Get a snapshot of the current mempool state. This allows for further
--   pure queries on the snapshot.
--   
--   This doesn't look at the ledger state at all.
[getSnapshot] :: Mempool m blk -> STM m (MempoolSnapshot blk)

-- | Get a snapshot of the mempool state that is valid with respect to the
--   given ledger state
--   
--   This does not update the state of the mempool.
[getSnapshotFor] :: Mempool m blk -> ForgeLedgerState blk -> STM m (MempoolSnapshot blk)

-- | Get the mempool's capacity in bytes.
--   
--   Note that the capacity of the Mempool, unless it is overridden with
--   <tt>MempoolCapacityBytesOverride</tt>, can dynamically change when the
--   ledger state is updated: it will be set to twice the current ledger's
--   maximum transaction capacity of a block.
--   
--   When the capacity happens to shrink at some point, we <i>do not</i>
--   remove transactions from the Mempool to satisfy this new lower limit.
--   Instead, we treat it the same way as a Mempool which is <i>at</i>
--   capacity, i.e., we won't admit new transactions until some have been
--   removed because they have become invalid.
[getCapacity] :: Mempool m blk -> STM m MempoolCapacityBytes

-- | Return the post-serialisation size in bytes of a <a>GenTx</a>.
[getTxSize] :: Mempool m blk -> GenTx blk -> TxSizeInBytes

-- | The result of attempting to add a transaction to the mempool.
data MempoolAddTxResult blk

-- | The transaction was added to the mempool.
MempoolTxAdded :: !Validated (GenTx blk) -> MempoolAddTxResult blk

-- | The transaction was rejected and could not be added to the mempool for
--   the specified reason.
MempoolTxRejected :: !GenTx blk -> !ApplyTxErr blk -> MempoolAddTxResult blk

-- | A wrapper around <a>addTx</a> that adds a sequence of transactions on
--   behalf of a local client. This reports more errors for local clients,
--   see <a>Intervene</a>.
--   
--   Note that transactions are added one by one, and can interleave with
--   other concurrent threads using <a>addTx</a>.
--   
--   See <a>addTx</a> for further details.
addLocalTxs :: forall m blk. MonadSTM m => Mempool m blk -> [GenTx blk] -> m [MempoolAddTxResult blk]

-- | A wrapper around <a>addTx</a> that adds a sequence of transactions on
--   behalf of a remote peer.
--   
--   Note that transactions are added one by one, and can interleave with
--   other concurrent threads using <a>addTx</a>.
--   
--   See <a>addTx</a> for further details.
addTxs :: forall m blk. MonadSTM m => Mempool m blk -> [GenTx blk] -> m [MempoolAddTxResult blk]
isMempoolTxAdded :: MempoolAddTxResult blk -> Bool
isMempoolTxRejected :: MempoolAddTxResult blk -> Bool
mempoolTxAddedToMaybe :: MempoolAddTxResult blk -> Maybe (Validated (GenTx blk))

-- | The ledger state wrt to which we should produce a block
--   
--   The transactions in the mempool will be part of the body of a block,
--   but a block consists of a header and a body, and the full validation
--   of a block consists of first processing its header and only then
--   processing the body. This is important, because processing the header
--   may change the state of the ledger: the update system might be
--   updated, scheduled delegations might be applied, etc., and such
--   changes should take effect before we validate any transactions.
data ForgeLedgerState blk

-- | The slot number of the block is known
--   
--   This will only be the case when we realized that we are the slot
--   leader and we are actually producing a block. It is the caller's
--   responsibility to call <a>applyChainTick</a> and produce the ticked
--   ledger state.
ForgeInKnownSlot :: SlotNo -> TickedLedgerState blk -> ForgeLedgerState blk

-- | The slot number of the block is not yet known
--   
--   When we are validating transactions before we know in which block they
--   will end up, we have to make an assumption about which slot number to
--   use for <a>applyChainTick</a> to prepare the ledger state; we will
--   assume that they will end up in the slot after the slot at the tip of
--   the ledger.
ForgeInUnknownSlot :: LedgerState blk -> ForgeLedgerState blk

-- | A pure snapshot of the contents of the mempool. It allows fetching
--   information about transactions in the mempool, and fetching individual
--   transactions.
--   
--   This uses a transaction sequence number type for identifying
--   transactions within the mempool sequence. The sequence number is local
--   to this mempool, unlike the transaction hash. This allows us to ask
--   for all transactions after a known sequence number, to get new
--   transactions. It is also used to look up individual transactions.
--   
--   Note that it is expected that <tt>getTx</tt> will often return
--   <a>Nothing</a> even for tx sequence numbers returned in previous
--   snapshots. This happens when the transaction has been removed from the
--   mempool between snapshots.
data MempoolSnapshot blk
MempoolSnapshot :: [(Validated (GenTx blk), TicketNo)] -> (TicketNo -> [(Validated (GenTx blk), TicketNo)]) -> (TicketNo -> Maybe (Validated (GenTx blk))) -> (GenTxId blk -> Bool) -> MempoolSize -> SlotNo -> TickedLedgerState blk -> MempoolSnapshot blk

-- | Get all transactions (oldest to newest) in the mempool snapshot along
--   with their ticket number.
[snapshotTxs] :: MempoolSnapshot blk -> [(Validated (GenTx blk), TicketNo)]

-- | Get all transactions (oldest to newest) in the mempool snapshot, along
--   with their ticket number, which are associated with a ticket number
--   greater than the one provided.
[snapshotTxsAfter] :: MempoolSnapshot blk -> TicketNo -> [(Validated (GenTx blk), TicketNo)]

-- | Get a specific transaction from the mempool snapshot by its ticket
--   number, if it exists.
[snapshotLookupTx] :: MempoolSnapshot blk -> TicketNo -> Maybe (Validated (GenTx blk))

-- | Determine whether a specific transaction exists within the mempool
--   snapshot.
[snapshotHasTx] :: MempoolSnapshot blk -> GenTxId blk -> Bool

-- | Get the size of the mempool snapshot.
[snapshotMempoolSize] :: MempoolSnapshot blk -> MempoolSize

-- | The block number of the "virtual block" under construction
[snapshotSlotNo] :: MempoolSnapshot blk -> SlotNo

-- | The ledger state after all transactions in the snapshot
[snapshotLedgerState] :: MempoolSnapshot blk -> TickedLedgerState blk

-- | We allocate each transaction a (monotonically increasing) ticket
--   number as it enters the mempool.
data TicketNo
type TxSizeInBytes = Word32

-- | The transaction ticket number from which our counter starts.
zeroTicketNo :: TicketNo

-- | Represents the maximum number of bytes worth of transactions that a
--   <tt>Mempool</tt> can contain.
newtype MempoolCapacityBytes
MempoolCapacityBytes :: Word32 -> MempoolCapacityBytes
[getMempoolCapacityBytes] :: MempoolCapacityBytes -> Word32

-- | An override for the default <a>MempoolCapacityBytes</a> which is 2x
--   the maximum transaction capacity
data MempoolCapacityBytesOverride

-- | Use 2x the maximum transaction capacity of a block. This will change
--   dynamically with the protocol parameters adopted in the current
--   ledger.
NoMempoolCapacityBytesOverride :: MempoolCapacityBytesOverride

-- | Use the following <a>MempoolCapacityBytes</a>.
MempoolCapacityBytesOverride :: !MempoolCapacityBytes -> MempoolCapacityBytesOverride

-- | If no override is provided, calculate the default mempool capacity as
--   2x the current ledger's maximum transaction capacity of a block.
computeMempoolCapacity :: LedgerSupportsMempool blk => TickedLedgerState blk -> MempoolCapacityBytesOverride -> MempoolCapacityBytes

-- | The size of a mempool.
data MempoolSize
MempoolSize :: !Word32 -> !Word32 -> MempoolSize

-- | The number of transactions in the mempool.
[msNumTxs] :: MempoolSize -> !Word32

-- | The summed byte size of all the transactions in the mempool.
[msNumBytes] :: MempoolSize -> !Word32
newtype ByteSize
ByteSize :: Word32 -> ByteSize
[unByteSize] :: ByteSize -> Word32

-- | Each block has its limits of how many transactions it can hold. That
--   limit is compared against the sum of measurements taken of each of the
--   transactions in that block.
--   
--   How we measure the transaction depends of the era that this
--   transaction belongs to (more specifically it depends on the block type
--   to which this transaction will be added). For initial eras (like Byron
--   and initial generations of Shelley based eras) this measure was simply
--   a ByteSize (block could not be bigger then given size - in bytes -
--   specified by the ledger state). In future eras (starting with Alonzo)
--   this measure was a bit more complex as it had to take other factors
--   into account (like execution units). For details please see the
--   individual instances for the TxLimits.
class BoundedMeasure (TxMeasure blk) => TxLimits blk where {
    type TxMeasure blk;
}

-- | What is the measure an individual tx?
txMeasure :: TxLimits blk => Validated (GenTx blk) -> TxMeasure blk

-- | What is the allowed capacity for txs in an individual block?
txsBlockCapacity :: TxLimits blk => Ticked (LedgerState blk) -> TxMeasure blk

-- | An override that lowers a capacity limit
--   
--   Specifically, we use this override to let the node operator limit the
--   total <a>TxMeasure</a> of transactions in blocks even more severely
--   than would the ledger state's <a>txsBlockCapacity</a>. The forge logic
--   will use the <a>min</a> (ie the lattice's <tt>meet</tt> operator) to
--   combine this override with the capacity given by the ledger state.
--   More concretely, that will typically be a componentwise minimum
--   operation, along each of the components/dimensions of
--   <tt><a>TxMeasure</a> blk</tt>.
--   
--   This newtype wrapper distinguishes the intention of this particular
--   <a>TxMeasure</a> as such an override. We use <a>TxMeasure</a> in
--   different ways in this code base. The newtype also allows us to
--   distinguish the one most appropriate monoid among many offered by the
--   <a>TxLimits</a> superclass constraints: it is the monoid induced by
--   the bounded meet-semilattice (see <a>BoundedMeasure</a>) that is
--   relevant to the notion of <i>overriding</i> the ledger's block
--   capacity.
data TxOverrides blk

-- | Apply the override
applyOverrides :: TxLimits blk => TxOverrides blk -> TxMeasure blk -> TxMeasure blk
getOverrides :: TxOverrides blk -> TxMeasure blk

-- | Smart constructor for <tt>Overrides</tt>.
mkOverrides :: TxMeasure blk -> TxOverrides blk

-- | <pre>
--   <a>applyOverrides</a> <tt>noOverrides</tt> m = m
--   </pre>
noOverridesMeasure :: BoundedMeasure a => a

-- | Create a <tt>Mempool m blk</tt> in <tt>m</tt> to manipulate the
--   mempool. It will also fork a thread that syncs the mempool and the
--   ledger when the ledger changes.
openMempool :: (IOLike m, LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => ResourceRegistry m -> LedgerInterface m blk -> LedgerConfig blk -> MempoolCapacityBytesOverride -> Tracer m (TraceEventMempool blk) -> (GenTx blk -> TxSizeInBytes) -> m (Mempool m blk)

-- | Unlike <a>openMempool</a>, this function does not fork a background
--   thread that synchronises with the ledger state whenever the later
--   changes.
--   
--   Intended for testing purposes.
openMempoolWithoutSyncThread :: (IOLike m, LedgerSupportsMempool blk, HasTxId (GenTx blk), ValidateEnvelope blk) => LedgerInterface m blk -> LedgerConfig blk -> MempoolCapacityBytesOverride -> Tracer m (TraceEventMempool blk) -> (GenTx blk -> TxSizeInBytes) -> m (Mempool m blk)

-- | Abstract interface needed to run a Mempool.
data LedgerInterface m blk
LedgerInterface :: STM m (LedgerState blk) -> LedgerInterface m blk
[getCurrentLedgerState] :: LedgerInterface m blk -> STM m (LedgerState blk)

-- | Create a <a>LedgerInterface</a> from a <a>ChainDB</a>.
chainDBLedgerInterface :: (IOLike m, IsLedger (LedgerState blk)) => ChainDB m blk -> LedgerInterface m blk

-- | Events traced by the Mempool.
data TraceEventMempool blk
TraceMempoolAddedTx :: Validated (GenTx blk) -> MempoolSize -> MempoolSize -> TraceEventMempool blk
TraceMempoolRejectedTx :: GenTx blk -> ApplyTxErr blk -> MempoolSize -> TraceEventMempool blk
TraceMempoolRemoveTxs :: [(Validated (GenTx blk), ApplyTxErr blk)] -> MempoolSize -> TraceEventMempool blk
TraceMempoolManuallyRemovedTxs :: [GenTxId blk] -> [Validated (GenTx blk)] -> MempoolSize -> TraceEventMempool blk

module Ouroboros.Consensus.MiniProtocol.LocalTxMonitor.Server

-- | Local transaction monitoring server, for inspecting the mempool.
localTxMonitorServer :: forall blk m. (MonadSTM m, LedgerSupportsMempool blk) => Mempool m blk -> LocalTxMonitorServer (GenTxId blk) (GenTx blk) SlotNo m ()


-- | PBFT chain state
--   
--   Intended for qualified import.
module Ouroboros.Consensus.Protocol.PBFT.State

-- | Slot and corresponding genesis key
data PBftSigner c
PBftSigner :: !SlotNo -> !PBftVerKeyHash c -> PBftSigner c
[pbftSignerSlotNo] :: PBftSigner c -> !SlotNo
[pbftSignerGenesisKey] :: PBftSigner c -> !PBftVerKeyHash c

-- | PBFT state
--   
--   For a window size of <tt>n</tt>, the PBFT chain state is a sequence of
--   signatures over the last <tt>n</tt> slots
--   
--   <pre>
--   +-------------------------------------------+
--   |                signatures                 |
--   +-------------------------------------------+
--   
--   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
--                    window of n
--   </pre>
--   
--   We need the last <tt>n</tt> signatures to verify that no single key
--   has signed more than a certain threshold percentage of the slots.
--   
--   When near genesis, we will have less than <tt>n</tt> signatures in the
--   history.
--   
--   The window size itself is pretty much arbitrary and will be fixed by a
--   particular blockchain specification (e.g., Byron).
data PBftState c
PBftState :: !StrictSeq (PBftSigner c) -> !Map (PBftVerKeyHash c) Word64 -> PBftState c

-- | Signatures in the window
--   
--   We should have precisely <tt>n</tt> signatures in the window, unless
--   we are near genesis.
--   
--   INVARIANT Empty if and only if we are exactly at genesis.
[inWindow] :: PBftState c -> !StrictSeq (PBftSigner c)

-- | Cached counts of the signatures in the window
[counts] :: PBftState c -> !Map (PBftVerKeyHash c) Word64

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type

-- | Window size
--   
--   See <a>PBftState</a> itself for a detailed discussion on the window
--   size versus the number of signatures.
newtype WindowSize
WindowSize :: Word64 -> WindowSize
[getWindowSize] :: WindowSize -> Word64

-- | Append new signature
--   
--   Drops the oldest signature, provided we have reached the required
--   number.
append :: forall c. PBftCrypto c => WindowSize -> PBftSigner c -> PBftState c -> PBftState c

-- | Empty PBFT chain state
--   
--   In other words, the PBFT chain state corresponding to genesis.
empty :: PBftState c

-- | Number of signatures in the window
--   
--   This will be equal to the specified window size, unless near genesis
countSignatures :: PBftState c -> Word64

-- | The number of blocks signed by the specified genesis key
--   
--   This only considers the signatures within the window, not in the
--   pre-window; see <a>PBftState</a> for detailed discussion.
countSignedBy :: PBftCrypto c => PBftState c -> PBftVerKeyHash c -> Word64

-- | The last (most recent) signed slot in the window
--   
--   Returns <a>Origin</a> if there are no signatures in the window (this
--   will happen exactly at genesis only).
--   
--   Unaffected by EBBs, since they're not signed.
lastSignedSlot :: PBftState c -> WithOrigin SlotNo

-- | Note: we are not checking the invariants because we don't want to
--   require the <a>WindowSize</a> to be in the context, see #2383. When
--   assertions are enabled, we would notice the invariant violation as
--   soon as we <a>append</a>.
--   
--   PRECONDITION: the slots of the signers are in ascending order.
fromList :: PBftCrypto c => [PBftSigner c] -> PBftState c
toList :: PBftState c -> [PBftSigner c]
decodePBftState :: forall c. (PBftCrypto c, Serialise (PBftVerKeyHash c)) => forall s. Decoder s (PBftState c)
encodePBftState :: (PBftCrypto c, Serialise (PBftVerKeyHash c)) => PBftState c -> Encoding
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.State.PBftSigner c)
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.State.PBftState c)
instance GHC.Real.Integral Ouroboros.Consensus.Protocol.PBFT.State.WindowSize
instance GHC.Real.Real Ouroboros.Consensus.Protocol.PBFT.State.WindowSize
instance GHC.Num.Num Ouroboros.Consensus.Protocol.PBFT.State.WindowSize
instance GHC.Enum.Enum Ouroboros.Consensus.Protocol.PBFT.State.WindowSize
instance GHC.Classes.Ord Ouroboros.Consensus.Protocol.PBFT.State.WindowSize
instance GHC.Classes.Eq Ouroboros.Consensus.Protocol.PBFT.State.WindowSize
instance GHC.Show.Show Ouroboros.Consensus.Protocol.PBFT.State.WindowSize
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Show.Show (Ouroboros.Consensus.Protocol.PBFT.State.PBftState c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Classes.Eq (Ouroboros.Consensus.Protocol.PBFT.State.PBftState c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.State.PBftState c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Show.Show (Ouroboros.Consensus.Protocol.PBFT.State.PBftSigner c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Classes.Eq (Ouroboros.Consensus.Protocol.PBFT.State.PBftSigner c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.State.PBftSigner c)
instance Codec.Serialise.Class.Serialise (Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftVerKeyHash c) => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.Protocol.PBFT.State.PBftSigner c)

module Ouroboros.Consensus.Protocol.PBFT

-- | Permissive BFT
--   
--   As defined in
--   <a>https://hydra.iohk.io/job/Cardano/cardano-ledger-specs/byronChainSpec/latest/download-by-type/doc-pdf/blockchain-spec</a>
data PBft c

-- | If we are a core node (i.e. a block producing node) we know which core
--   node we are, and we have the operational key pair and delegation
--   certificate.
data PBftCanBeLeader c
PBftCanBeLeader :: !CoreNodeId -> !SignKeyDSIGN (PBftDSIGN c) -> !PBftDelegationCert c -> PBftCanBeLeader c
[pbftCanBeLeaderCoreNodeId] :: PBftCanBeLeader c -> !CoreNodeId
[pbftCanBeLeaderSignKey] :: PBftCanBeLeader c -> !SignKeyDSIGN (PBftDSIGN c)
[pbftCanBeLeaderDlgCert] :: PBftCanBeLeader c -> !PBftDelegationCert c
data PBftFields c toSign
PBftFields :: VerKeyDSIGN (PBftDSIGN c) -> VerKeyDSIGN (PBftDSIGN c) -> SignedDSIGN (PBftDSIGN c) toSign -> PBftFields c toSign

-- | The actual issuer of a block
[pbftIssuer] :: PBftFields c toSign -> VerKeyDSIGN (PBftDSIGN c)

-- | The stakeholder on whose behalf the block is being issued
[pbftGenKey] :: PBftFields c toSign -> VerKeyDSIGN (PBftDSIGN c)
[pbftSignature] :: PBftFields c toSign -> SignedDSIGN (PBftDSIGN c) toSign

-- | Information required to produce a block.
data PBftIsLeader c
PBftIsLeader :: !SignKeyDSIGN (PBftDSIGN c) -> !PBftDelegationCert c -> PBftIsLeader c
[pbftIsLeaderSignKey] :: PBftIsLeader c -> !SignKeyDSIGN (PBftDSIGN c)
[pbftIsLeaderDlgCert] :: PBftIsLeader c -> !PBftDelegationCert c
newtype PBftLedgerView c
PBftLedgerView :: Bimap (PBftVerKeyHash c) (PBftVerKeyHash c) -> PBftLedgerView c

-- | ProtocolParameters: map from genesis to delegate keys.
[pbftDelegates] :: PBftLedgerView c -> Bimap (PBftVerKeyHash c) (PBftVerKeyHash c)

-- | Protocol parameters
data PBftParams
PBftParams :: !SecurityParam -> !NumCoreNodes -> !PBftSignatureThreshold -> PBftParams

-- | Security parameter
--   
--   Although the protocol proper does not have such a security parameter,
--   we insist on it.
[pbftSecurityParam] :: PBftParams -> !SecurityParam

-- | Number of core nodes
[pbftNumNodes] :: PBftParams -> !NumCoreNodes

-- | Signature threshold
--   
--   This bounds the proportion of the latest <a>pbftSecurityParam</a>-many
--   blocks which is allowed to be signed by any single key. The protocol
--   proper is parameterized over the size of this window of recent blocks,
--   but this implementation follows the specification by fixing that
--   parameter to the ambient security parameter <tt>k</tt>.
[pbftSignatureThreshold] :: PBftParams -> !PBftSignatureThreshold

-- | Part of the header required for chain selection
--   
--   EBBs share a block number with regular blocks, and so for chain
--   selection we need to know if a block is an EBB or not (because a chain
--   ending on an EBB with a particular block number is longer than a chain
--   on a regular block with that same block number).
data PBftSelectView
PBftSelectView :: BlockNo -> IsEBB -> PBftSelectView
[pbftSelectViewBlockNo] :: PBftSelectView -> BlockNo
[pbftSelectViewIsEBB] :: PBftSelectView -> IsEBB

-- | Signature threshold. This represents the proportion of blocks in a
--   <tt>pbftSignatureWindow</tt>-sized window which may be signed by any
--   single key.
newtype PBftSignatureThreshold
PBftSignatureThreshold :: Double -> PBftSignatureThreshold
[getPBftSignatureThreshold] :: PBftSignatureThreshold -> Double
mkPBftSelectView :: GetHeader blk => Header blk -> PBftSelectView

-- | Does the number of blocks signed by this key exceed the threshold?
--   
--   Returns <tt>Just</tt> the number of blocks signed if exceeded.
pbftWindowExceedsThreshold :: PBftCrypto c => PBftWindowParams -> PBftState c -> PBftVerKeyHash c -> Either Word64 ()

-- | Window size used by PBFT
--   
--   We set the window size to be equal to k.
pbftWindowSize :: SecurityParam -> WindowSize
forgePBftFields :: forall c toSign. (PBftCrypto c, Signable (PBftDSIGN c) toSign) => (VerKeyDSIGN (PBftDSIGN c) -> ContextDSIGN (PBftDSIGN c)) -> IsLeader (PBft c) -> toSign -> PBftFields c toSign

-- | Crypto primitives required by BFT
--   
--   Cardano stores a map of stakeholder IDs rather than the verification
--   key directly. We make this family injective for convenience - whilst
--   it's _possible_ that there could be non-injective instances, the
--   chances of there being more than the two instances here are basically
--   non-existent.
class (Typeable c, DSIGNAlgorithm (PBftDSIGN c), Condense (SigDSIGN (PBftDSIGN c)), Show (PBftVerKeyHash c), Ord (PBftVerKeyHash c), Eq (PBftVerKeyHash c), Show (PBftVerKeyHash c), NoThunks (PBftVerKeyHash c), NoThunks (PBftDelegationCert c)) => PBftCrypto c where {
    type PBftDSIGN c :: Type;
    type PBftDelegationCert c = (d :: Type) | d -> c;
    type PBftVerKeyHash c = (d :: Type) | d -> c;
}
dlgCertGenVerKey :: PBftCrypto c => PBftDelegationCert c -> VerKeyDSIGN (PBftDSIGN c)
dlgCertDlgVerKey :: PBftCrypto c => PBftDelegationCert c -> VerKeyDSIGN (PBftDSIGN c)
hashVerKey :: PBftCrypto c => VerKeyDSIGN (PBftDSIGN c) -> PBftVerKeyHash c
data PBftMockCrypto

-- | We don't hash and just use the underlying <a>Word64</a>.
newtype PBftMockVerKeyHash
PBftMockVerKeyHash :: VerKeyDSIGN MockDSIGN -> PBftMockVerKeyHash
[getPBftMockVerKeyHash] :: PBftMockVerKeyHash -> VerKeyDSIGN MockDSIGN

-- | Part of the header that we validate
data PBftValidateView c

-- | Regular block
--   
--   Regular blocks are signed, and so we need to validate them. We also
--   need to know the slot number of the block
PBftValidateRegular :: PBftFields c signed -> signed -> ContextDSIGN (PBftDSIGN c) -> PBftValidateView c

-- | Boundary block (EBB)
--   
--   EBBs are not signed and they do not affect the consensus state.
PBftValidateBoundary :: PBftValidateView c

-- | Convenience constructor for <a>PBftValidateView</a> for boundary
--   blocks
pbftValidateBoundary :: hdr -> PBftValidateView c

-- | Convenience constructor for <a>PBftValidateView</a> for regular blocks
pbftValidateRegular :: (SignedHeader hdr, Signable (PBftDSIGN c) (Signed hdr)) => ContextDSIGN (PBftDSIGN c) -> (hdr -> PBftFields c (Signed hdr)) -> hdr -> PBftValidateView c

-- | Expresses that, whilst we believe ourselves to be a leader for this
--   slot, we are nonetheless unable to forge a block.
data PBftCannotForge c

-- | We cannot forge a block because we are not the current delegate of the
--   genesis key we have a delegation certificate from.
PBftCannotForgeInvalidDelegation :: !PBftVerKeyHash c -> PBftCannotForge c

-- | We cannot lead because delegates of the genesis key we have a
--   delegation from have already forged the maximum number of blocks in
--   this signing window.
PBftCannotForgeThresholdExceeded :: !Word64 -> PBftCannotForge c
pbftCheckCanForge :: forall c. PBftCrypto c => ConsensusConfig (PBft c) -> PBftCanBeLeader c -> SlotNo -> Ticked (PBftState c) -> Either (PBftCannotForge c) ()

-- | Static configuration required to run the consensus protocol
--   
--   Every method in the <a>ConsensusProtocol</a> class takes the consensus
--   configuration as a parameter, so having this as a data family rather
--   than a type family resolves most ambiguity.
--   
--   Defined out of the class so that protocols can define this type
--   without having to define the entire protocol at the same time (or
--   indeed in the same module).
data family ConsensusConfig p :: Type

-- | " Ticked " piece of state, either <tt>LedgerState</tt> or
--   <tt>ChainDepState</tt>
--   
--   Ticking refers to the passage of time (the ticking of the clock). When
--   a piece of state is marked as ticked, it means that time-related
--   changes have been applied to the state. There are exactly two methods
--   in the interface that do that: <a>tickChainDepState</a> and
--   <a>applyChainTickLedgerResult</a>.
--   
--   Also note that a successful forecast <tt><a>forecastFor</a>
--   (<a>ledgerViewForecastAt</a> cfg st) slot</tt> must equal
--   <tt><a>protocolLedgerView</a> cfg (<a>applyChainTick</a> cfg slot
--   st)</tt>. Thus a <a>LedgerView</a> can only be projected from a
--   <a>Ticked</a> state, but cannot itself be ticked.
--   
--   Some examples of time related changes:
--   
--   <ul>
--   <li>Scheduled delegations might have been applied in Byron</li>
--   <li>New leader schedule computed for Shelley</li>
--   <li>Transition from Byron to Shelley activated in the hard fork
--   combinator.</li>
--   <li>Nonces switched out at the start of a new epoch.</li>
--   </ul>
data family Ticked st :: Type

-- | NOTE: this type is stored in the state, so it must be in normal form
--   to avoid space leaks.
data PBftValidationErr c
PBftInvalidSignature :: !Text -> PBftValidationErr c
PBftNotGenesisDelegate :: !PBftVerKeyHash c -> !PBftLedgerView c -> PBftValidationErr c

-- | We record how many slots this key signed
PBftExceededSignThreshold :: !PBftVerKeyHash c -> !Word64 -> PBftValidationErr c
PBftInvalidSlot :: PBftValidationErr c
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.PBftFields c toSign)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Protocol.PBFT.PBftSelectView
instance GHC.Generics.Generic Ouroboros.Consensus.Protocol.PBFT.PBftSelectView
instance GHC.Classes.Eq Ouroboros.Consensus.Protocol.PBFT.PBftSelectView
instance GHC.Show.Show Ouroboros.Consensus.Protocol.PBFT.PBftSelectView
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.PBftLedgerView c)
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Protocol.PBFT.PBftSignatureThreshold
instance GHC.Generics.Generic Ouroboros.Consensus.Protocol.PBFT.PBftSignatureThreshold
instance GHC.Show.Show Ouroboros.Consensus.Protocol.PBFT.PBftSignatureThreshold
instance GHC.Classes.Eq Ouroboros.Consensus.Protocol.PBFT.PBftSignatureThreshold
instance GHC.Show.Show Ouroboros.Consensus.Protocol.PBFT.PBftParams
instance NoThunks.Class.NoThunks Ouroboros.Consensus.Protocol.PBFT.PBftParams
instance GHC.Generics.Generic Ouroboros.Consensus.Protocol.PBFT.PBftParams
instance NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.Protocol.PBFT.PBft c))
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.Abstract.ConsensusConfig (Ouroboros.Consensus.Protocol.PBFT.PBft c))
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.PBftCanBeLeader c)
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.PBftIsLeader c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.PBftValidationErr c)
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.PBftValidationErr c)
instance GHC.Generics.Generic (Ouroboros.Consensus.Protocol.PBFT.PBftCannotForge c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Show.Show (Ouroboros.Consensus.Protocol.PBFT.PBftFields c toSign)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Classes.Eq (Ouroboros.Consensus.Protocol.PBFT.PBftFields c toSign)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.PBftLedgerView c)
instance GHC.Classes.Eq (Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftVerKeyHash c) => GHC.Classes.Eq (Ouroboros.Consensus.Protocol.PBFT.PBftLedgerView c)
instance GHC.Show.Show (Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftVerKeyHash c) => GHC.Show.Show (Ouroboros.Consensus.Protocol.PBFT.PBftLedgerView c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Show.Show (Ouroboros.Consensus.Protocol.PBFT.PBftValidationErr c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Classes.Eq (Ouroboros.Consensus.Protocol.PBFT.PBftValidationErr c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => GHC.Show.Show (Ouroboros.Consensus.Protocol.PBFT.PBftCannotForge c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.PBftCannotForge c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => Ouroboros.Consensus.Protocol.Abstract.ConsensusProtocol (Ouroboros.Consensus.Protocol.PBFT.PBft c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.PBftIsLeader c)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.PBftCanBeLeader c)
instance (Codec.Serialise.Class.Serialise (Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftVerKeyHash c), GHC.Classes.Ord (Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftVerKeyHash c)) => Codec.Serialise.Class.Serialise (Ouroboros.Consensus.Protocol.PBFT.PBftLedgerView c)
instance GHC.Classes.Ord Ouroboros.Consensus.Protocol.PBFT.PBftSelectView
instance (Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c, Data.Typeable.Internal.Typeable toSign) => NoThunks.Class.NoThunks (Ouroboros.Consensus.Protocol.PBFT.PBftFields c toSign)
instance Ouroboros.Consensus.Protocol.PBFT.Crypto.PBftCrypto c => Ouroboros.Consensus.Util.Condense.Condense (Ouroboros.Consensus.Protocol.PBFT.PBftFields c toSign)
