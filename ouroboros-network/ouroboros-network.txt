-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | A networking layer for the Ouroboros blockchain protocol
--   
--   A networking layer for the Ouroboros blockchain protocol.
@package ouroboros-network
@version 0.11.0.0


-- | ùö´Q representation and primitives.
--   
--   See also
--   <a>https://www.ofcom.org.uk/__data/assets/pdf_file/0024/71682/traffic-management-detection.pdf</a>
--   appendix A.
module Ouroboros.Network.DeltaQ

-- | A "ùö´Q" is a probability distribution on the duration between two
--   events. It is an "improper" probability distribution in that it may
--   not integrate to 1. The "missing" probability mass represents failure.
--   This allows both timing and failure to be represented in one
--   mathematical object.
--   
--   In the case of networks a ùö´Q can be used for example distributions
--   such as the time for a leading edge or trailing edge of a packet to
--   traverse a network (or failing to do so), and many others besides.
newtype DeltaQ
DeltaQ :: Distribution DiffTime -> DeltaQ

-- | The point in time in the distribution for which 99% of the probability
--   mass is before that time.
--   
--   This tells us how long we would have to wait to have a 99% chance of
--   the end event having happened. Of course 99% is somewhat arbitrary and
--   other points in the distribution could be of interest.
--   
--   TODO: this needs to be specified better for improper distributions.
deltaqQ99thPercentile :: DeltaQ -> DiffTime
deltaqQ50thPercentile :: DeltaQ -> DiffTime

-- | This is another way of looking at a ùö´Q distribution. Instead of giving
--   a fraction of the probability mass (like 99%) and asking how long we
--   would have to wait, we can say how long we are prepared to wait and
--   ask what fraction of the probability mass is before that time.
--   
--   So this tells us the chance of the end event happening within the time
--   we are prepared to wait. This is useful for evaluating different
--   options for which has the greatest probability of success within a
--   deadline.
deltaqProbabilityMassBeforeDeadline :: DiffTime -> DeltaQ -> Double

-- | A "GSV" corresponds to a ùö´Q that is a function of the size of a data
--   unit to be transmitted over a network. That is, it gives the ùö´Q of the
--   transmission time for different sizes of data in <a>SizeInBytes</a>.
--   
--   The ùö´Q is broken out into three separate ùö´Q distributions, ùö´Q‚à£G, ùö´Q‚à£S
--   and ùö´Q‚à£V, with the overall ùö´Q being the convolution of the three
--   components. The G and S components captures the <i>structural</i>
--   aspects of networks, while the V captures the <i>variable</i> aspects:
--   
--   <ul>
--   <li><i><i>G</i></i> the <i>geographical</i> component of network
--   delay. This is the minimum time to transmit a hypothetical zero-sized
--   data unit. This component of the distribution does not depend on the
--   data unit size. It is a degenerate distribution, taking only one
--   value.</li>
--   <li><i><i>S</i></i> the <i>serialisation</i> component of network
--   delay. This is time to serialise a data unit as it is being
--   transmitted. This is of course a function of the data unit size. For
--   each size it is a degenerate distribution, taking only one value.</li>
--   <li><i><i>V</i></i> the <i>variable</i> aspect of network delay. This
--   captures the variability in network delay due to issues such as
--   congestion. This does not depend on the data unit size, and is
--   <i>not</i> a degenerate disruption.</li>
--   </ul>
--   
--   For ballistic transmission of packets, <i>S</i> is typically directly
--   proportional to the size. Thus the combination of <i>G</i> and
--   <i>S</i> is simply a linear function of the size.
data GSV
GSV :: !DiffTime -> !SizeInBytes -> DiffTime -> !Distribution DiffTime -> GSV
newtype () => SizeInBytes
SizeInBytes :: Word32 -> SizeInBytes
[getSizeInBytes] :: SizeInBytes -> Word32

-- | The case of ballistic packet transmission where the <i>S</i> is
--   directly proportional to the packet size.
ballisticGSV :: DiffTime -> DiffTime -> Distribution DiffTime -> GSV

-- | The ùö´Q for when the leading edge of a transmission unit arrives at the
--   destination. This is just the convolution of the <i>G</i> and <i>V</i>
--   components.
gsvLeadingEdgeArrive :: GSV -> DeltaQ

-- | The ùö´Q for when the trailing edge of a transmission unit departs the
--   sending end. This is just the convolution of the <i>S</i> and <i>V</i>
--   components.
--   
--   Since it involves <i>S</i> then it depends on the <a>SizeInBytes</a>
--   of the transmission unit.
gsvTrailingEdgeDepart :: GSV -> SizeInBytes -> DeltaQ

-- | The ùö´Q for when the trailing edge of a transmission unit arrives at
--   the destination. This is the convolution of the <i>G</i>, <i>S</i> and
--   <i>V</i> components.
--   
--   Since it involves <i>S</i> then it depends on the <a>SizeInBytes</a>
--   of the transmission unit.
gsvTrailingEdgeArrive :: GSV -> SizeInBytes -> DeltaQ

-- | An improper probability distribution over some underlying type (such
--   as time durations).
--   
--   The current representation only covers the case of degenerate
--   distributions, that take a single value with probability 1. This is
--   just a proof of concept to illustrate the API.
data Distribution n

-- | Make a degenerate distribution.
--   
--   <a>https://en.wikipedia.org/wiki/Degenerate_distribution</a>
degenerateDistribution :: n -> Distribution n

-- | The <a>GSV</a> for both directions with a peer, outbound and inbound.
data PeerGSV
PeerGSV :: !Time -> !GSV -> !GSV -> PeerGSV
[sampleTime] :: PeerGSV -> !Time
[outboundGSV] :: PeerGSV -> !GSV
[inboundGSV] :: PeerGSV -> !GSV

-- | This is an example derived operation using the other <a>GSV</a> and
--   <a>DeltaQ</a> primitives.
--   
--   It calculates the ùö´Q for the time to send a request of a certain size
--   and receive a reply of an expected size. It then takes the 99%
--   percentile as an approximation of the maximum time we might be
--   prepared to wait.
--   
--   <pre>
--   deltaqQ99thPercentile $
--       gsvTrailingEdgeArrive outboundGSV reqSize
--    &lt;&gt; gsvTrailingEdgeArrive inboundGSV respSize
--   </pre>
--   
--   This is not realistic in that it omits processing time, but that could
--   be added as yet another <a>DeltaQ</a> value, if there's any estimate
--   for it:
--   
--   <pre>
--   deltaqQ99thPercentile $
--       gsvTrailingEdgeArrive outboundGSV reqSize
--    &lt;&gt; gsvTrailingEdgeArrive inboundGSV respSize
--    &lt;&gt; processingDeltaQ
--   </pre>
gsvRequestResponseDuration :: PeerGSV -> SizeInBytes -> SizeInBytes -> DiffTime
defaultGSV :: PeerGSV
fromSample :: Time -> Time -> SizeInBytes -> PeerGSV
instance GHC.Show.Show Ouroboros.Network.DeltaQ.PeerGSV
instance GHC.Base.Semigroup Ouroboros.Network.DeltaQ.DeltaQ
instance GHC.Base.Semigroup Ouroboros.Network.DeltaQ.PeerGSV
instance GHC.Base.Semigroup Ouroboros.Network.DeltaQ.GSV
instance GHC.Show.Show Ouroboros.Network.DeltaQ.GSV
instance GHC.Num.Num n => GHC.Base.Semigroup (Ouroboros.Network.DeltaQ.Distribution n)

module Ouroboros.Network.BlockFetch.DeltaQ

-- | A "GSV" corresponds to a ùö´Q that is a function of the size of a data
--   unit to be transmitted over a network. That is, it gives the ùö´Q of the
--   transmission time for different sizes of data in <a>SizeInBytes</a>.
--   
--   The ùö´Q is broken out into three separate ùö´Q distributions, ùö´Q‚à£G, ùö´Q‚à£S
--   and ùö´Q‚à£V, with the overall ùö´Q being the convolution of the three
--   components. The G and S components captures the <i>structural</i>
--   aspects of networks, while the V captures the <i>variable</i> aspects:
--   
--   <ul>
--   <li><i><i>G</i></i> the <i>geographical</i> component of network
--   delay. This is the minimum time to transmit a hypothetical zero-sized
--   data unit. This component of the distribution does not depend on the
--   data unit size. It is a degenerate distribution, taking only one
--   value.</li>
--   <li><i><i>S</i></i> the <i>serialisation</i> component of network
--   delay. This is time to serialise a data unit as it is being
--   transmitted. This is of course a function of the data unit size. For
--   each size it is a degenerate distribution, taking only one value.</li>
--   <li><i><i>V</i></i> the <i>variable</i> aspect of network delay. This
--   captures the variability in network delay due to issues such as
--   congestion. This does not depend on the data unit size, and is
--   <i>not</i> a degenerate disruption.</li>
--   </ul>
--   
--   For ballistic transmission of packets, <i>S</i> is typically directly
--   proportional to the size. Thus the combination of <i>G</i> and
--   <i>S</i> is simply a linear function of the size.
data GSV

-- | An improper probability distribution over some underlying type (such
--   as time durations).
--   
--   The current representation only covers the case of degenerate
--   distributions, that take a single value with probability 1. This is
--   just a proof of concept to illustrate the API.
data Distribution n

-- | A "ùö´Q" is a probability distribution on the duration between two
--   events. It is an "improper" probability distribution in that it may
--   not integrate to 1. The "missing" probability mass represents failure.
--   This allows both timing and failure to be represented in one
--   mathematical object.
--   
--   In the case of networks a ùö´Q can be used for example distributions
--   such as the time for a leading edge or trailing edge of a packet to
--   traverse a network (or failing to do so), and many others besides.
data DeltaQ

-- | The <a>GSV</a> for both directions with a peer, outbound and inbound.
data PeerGSV
PeerGSV :: !Time -> !GSV -> !GSV -> PeerGSV
[sampleTime] :: PeerGSV -> !Time
[outboundGSV] :: PeerGSV -> !GSV
[inboundGSV] :: PeerGSV -> !GSV
data () => SizeInBytes
data PeerFetchInFlightLimits
PeerFetchInFlightLimits :: SizeInBytes -> SizeInBytes -> PeerFetchInFlightLimits
[inFlightBytesHighWatermark] :: PeerFetchInFlightLimits -> SizeInBytes
[inFlightBytesLowWatermark] :: PeerFetchInFlightLimits -> SizeInBytes
calculatePeerFetchInFlightLimits :: PeerGSV -> PeerFetchInFlightLimits

-- | Given the <a>PeerGSV</a>, the bytes already in flight and the size of
--   new blocks to download, estimate the probability of the download
--   completing within the deadline.
--   
--   This is an appropriate estimator to use in a situation where meeting a
--   known deadline is the goal.
estimateResponseDeadlineProbability :: PeerGSV -> SizeInBytes -> SizeInBytes -> DiffTime -> Double

-- | Given the <a>PeerGSV</a>, the bytes already in flight and the size of
--   new blocks to download, estimate the expected (mean) time to complete
--   the download.
--   
--   This is an appropriate estimator to use when trying to minimising the
--   expected overall download time case in the long run (rather than
--   optimising for the worst case in the short term).
estimateExpectedResponseDuration :: PeerGSV -> SizeInBytes -> SizeInBytes -> DiffTime

-- | Order two PeerGSVs based on <tt>g</tt>. Incase the g values are within
--   +/- 5% of each other <tt>peer</tt> is used as a tie breaker. The salt
--   is unique per running node, which avoids all nodes prefering the same
--   peer in case of a tie.
comparePeerGSV :: forall peer. (Hashable peer, Ord peer) => Set peer -> Int -> (PeerGSV, peer) -> (PeerGSV, peer) -> Ordering

-- | Order two PeerGSVs based on <tt>g</tt>. Like comparePeerGSV but
--   doesn't take active status into account
comparePeerGSV' :: forall peer. (Hashable peer, Ord peer) => Int -> (PeerGSV, peer) -> (PeerGSV, peer) -> Ordering
instance GHC.Show.Show Ouroboros.Network.BlockFetch.DeltaQ.PeerFetchInFlightLimits

module Ouroboros.Network.BlockFetch.ClientState

-- | The context that is passed into the block fetch protocol client when
--   it is started.
data FetchClientContext header block m
FetchClientContext :: Tracer m (TraceFetchClientState header) -> FetchClientPolicy header block m -> FetchClientStateVars m header -> FetchClientContext header block m
[fetchClientCtxTracer] :: FetchClientContext header block m -> Tracer m (TraceFetchClientState header)
[fetchClientCtxPolicy] :: FetchClientContext header block m -> FetchClientPolicy header block m
[fetchClientCtxStateVars] :: FetchClientContext header block m -> FetchClientStateVars m header

-- | The policy used by the fetch clients. It is set by the central block
--   fetch logic, and passed to them via the <tt>FetchClientRegistry</tt>.
data FetchClientPolicy header block m
FetchClientPolicy :: (header -> SizeInBytes) -> (header -> block -> Bool) -> (Point block -> block -> m ()) -> (FromConsensus block -> STM m UTCTime) -> FetchClientPolicy header block m
[blockFetchSize] :: FetchClientPolicy header block m -> header -> SizeInBytes
[blockMatchesHeader] :: FetchClientPolicy header block m -> header -> block -> Bool
[addFetchedBlock] :: FetchClientPolicy header block m -> Point block -> block -> m ()
[blockForgeUTCTime] :: FetchClientPolicy header block m -> FromConsensus block -> STM m UTCTime

-- | A set of variables shared between the block fetch logic thread and
--   each thread executing the client side of the block fetch protocol.
--   That is, these are the shared variables per peer. The
--   <tt>FetchClientRegistry</tt> contains the mapping of these for all
--   peers.
--   
--   The variables are used for communicating from the protocol thread to
--   the decision making thread the status of things with that peer. And in
--   the other direction one shared variable is for providing new fetch
--   requests.
data FetchClientStateVars m header
FetchClientStateVars :: StrictTVar m (PeerFetchStatus header) -> StrictTVar m (PeerFetchInFlight header) -> TFetchRequestVar m header -> FetchClientStateVars m header

-- | The current status of communication with the peer. It is written by
--   the protocol thread and monitored and read by the decision logic
--   thread. Changes in this state trigger re-evaluation of fetch
--   decisions.
[fetchClientStatusVar] :: FetchClientStateVars m header -> StrictTVar m (PeerFetchStatus header)

-- | The current number of requests in-flight and the amount of data
--   in-flight with the peer. It is written by the protocol thread and read
--   by the decision logic thread. This is used in fetch decisions but
--   changes here do not trigger re-evaluation of fetch decisions.
[fetchClientInFlightVar] :: FetchClientStateVars m header -> StrictTVar m (PeerFetchInFlight header)

-- | The shared variable used to communicate fetch requests to the thread
--   running the block fetch protocol. Fetch requests are posted by the
--   decision logic thread. The protocol thread accepts the requests and
--   acts on them, updating the in-flight stats. While this is a
--   <tt>TMVar</tt>, it is not used as a one-place queue: the requests can
--   be updated before being accepted.
[fetchClientRequestVar] :: FetchClientStateVars m header -> TFetchRequestVar m header
newFetchClientStateVars :: MonadSTM m => STM m (FetchClientStateVars m header)
readFetchClientState :: MonadSTM m => FetchClientStateVars m header -> STM m (PeerFetchStatus header, PeerFetchInFlight header, FetchClientStateVars m header)

-- | The status of the block fetch communication with a peer. This is
--   maintained by fetch protocol threads and used in the block fetch
--   decision making logic. Changes in this status trigger re-evaluation of
--   fetch decisions.
data PeerFetchStatus header

-- | Communication with the peer has failed. This is a temporary status
--   that may occur during the process of shutting down the thread that
--   runs the block fetch protocol. The peer will promptly be removed from
--   the peer registry and so will not be considered at all.
PeerFetchStatusShutdown :: PeerFetchStatus header

-- | Blockfetch is starting up and waiting on corresponding Chainsync
PeerFetchStatusStarting :: PeerFetchStatus header

-- | The peer is in a potentially-temporary state in which it has not
--   responded to us within a certain expected time limit. This is not a
--   hard protocol timeout where the whole connection will be abandoned, it
--   is simply a reply that has taken longer than expected. This status is
--   used to trigger re-evaluating which peer to ask for blocks from, so
--   that we can swiftly ask other peers for blocks if one unexpectedly
--   responds too slowly
--   
--   Peers in this state may later return to normal states if communication
--   resumes, or they may eventually hit a hard timeout and fail.
PeerFetchStatusAberrant :: PeerFetchStatus header

-- | Communication with the peer is in a normal state, and the peer is
--   considered too busy to accept new requests. Changing from this state
--   to the ready state is used to trigger re-evaluating fetch decisions
--   and may eventually result in new fetch requests. This state is used as
--   part of a policy to batch new requests: instead of switching to the
--   ready state the moment there is tiny bit of capacity available, the
--   state is changed once the capacity reaches a certain threshold.
PeerFetchStatusBusy :: PeerFetchStatus header

-- | Communication with the peer is in a normal state, and the peer is
--   considered ready to accept new requests.
--   
--   The <a>Set</a> is the blocks in flight.
PeerFetchStatusReady :: Set (Point header) -> IsIdle -> PeerFetchStatus header

-- | Whether this mini protocol instance is in the <tt>Idle</tt> State
data IsIdle
IsIdle :: IsIdle
IsNotIdle :: IsIdle

-- | The number of requests in-flight and the amount of data in-flight with
--   a peer. This is maintained by fetch protocol threads and used in the
--   block fetch decision making logic.
data PeerFetchInFlight header
PeerFetchInFlight :: !Word -> !SizeInBytes -> Set (Point header) -> !MaxSlotNo -> PeerFetchInFlight header

-- | The number of block fetch requests that are currently in-flight. This
--   is the number of <i>requests</i> not the number of blocks. Each
--   request is for a range of blocks.
--   
--   We track this because there is a fixed maximum number of outstanding
--   requests that the protocol allows.
[peerFetchReqsInFlight] :: PeerFetchInFlight header -> !Word

-- | The sum of the byte count of blocks expected from all in-flight fetch
--   requests. This is a close approximation of the amount of data we
--   expect to receive, assuming no failures.
--   
--   We track this because we pipeline fetch requests and we want to keep
--   some but not too much data in flight at once.
[peerFetchBytesInFlight] :: PeerFetchInFlight header -> !SizeInBytes

-- | The points for the set of blocks that are currently in-flight. Note
--   that since requests are for ranges of blocks this does not correspond
--   to the number of requests in flight.
--   
--   We track this because as part of the decision for which blocks to
--   fetch from which peers we take into account what blocks are already
--   in-flight with peers.
[peerFetchBlocksInFlight] :: PeerFetchInFlight header -> Set (Point header)

-- | The maximum slot of a block that <i>has ever been</i> in flight for
--   this peer.
--   
--   We track this to more efficiently remove blocks that are already
--   in-flight from the candidate fragments: blocks with a slot number
--   higher than this one do not have to be filtered out.
[peerFetchMaxSlotNo] :: PeerFetchInFlight header -> !MaxSlotNo
initialPeerFetchInFlight :: PeerFetchInFlight header
newtype FetchRequest header
FetchRequest :: [AnchoredFragment header] -> FetchRequest header
[fetchRequestFragments] :: FetchRequest header -> [AnchoredFragment header]

-- | Add a new fetch request for a single peer. This is used by the fetch
--   decision logic thread to add new fetch requests.
--   
--   We have as a pre-condition that all requested blocks are new, i.e.
--   none should appear in the existing <a>peerFetchBlocksInFlight</a>.
--   This is a relatively easy precondition to satisfy since the decision
--   logic can filter its requests based on this in-flight blocks state,
--   and this operation is the only operation that grows the in-flight
--   blocks, and is only used by the fetch decision logic thread.
addNewFetchRequest :: (MonadSTM m, HasHeader header) => Tracer m (TraceFetchClientState header) -> (header -> SizeInBytes) -> FetchRequest header -> PeerGSV -> FetchClientStateVars m header -> m (PeerFetchStatus header)

-- | This is used by the fetch client threads.
acknowledgeFetchRequest :: MonadSTM m => Tracer m (TraceFetchClientState header) -> ControlMessageSTM m -> FetchClientStateVars m header -> m (Maybe (FetchRequest header, PeerGSV, PeerFetchInFlightLimits))
startedFetchBatch :: MonadSTM m => Tracer m (TraceFetchClientState header) -> PeerFetchInFlightLimits -> ChainRange (Point header) -> FetchClientStateVars m header -> m ()
completeBlockDownload :: (MonadSTM m, HasHeader header) => Tracer m (TraceFetchClientState header) -> (header -> SizeInBytes) -> PeerFetchInFlightLimits -> header -> NominalDiffTime -> FetchClientStateVars m header -> m ()
completeFetchBatch :: MonadSTM m => Tracer m (TraceFetchClientState header) -> PeerFetchInFlightLimits -> ChainRange (Point header) -> FetchClientStateVars m header -> m ()
rejectedFetchBatch :: (MonadSTM m, HasHeader header) => Tracer m (TraceFetchClientState header) -> (header -> SizeInBytes) -> PeerFetchInFlightLimits -> ChainRange (Point header) -> [header] -> FetchClientStateVars m header -> m ()

-- | Tracing types for the various events that change the state (i.e.
--   <a>FetchClientStateVars</a>) for a block fetch client.
--   
--   Note that while these are all state changes, the
--   <a>AddedFetchRequest</a> occurs in the decision thread while the other
--   state changes occur in the block fetch client threads.
data TraceFetchClientState header

-- | The block fetch decision thread has added a new fetch instruction
--   consisting of one or more individual request ranges.
AddedFetchRequest :: FetchRequest header -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | Mark the point when the fetch client picks up the request added by the
--   block fetch decision thread. Note that this event can happen fewer
--   times than the <a>AddedFetchRequest</a> due to fetch request merging.
AcknowledgedFetchRequest :: FetchRequest header -> TraceFetchClientState header

-- | Mark the point when fetch request for a fragment is actually sent over
--   the wire.
SendFetchRequest :: AnchoredFragment header -> PeerGSV -> TraceFetchClientState header

-- | Mark the start of receiving a streaming batch of blocks. This will be
--   followed by one or more <a>CompletedBlockFetch</a> and a final
--   <a>CompletedFetchBatch</a>.
StartedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | Mark the completion of of receiving a single block within a streaming
--   batch of blocks.
CompletedBlockFetch :: Point header -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> NominalDiffTime -> SizeInBytes -> TraceFetchClientState header

-- | Mark the successful end of receiving a streaming batch of blocks
CompletedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | If the other peer rejects our request then we have this event instead
--   of <a>StartedFetchBatch</a> and <a>CompletedFetchBatch</a>.
RejectedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | The client is terminating. Log the number of outstanding requests.
ClientTerminating :: Int -> TraceFetchClientState header
data () => TraceLabelPeer peerid a
TraceLabelPeer :: peerid -> a -> TraceLabelPeer peerid a
data () => ChainRange point
ChainRange :: !point -> !point -> ChainRange point
newtype () => FromConsensus a
FromConsensus :: a -> FromConsensus a
[unFromConsensus] :: FromConsensus a -> a
data () => WhetherReceivingTentativeBlocks
ReceivingTentativeBlocks :: WhetherReceivingTentativeBlocks
NotReceivingTentativeBlocks :: WhetherReceivingTentativeBlocks
instance GHC.Show.Show Ouroboros.Network.BlockFetch.ClientState.IsIdle
instance GHC.Classes.Eq Ouroboros.Network.BlockFetch.ClientState.IsIdle
instance Ouroboros.Network.Block.StandardHash header => GHC.Show.Show (Ouroboros.Network.BlockFetch.ClientState.PeerFetchStatus header)
instance Ouroboros.Network.Block.StandardHash header => GHC.Classes.Eq (Ouroboros.Network.BlockFetch.ClientState.PeerFetchStatus header)
instance Ouroboros.Network.Block.StandardHash header => GHC.Show.Show (Ouroboros.Network.BlockFetch.ClientState.PeerFetchInFlight header)
instance Ouroboros.Network.Block.StandardHash header => GHC.Classes.Eq (Ouroboros.Network.BlockFetch.ClientState.PeerFetchInFlight header)
instance (Ouroboros.Network.Block.StandardHash header, GHC.Show.Show header) => GHC.Show.Show (Ouroboros.Network.BlockFetch.ClientState.FetchRequest header)
instance (Ouroboros.Network.Block.StandardHash header, GHC.Show.Show header) => GHC.Show.Show (Ouroboros.Network.BlockFetch.ClientState.TraceFetchClientState header)
instance Ouroboros.Network.Block.HasHeader header => GHC.Base.Semigroup (Ouroboros.Network.BlockFetch.ClientState.FetchRequest header)

module Ouroboros.Network.BlockFetch.Decision
fetchDecisions :: (Ord peer, Hashable peer, HasHeader header, HeaderHash header ~ HeaderHash block) => FetchDecisionPolicy header -> FetchMode -> AnchoredFragment header -> (Point block -> Bool) -> MaxSlotNo -> [(AnchoredFragment header, PeerInfo header peer extra)] -> [(FetchDecision (FetchRequest header), PeerInfo header peer extra)]
data FetchDecisionPolicy header
FetchDecisionPolicy :: Word -> Word -> Word -> DiffTime -> Int -> (HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Bool) -> (HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Ordering) -> (header -> SizeInBytes) -> FetchDecisionPolicy header
[maxInFlightReqsPerPeer] :: FetchDecisionPolicy header -> Word
[maxConcurrencyBulkSync] :: FetchDecisionPolicy header -> Word
[maxConcurrencyDeadline] :: FetchDecisionPolicy header -> Word
[decisionLoopInterval] :: FetchDecisionPolicy header -> DiffTime
[peerSalt] :: FetchDecisionPolicy header -> Int
[plausibleCandidateChain] :: FetchDecisionPolicy header -> HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Bool
[compareCandidateChains] :: FetchDecisionPolicy header -> HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Ordering
[blockFetchSize] :: FetchDecisionPolicy header -> header -> SizeInBytes
data () => FetchMode
FetchModeBulkSync :: FetchMode
FetchModeDeadline :: FetchMode
type PeerInfo header peer extra = (PeerFetchStatus header, PeerFetchInFlight header, PeerGSV, peer, extra)

-- | Throughout the decision making process we accumulate reasons to
--   decline to fetch any blocks. This type is used to wrap intermediate
--   and final results.
type FetchDecision result = Either FetchDecline result

-- | All the various reasons we can decide not to fetch blocks from a peer.
--   
--   It is worth highlighting which of these reasons result from
--   competition among upstream peers.
--   
--   <ul>
--   <li><a>FetchDeclineInFlightOtherPeer</a>: decline this peer because
--   all the unfetched blocks of its candidate chain have already been
--   requested from other peers. This reason reflects the
--   least-consequential competition among peers: the competition that
--   determines merely which upstream peer to burden with the request (eg
--   the one with the best <a>DeltaQ</a> metrics). The consequences are
--   relatively minor because the unfetched blocks on this peer's candidate
--   chain will be requested regardless; it's merely a question of "From
--   who?". (One exception: if an adversarial peer wins this competition
--   such that the blocks are only requested from them, then it may be
--   possible that this decision determines whether the blocks are ever
--   <i>received</i>. But that depends on details of timeouts, a longer
--   competing chain being soon received within those timeouts, and so
--   on.)</li>
--   <li><a>FetchDeclineChainNotPlausible</a>: decline this peer because
--   the node has already fetched, validated, and selected a chain better
--   than its candidate chain from other peers (or from the node's own
--   block forge). Because the node's current selection is influenced by
--   what blocks other peers have recently served (or it recently minted),
--   this reason reflects that peers <i>indirectly</i> compete by serving
--   as long of a chain as possible and as promptly as possible. When the
--   tips of the peers' selections are all within their respective forecast
--   horizons (see <a>ledgerViewForecastAt</a>), then the length of their
--   candidate chains will typically be the length of their selections,
--   since the ChainSync is free to race ahead (in contrast, the BlockFetch
--   pipeline depth is bounded such that it will, for a syncing node, not
--   be able to request all blocks between the selection and the end of the
--   forecast window). But if one or more of their tips is beyond the
--   horizon, then the relative length of the candidate chains is more
--   complicated, influenced by both the relative density of the chains'
--   suffixes and the relative age of the chains' intersection with the
--   node's selection (since each peer's forecast horizon is a fixed number
--   of slots after the candidate's successor of that intersection).</li>
--   <li><a>FetchDeclineConcurrencyLimit</a>: decline this peer while the
--   node has already fully allocated the artificially scarce
--   <tt>maxConcurrentFetchPeers</tt> resource amongst its other peers.
--   This reason reflects the least-fundamental competition: it's the only
--   way a node would decline a candidate chain C that it would immediately
--   switch to if C had somehow already been fetched (and any better
--   current candidates hadn't). It is possible that this peer's candidate
--   fragment is better than the candidate fragments of other peers, but
--   that should only happen ephemerally (eg for a brief while immediately
--   after first connecting to this peer).</li>
--   <li><a>FetchDeclineChainIntersectionTooDeep</a>: decline this peer
--   because the node's selection has more than <tt>K</tt> blocks that are
--   not on this peer's candidate chain. Typically, this reason occurs
--   after the node has been declined---ie lost the above
--   competitions---for a long enough duration. This decision only arises
--   if the BlockFetch decision logic wins a harmless race against the
--   ChainSync client once the node's selection gets longer, since
--   <a>ForkTooDeep</a> disconnects from such a peer.</li>
--   </ul>
data FetchDecline

-- | This peer's candidate chain is not longer than our chain. For more
--   details see <a>mkBlockFetchConsensusInterface</a> which implements
--   <a>plausibleCandidateChain</a>.
FetchDeclineChainNotPlausible :: FetchDecline

-- | Switching to this peer's candidate chain would require rolling back
--   more than <tt>K</tt> blocks.
FetchDeclineChainIntersectionTooDeep :: FetchDecline

-- | Every block on this peer's candidate chain has already been fetched.
FetchDeclineAlreadyFetched :: FetchDecline

-- | This peer's candidate chain has already been requested from this peer.
FetchDeclineInFlightThisPeer :: FetchDecline

-- | Some blocks on this peer's candidate chain have not yet been fetched,
--   but all of those have already been requested from other peers.
FetchDeclineInFlightOtherPeer :: FetchDecline

-- | This peer's BlockFetch client is shutting down, see
--   <a>PeerFetchStatusShutdown</a>.
FetchDeclinePeerShutdown :: FetchDecline

-- | Blockfetch is starting up and waiting on corresponding Chainsync.
FetchDeclinePeerStarting :: FetchDecline

-- | This peer is in a potentially-temporary state in which it has not
--   responded to us within a certain expected time limit, see
--   <a>PeerFetchStatusAberrant</a>.
FetchDeclinePeerSlow :: FetchDecline

-- | This peer is not under the <a>maxInFlightReqsPerPeer</a> limit.
--   
--   The argument is the <a>maxInFlightReqsPerPeer</a> constant.
FetchDeclineReqsInFlightLimit :: !Word -> FetchDecline

-- | This peer is not under the <a>inFlightBytesHighWatermark</a> bytes
--   limit.
--   
--   The arguments are:
--   
--   <ul>
--   <li>number of bytes currently in flight for that peer</li>
--   <li>the configured <a>inFlightBytesLowWatermark</a> constant</li>
--   <li>the configured <a>inFlightBytesHighWatermark</a> constant</li>
--   </ul>
FetchDeclineBytesInFlightLimit :: !SizeInBytes -> !SizeInBytes -> !SizeInBytes -> FetchDecline

-- | This peer is not under the <a>inFlightBytesLowWatermark</a>.
--   
--   The arguments are:
--   
--   <ul>
--   <li>number of bytes currently in flight for that peer</li>
--   <li>the configured <a>inFlightBytesLowWatermark</a> constant</li>
--   <li>the configured <a>inFlightBytesHighWatermark</a> constant</li>
--   </ul>
FetchDeclinePeerBusy :: !SizeInBytes -> !SizeInBytes -> !SizeInBytes -> FetchDecline

-- | The node is not under the <tt>maxConcurrentFetchPeers</tt> limit.
--   
--   The arguments are:
--   
--   <ul>
--   <li>the current <a>FetchMode</a></li>
--   <li>the corresponding configured limit constant, either
--   <a>maxConcurrencyBulkSync</a> or <a>maxConcurrencyDeadline</a></li>
--   </ul>
FetchDeclineConcurrencyLimit :: !FetchMode -> !Word -> FetchDecline

-- | Keep only those candidate chains that are preferred over the current
--   chain. Typically, this means that their length is longer than the
--   length of the current chain.
filterPlausibleCandidates :: (AnchoredFragment block -> AnchoredFragment header -> Bool) -> AnchoredFragment block -> [(AnchoredFragment header, peerinfo)] -> [(FetchDecision (AnchoredFragment header), peerinfo)]
selectForkSuffixes :: (HasHeader header, HasHeader block, HeaderHash header ~ HeaderHash block) => AnchoredFragment block -> [(FetchDecision (AnchoredFragment header), peerinfo)] -> [(FetchDecision (ChainSuffix header), peerinfo)]

-- | Find the fragments of the chain suffix that we still need to fetch,
--   these are the fragments covering blocks that have not yet been fetched
--   and are not currently in the process of being fetched from this peer.
--   
--   Typically this is a single fragment forming a suffix of the chain, but
--   in the general case we can get a bunch of discontiguous chain
--   fragments.
filterNotAlreadyFetched :: (HasHeader header, HeaderHash header ~ HeaderHash block) => (Point block -> Bool) -> MaxSlotNo -> [(FetchDecision (ChainSuffix header), peerinfo)] -> [(FetchDecision (CandidateFragments header), peerinfo)]
filterNotAlreadyInFlightWithPeer :: HasHeader header => [(FetchDecision (CandidateFragments header), PeerFetchInFlight header, peerinfo)] -> [(FetchDecision (CandidateFragments header), peerinfo)]
prioritisePeerChains :: forall extra header peer. (HasHeader header, Hashable peer, Ord peer) => FetchMode -> Int -> (AnchoredFragment header -> AnchoredFragment header -> Ordering) -> (header -> SizeInBytes) -> [(FetchDecision (CandidateFragments header), PeerFetchInFlight header, PeerGSV, peer, extra)] -> [(FetchDecision [AnchoredFragment header], extra)]

-- | A penultimate step of filtering, but this time across peers, rather
--   than individually for each peer. If we're following the parallel fetch
--   mode then we filter out blocks that are already in-flight with other
--   peers.
--   
--   Note that this does <i>not</i> cover blocks that are proposed to be
--   fetched in this round of decisions. That step is covered in
--   <a>fetchRequestDecisions</a>.
filterNotAlreadyInFlightWithOtherPeers :: HasHeader header => FetchMode -> [(FetchDecision [AnchoredFragment header], PeerFetchStatus header, PeerFetchInFlight header, peerinfo)] -> [(FetchDecision [AnchoredFragment header], peerinfo)]
fetchRequestDecisions :: forall extra header peer. (Hashable peer, HasHeader header, Ord peer) => FetchDecisionPolicy header -> FetchMode -> [(FetchDecision [AnchoredFragment header], PeerFetchStatus header, PeerFetchInFlight header, PeerGSV, peer, extra)] -> [(FetchDecision (FetchRequest header), extra)]
instance GHC.Show.Show Ouroboros.Network.BlockFetch.Decision.FetchDecline
instance GHC.Classes.Eq Ouroboros.Network.BlockFetch.Decision.FetchDecline
instance GHC.Show.Show Ouroboros.Network.BlockFetch.Decision.ProbabilityBand
instance GHC.Classes.Ord Ouroboros.Network.BlockFetch.Decision.ProbabilityBand
instance GHC.Classes.Eq Ouroboros.Network.BlockFetch.Decision.ProbabilityBand

module Ouroboros.Network.BlockFetch.State
fetchLogicIterations :: (HasHeader header, HasHeader block, HeaderHash header ~ HeaderHash block, MonadDelay m, MonadSTM m, Ord peer, Hashable peer) => Tracer m [TraceLabelPeer peer (FetchDecision [Point header])] -> Tracer m (TraceLabelPeer peer (TraceFetchClientState header)) -> FetchDecisionPolicy header -> FetchTriggerVariables peer header m -> FetchNonTriggerVariables peer header block m -> m Void
data FetchDecisionPolicy header
FetchDecisionPolicy :: Word -> Word -> Word -> DiffTime -> Int -> (HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Bool) -> (HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Ordering) -> (header -> SizeInBytes) -> FetchDecisionPolicy header
[maxInFlightReqsPerPeer] :: FetchDecisionPolicy header -> Word
[maxConcurrencyBulkSync] :: FetchDecisionPolicy header -> Word
[maxConcurrencyDeadline] :: FetchDecisionPolicy header -> Word
[decisionLoopInterval] :: FetchDecisionPolicy header -> DiffTime
[peerSalt] :: FetchDecisionPolicy header -> Int
[plausibleCandidateChain] :: FetchDecisionPolicy header -> HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Bool
[compareCandidateChains] :: FetchDecisionPolicy header -> HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Ordering
[blockFetchSize] :: FetchDecisionPolicy header -> header -> SizeInBytes

-- | STM actions to read various state variables that the fetch logic
--   depends upon. Any change in these variables is a trigger to
--   re-evaluate the decision on what blocks to fetch.
--   
--   Note that this is a "level trigger" not an "edge trigger": we do not
--   have to re-evaluate on every change, it is sufficient to re-evaluate
--   at some stage after one or more changes. This means it is ok to get
--   somewhat behind, and it is not necessary to determine exactly what
--   changed, just that there was some change.
data FetchTriggerVariables peer header m
FetchTriggerVariables :: STM m (AnchoredFragment header) -> STM m (Map peer (AnchoredFragment header)) -> STM m (Map peer (PeerFetchStatus header)) -> FetchTriggerVariables peer header m
[readStateCurrentChain] :: FetchTriggerVariables peer header m -> STM m (AnchoredFragment header)
[readStateCandidateChains] :: FetchTriggerVariables peer header m -> STM m (Map peer (AnchoredFragment header))
[readStatePeerStatus] :: FetchTriggerVariables peer header m -> STM m (Map peer (PeerFetchStatus header))

-- | STM actions to read various state variables that the fetch logic uses.
--   While the decisions do make use of the values of these variables, it
--   is not necessary to re-evaluate when these variables change.
data FetchNonTriggerVariables peer header block m
FetchNonTriggerVariables :: STM m (Point block -> Bool) -> STM m (Map peer (FetchClientStateVars m header)) -> STM m (Map peer PeerGSV) -> STM m FetchMode -> STM m MaxSlotNo -> FetchNonTriggerVariables peer header block m
[readStateFetchedBlocks] :: FetchNonTriggerVariables peer header block m -> STM m (Point block -> Bool)
[readStatePeerStateVars] :: FetchNonTriggerVariables peer header block m -> STM m (Map peer (FetchClientStateVars m header))
[readStatePeerGSVs] :: FetchNonTriggerVariables peer header block m -> STM m (Map peer PeerGSV)
[readStateFetchMode] :: FetchNonTriggerVariables peer header block m -> STM m FetchMode
[readStateFetchedMaxSlotNo] :: FetchNonTriggerVariables peer header block m -> STM m MaxSlotNo

-- | Throughout the decision making process we accumulate reasons to
--   decline to fetch any blocks. This type is used to wrap intermediate
--   and final results.
type FetchDecision result = Either FetchDecline result

-- | All the various reasons we can decide not to fetch blocks from a peer.
--   
--   It is worth highlighting which of these reasons result from
--   competition among upstream peers.
--   
--   <ul>
--   <li><a>FetchDeclineInFlightOtherPeer</a>: decline this peer because
--   all the unfetched blocks of its candidate chain have already been
--   requested from other peers. This reason reflects the
--   least-consequential competition among peers: the competition that
--   determines merely which upstream peer to burden with the request (eg
--   the one with the best <a>DeltaQ</a> metrics). The consequences are
--   relatively minor because the unfetched blocks on this peer's candidate
--   chain will be requested regardless; it's merely a question of "From
--   who?". (One exception: if an adversarial peer wins this competition
--   such that the blocks are only requested from them, then it may be
--   possible that this decision determines whether the blocks are ever
--   <i>received</i>. But that depends on details of timeouts, a longer
--   competing chain being soon received within those timeouts, and so
--   on.)</li>
--   <li><a>FetchDeclineChainNotPlausible</a>: decline this peer because
--   the node has already fetched, validated, and selected a chain better
--   than its candidate chain from other peers (or from the node's own
--   block forge). Because the node's current selection is influenced by
--   what blocks other peers have recently served (or it recently minted),
--   this reason reflects that peers <i>indirectly</i> compete by serving
--   as long of a chain as possible and as promptly as possible. When the
--   tips of the peers' selections are all within their respective forecast
--   horizons (see <a>ledgerViewForecastAt</a>), then the length of their
--   candidate chains will typically be the length of their selections,
--   since the ChainSync is free to race ahead (in contrast, the BlockFetch
--   pipeline depth is bounded such that it will, for a syncing node, not
--   be able to request all blocks between the selection and the end of the
--   forecast window). But if one or more of their tips is beyond the
--   horizon, then the relative length of the candidate chains is more
--   complicated, influenced by both the relative density of the chains'
--   suffixes and the relative age of the chains' intersection with the
--   node's selection (since each peer's forecast horizon is a fixed number
--   of slots after the candidate's successor of that intersection).</li>
--   <li><a>FetchDeclineConcurrencyLimit</a>: decline this peer while the
--   node has already fully allocated the artificially scarce
--   <tt>maxConcurrentFetchPeers</tt> resource amongst its other peers.
--   This reason reflects the least-fundamental competition: it's the only
--   way a node would decline a candidate chain C that it would immediately
--   switch to if C had somehow already been fetched (and any better
--   current candidates hadn't). It is possible that this peer's candidate
--   fragment is better than the candidate fragments of other peers, but
--   that should only happen ephemerally (eg for a brief while immediately
--   after first connecting to this peer).</li>
--   <li><a>FetchDeclineChainIntersectionTooDeep</a>: decline this peer
--   because the node's selection has more than <tt>K</tt> blocks that are
--   not on this peer's candidate chain. Typically, this reason occurs
--   after the node has been declined---ie lost the above
--   competitions---for a long enough duration. This decision only arises
--   if the BlockFetch decision logic wins a harmless race against the
--   ChainSync client once the node's selection gets longer, since
--   <a>ForkTooDeep</a> disconnects from such a peer.</li>
--   </ul>
data FetchDecline

-- | This peer's candidate chain is not longer than our chain. For more
--   details see <a>mkBlockFetchConsensusInterface</a> which implements
--   <a>plausibleCandidateChain</a>.
FetchDeclineChainNotPlausible :: FetchDecline

-- | Switching to this peer's candidate chain would require rolling back
--   more than <tt>K</tt> blocks.
FetchDeclineChainIntersectionTooDeep :: FetchDecline

-- | Every block on this peer's candidate chain has already been fetched.
FetchDeclineAlreadyFetched :: FetchDecline

-- | This peer's candidate chain has already been requested from this peer.
FetchDeclineInFlightThisPeer :: FetchDecline

-- | Some blocks on this peer's candidate chain have not yet been fetched,
--   but all of those have already been requested from other peers.
FetchDeclineInFlightOtherPeer :: FetchDecline

-- | This peer's BlockFetch client is shutting down, see
--   <a>PeerFetchStatusShutdown</a>.
FetchDeclinePeerShutdown :: FetchDecline

-- | Blockfetch is starting up and waiting on corresponding Chainsync.
FetchDeclinePeerStarting :: FetchDecline

-- | This peer is in a potentially-temporary state in which it has not
--   responded to us within a certain expected time limit, see
--   <a>PeerFetchStatusAberrant</a>.
FetchDeclinePeerSlow :: FetchDecline

-- | This peer is not under the <a>maxInFlightReqsPerPeer</a> limit.
--   
--   The argument is the <a>maxInFlightReqsPerPeer</a> constant.
FetchDeclineReqsInFlightLimit :: !Word -> FetchDecline

-- | This peer is not under the <a>inFlightBytesHighWatermark</a> bytes
--   limit.
--   
--   The arguments are:
--   
--   <ul>
--   <li>number of bytes currently in flight for that peer</li>
--   <li>the configured <a>inFlightBytesLowWatermark</a> constant</li>
--   <li>the configured <a>inFlightBytesHighWatermark</a> constant</li>
--   </ul>
FetchDeclineBytesInFlightLimit :: !SizeInBytes -> !SizeInBytes -> !SizeInBytes -> FetchDecline

-- | This peer is not under the <a>inFlightBytesLowWatermark</a>.
--   
--   The arguments are:
--   
--   <ul>
--   <li>number of bytes currently in flight for that peer</li>
--   <li>the configured <a>inFlightBytesLowWatermark</a> constant</li>
--   <li>the configured <a>inFlightBytesHighWatermark</a> constant</li>
--   </ul>
FetchDeclinePeerBusy :: !SizeInBytes -> !SizeInBytes -> !SizeInBytes -> FetchDecline

-- | The node is not under the <tt>maxConcurrentFetchPeers</tt> limit.
--   
--   The arguments are:
--   
--   <ul>
--   <li>the current <a>FetchMode</a></li>
--   <li>the corresponding configured limit constant, either
--   <a>maxConcurrencyBulkSync</a> or <a>maxConcurrencyDeadline</a></li>
--   </ul>
FetchDeclineConcurrencyLimit :: !FetchMode -> !Word -> FetchDecline
data () => FetchMode
FetchModeBulkSync :: FetchMode
FetchModeDeadline :: FetchMode
data () => TraceLabelPeer peerid a
TraceLabelPeer :: peerid -> a -> TraceLabelPeer peerid a

-- | Tracing types for the various events that change the state (i.e.
--   <a>FetchClientStateVars</a>) for a block fetch client.
--   
--   Note that while these are all state changes, the
--   <a>AddedFetchRequest</a> occurs in the decision thread while the other
--   state changes occur in the block fetch client threads.
data TraceFetchClientState header

-- | The block fetch decision thread has added a new fetch instruction
--   consisting of one or more individual request ranges.
AddedFetchRequest :: FetchRequest header -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | Mark the point when the fetch client picks up the request added by the
--   block fetch decision thread. Note that this event can happen fewer
--   times than the <a>AddedFetchRequest</a> due to fetch request merging.
AcknowledgedFetchRequest :: FetchRequest header -> TraceFetchClientState header

-- | Mark the point when fetch request for a fragment is actually sent over
--   the wire.
SendFetchRequest :: AnchoredFragment header -> PeerGSV -> TraceFetchClientState header

-- | Mark the start of receiving a streaming batch of blocks. This will be
--   followed by one or more <a>CompletedBlockFetch</a> and a final
--   <a>CompletedFetchBatch</a>.
StartedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | Mark the completion of of receiving a single block within a streaming
--   batch of blocks.
CompletedBlockFetch :: Point header -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> NominalDiffTime -> SizeInBytes -> TraceFetchClientState header

-- | Mark the successful end of receiving a streaming batch of blocks
CompletedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | If the other peer rejects our request then we have this event instead
--   of <a>StartedFetchBatch</a> and <a>CompletedFetchBatch</a>.
RejectedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | The client is terminating. Log the number of outstanding requests.
ClientTerminating :: Int -> TraceFetchClientState header
instance (Ouroboros.Network.Block.StandardHash block, Ouroboros.Network.Block.StandardHash header, GHC.Classes.Eq peer) => GHC.Classes.Eq (Ouroboros.Network.BlockFetch.State.FetchStateFingerprint peer header block)

module Ouroboros.Network.BlockFetch.Client

-- | The implementation of the client side of block fetch protocol designed
--   to work in conjunction with our fetch logic.
blockFetchClient :: forall header block versionNumber m. (MonadSTM m, MonadThrow m, MonadTime m, MonadMonotonicTime m, HasHeader header, HasHeader block, HeaderHash header ~ HeaderHash block) => versionNumber -> ControlMessageSTM m -> FetchedMetricsTracer m -> FetchClientContext header block m -> PeerPipelined (BlockFetch block (Point block)) AsClient BFIdle m ()

-- | TODO: use a fetch client wrapper type rather than the raw
--   PeerPipelined, and eliminate this alias. It is only here to avoid
--   large types leaking into the consensus layer.
type BlockFetchClient header block m a = FetchClientContext header block m -> PeerPipelined (BlockFetch block (Point block)) AsClient BFIdle m a

-- | The context that is passed into the block fetch protocol client when
--   it is started.
data FetchClientContext header block m

-- | Tracing types for the various events that change the state (i.e.
--   <a>FetchClientStateVars</a>) for a block fetch client.
--   
--   Note that while these are all state changes, the
--   <a>AddedFetchRequest</a> occurs in the decision thread while the other
--   state changes occur in the block fetch client threads.
data TraceFetchClientState header
newtype FetchRequest header
FetchRequest :: [AnchoredFragment header] -> FetchRequest header
[fetchRequestFragments] :: FetchRequest header -> [AnchoredFragment header]

-- | A set of variables shared between the block fetch logic thread and
--   each thread executing the client side of the block fetch protocol.
--   That is, these are the shared variables per peer. The
--   <tt>FetchClientRegistry</tt> contains the mapping of these for all
--   peers.
--   
--   The variables are used for communicating from the protocol thread to
--   the decision making thread the status of things with that peer. And in
--   the other direction one shared variable is for providing new fetch
--   requests.
data FetchClientStateVars m header
data BlockFetchProtocolFailure
instance GHC.Show.Show Ouroboros.Network.BlockFetch.Client.BlockFetchProtocolFailure
instance GHC.Classes.Eq Ouroboros.Network.BlockFetch.Client.BlockFetchProtocolFailure
instance GHC.Exception.Type.Exception Ouroboros.Network.BlockFetch.Client.BlockFetchProtocolFailure

module Ouroboros.Network.BlockFetch.ClientRegistry

-- | A registry for the threads that are executing the client side of the
--   <tt>BlockFetch</tt> protocol to communicate with our peers.
--   
--   The registry contains the shared variables we use to communicate with
--   these threads, both to track their status and to provide instructions.
--   
--   The threads add/remove themselves to/from this registry when they
--   start up and shut down.
data FetchClientRegistry peer header block m
FetchClientRegistry :: StrictTMVar m (Tracer m (TraceLabelPeer peer (TraceFetchClientState header)), WhetherReceivingTentativeBlocks -> STM m (FetchClientPolicy header block m)) -> StrictTVar m (Map peer (FetchClientStateVars m header)) -> StrictTVar m (Map peer (ThreadId m, StrictTMVar m (), StrictTMVar m ())) -> StrictTVar m (Map peer PeerGSV) -> StrictTVar m (Map peer (ThreadId m, StrictTMVar m ())) -> StrictTVar m (Set peer) -> FetchClientRegistry peer header block m
[fcrCtxVar] :: FetchClientRegistry peer header block m -> StrictTMVar m (Tracer m (TraceLabelPeer peer (TraceFetchClientState header)), WhetherReceivingTentativeBlocks -> STM m (FetchClientPolicy header block m))
[fcrFetchRegistry] :: FetchClientRegistry peer header block m -> StrictTVar m (Map peer (FetchClientStateVars m header))
[fcrSyncRegistry] :: FetchClientRegistry peer header block m -> StrictTVar m (Map peer (ThreadId m, StrictTMVar m (), StrictTMVar m ()))
[fcrDqRegistry] :: FetchClientRegistry peer header block m -> StrictTVar m (Map peer PeerGSV)
[fcrKeepRegistry] :: FetchClientRegistry peer header block m -> StrictTVar m (Map peer (ThreadId m, StrictTMVar m ()))
[fcrDying] :: FetchClientRegistry peer header block m -> StrictTVar m (Set peer)
newFetchClientRegistry :: MonadSTM m => m (FetchClientRegistry peer header block m)

-- | This is needed to start a block fetch client. It provides the required
--   <a>FetchClientContext</a>. It registers and unregisters the fetch
--   client on start and end.
--   
--   It also manages synchronisation with the corresponding chain sync
--   client.
bracketFetchClient :: forall m a peer header block version. (MonadSTM m, MonadFork m, MonadMask m, Ord peer) => FetchClientRegistry peer header block m -> version -> (version -> WhetherReceivingTentativeBlocks) -> peer -> (FetchClientContext header block m -> m a) -> m a
bracketKeepAliveClient :: forall m a peer header block. (MonadSTM m, MonadFork m, MonadMask m, Ord peer) => FetchClientRegistry peer header block m -> peer -> (StrictTVar m (Map peer PeerGSV) -> m a) -> m a

-- | The block fetch and chain sync clients for each peer need to
--   synchronise their startup and shutdown. This bracket operation
--   provides that synchronisation for the chain sync client.
--   
--   This must be used for the chain sync client <i>outside</i> of its own
--   state registration and deregistration.
bracketSyncWithFetchClient :: forall m a peer header block. (MonadSTM m, MonadFork m, MonadCatch m, Ord peer) => FetchClientRegistry peer header block m -> peer -> m a -> m a
setFetchClientContext :: MonadSTM m => FetchClientRegistry peer header block m -> Tracer m (TraceLabelPeer peer (TraceFetchClientState header)) -> (WhetherReceivingTentativeBlocks -> STM m (FetchClientPolicy header block m)) -> m ()

-- | The policy used by the fetch clients. It is set by the central block
--   fetch logic, and passed to them via the <tt>FetchClientRegistry</tt>.
data FetchClientPolicy header block m
FetchClientPolicy :: (header -> SizeInBytes) -> (header -> block -> Bool) -> (Point block -> block -> m ()) -> (FromConsensus block -> STM m UTCTime) -> FetchClientPolicy header block m
[blockFetchSize] :: FetchClientPolicy header block m -> header -> SizeInBytes
[blockMatchesHeader] :: FetchClientPolicy header block m -> header -> block -> Bool
[addFetchedBlock] :: FetchClientPolicy header block m -> Point block -> block -> m ()
[blockForgeUTCTime] :: FetchClientPolicy header block m -> FromConsensus block -> STM m UTCTime

-- | A read-only <a>STM</a> action to get the current
--   <a>PeerFetchStatus</a> for all fetch clients in the
--   <a>FetchClientRegistry</a>.
readFetchClientsStatus :: MonadSTM m => FetchClientRegistry peer header block m -> STM m (Map peer (PeerFetchStatus header))

-- | A read-only <a>STM</a> action to get the <a>FetchClientStateVars</a>
--   for all fetch clients in the <a>FetchClientRegistry</a>.
readFetchClientsStateVars :: MonadSTM m => FetchClientRegistry peer header block m -> STM m (Map peer (FetchClientStateVars m header))

-- | A read-only <a>STM</a> action to get the <a>PeerGSV</a>s for all fetch
--   clients in the <a>FetchClientRegistry</a>.
readPeerGSVs :: forall block header m peer. (MonadSTM m, Ord peer) => FetchClientRegistry peer header block m -> STM m (Map peer PeerGSV)


-- | Let's start with the big picture...
--   
--   <pre>
--   Key:  ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó  ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì   ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
--         ‚îÉ STM-based  ‚îÉ  ‚ïëactive thread‚ïë  ‚îÉstate instance‚îÉ‚îì  ‚ïë one thread ‚ïë‚ïó
--         ‚îÉshared state‚îÉ  ‚ïë             ‚ïë  ‚îÉ   per peer   ‚îÉ‚îÉ  ‚ïë  per peer  ‚ïë‚ïë
--         ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ‚îÉ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïë
--                                           ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
--   </pre>
--   
--   <pre>
--     ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó     ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
--     ‚ïë Chain sync  ‚ïë‚ïó    ‚îÉ   Ledger    ‚îÉ
--     ‚ïë  protocol   ‚ïë‚ïë‚óÄ‚îÄ‚îÄ‚îÄ‚î®   state     ‚îÉ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
--     ‚ïë(client side)‚ïë‚ïë    ‚îÉ             ‚îÉ            ‚îÇ
--     ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïë    ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ            ‚îÇ
--      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù                               ‚îÇ
--            ‚ñº                                       ‚îÇ
--     ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì     ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì     ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
--     ‚îÉ  Candidate  ‚îÉ     ‚îÉ   Set of    ‚îÉ     ‚ïë  Chain and  ‚ïë
--     ‚îÉ  chains     ‚îÉ     ‚îÉ  downloaded ‚î†‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚ïë   ledger    ‚ïë
--     ‚îÉ  (headers)  ‚îÉ     ‚îÉ   blocks    ‚îÉ     ‚ïë  validation ‚ïë
--     ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îØ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ     ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îØ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ     ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
--           ‚îÇ                   ‚îÇ ‚ñ≤                  ‚îÇ
--           ‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ                  ‚îÇ
--   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñº‚ñë‚ñº‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë           ‚îÇ                  ‚ñº
--   ‚ñë‚ñë‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó‚ñë‚ñë           ‚îÇ           ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì     ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
--   ‚ñë‚ñë‚ïë    Block    ‚ïë‚ñë‚ñë           ‚îÇ           ‚îÉ   Current   ‚îÉ     ‚ïë Block fetch ‚ïë‚ïó
--   ‚ñë‚ñë‚ï¢    fetch    ‚ïë‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î®    chain    ‚î†‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚ïë protocol    ‚ïë‚ïë
--   ‚ñë‚ñë‚ïë    logic    ‚ïë‚ñë‚ñë           ‚îÇ           ‚îÉ  (blocks)   ‚îÉ     ‚ïë(server side)‚ïë‚ïë
--   ‚ñë‚ñë‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ñë‚ñë           ‚îÇ           ‚î†‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î®     ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïë
--   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñ≤‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë           ‚îÇ           ‚îÉ  Tentative  ‚îÉ      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
--   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñº‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   ‚îÉ    chain    ‚î†‚îÄ‚îÄ‚ïÆ
--   ‚ñë‚ñë‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì‚ñë‚ñë‚ñë‚ñë‚ñë‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó‚ñë‚ñë   ‚îÉ  (headers)  ‚îÉ  ‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
--   ‚ñë‚ñë‚îÉ Block fetch ‚îÉ‚îì‚ñë‚ñë‚ñë‚ñë‚ïë block fetch ‚ïë‚ïó‚ñë   ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ  ‚îÇ  ‚ïë Chain sync  ‚ïë‚ïó
--   ‚ñë‚ñë‚îÉ  state and  ‚îÉ‚îÉ‚óÄ‚îÄ‚îÄ‚ñ∂‚ïë  protocol   ‚ïë‚ïë‚ñë                    ‚ï∞‚îÄ‚ñ∂‚ïë protocol    ‚ïë‚ïë
--   ‚ñë‚ñë‚îÉ  requests   ‚îÉ‚îÉ‚ñë‚ñë‚ñë‚ñë‚ïë(client side)‚ïë‚ïë‚ñë                       ‚ïë(server side)‚ïë‚ïë
--   ‚ñë‚ñë‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ‚îÉ‚ñë‚ñë‚ñë‚ñë‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïë‚ñë                       ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïë
--   ‚ñë‚ñë‚ñë‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ‚ñë‚ñë‚ñë‚ñë‚ñë‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ñë                        ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
--   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
--   </pre>
--   
--   Notes:
--   
--   <ul>
--   <li>Thread communication is via STM based state.</li>
--   <li>Outbound: threads update STM state.</li>
--   <li>Inbound: threads wait on STM state changing (using retry).</li>
--   <li>These are no queues: there is only the current state, not all
--   change events.</li>
--   </ul>
--   
--   We consider the block fetch logic and the policy for the block fetch
--   protocol client together as one unit of functionality. This is the
--   shaded area in the diagram.
--   
--   Looking at the diagram we see that these two threads interact with
--   each other and other threads via the following shared state
--   
--   TODO: table
--   
--   The block fetch requests state is private between the block fetch
--   logic and the block fetch protocol client, so it is implemented here.
--   
--   The other state is managed by the consensus layer and is considered
--   external here. So here we define interfaces for interacting with the
--   external state. These have to be provided when instantiating the block
--   fetch logic.
module Ouroboros.Network.BlockFetch

-- | Execute the block fetch logic. It monitors the current chain and
--   candidate chains. It decided which block bodies to fetch and manages
--   the process of fetching them, including making alternative decisions
--   based on timeouts and failures.
--   
--   This runs forever and should be shut down using mechanisms such as
--   async.
blockFetchLogic :: forall addr header block m. (HasHeader header, HasHeader block, HeaderHash header ~ HeaderHash block, MonadDelay m, MonadSTM m, Ord addr, Hashable addr) => Tracer m [TraceLabelPeer addr (FetchDecision [Point header])] -> Tracer m (TraceLabelPeer addr (TraceFetchClientState header)) -> BlockFetchConsensusInterface addr header block m -> FetchClientRegistry addr header block m -> BlockFetchConfiguration -> m Void

-- | Configuration for FetchDecisionPolicy. Should be determined by
--   external local node config.
data BlockFetchConfiguration
BlockFetchConfiguration :: !Word -> !Word -> !Word -> !DiffTime -> !Int -> BlockFetchConfiguration

-- | Maximum concurrent downloads during bulk syncing.
[bfcMaxConcurrencyBulkSync] :: BlockFetchConfiguration -> !Word

-- | Maximum concurrent downloads during deadline syncing.
[bfcMaxConcurrencyDeadline] :: BlockFetchConfiguration -> !Word

-- | Maximum requests in flight per each peer.
[bfcMaxRequestsInflight] :: BlockFetchConfiguration -> !Word

-- | Desired interval between calls to fetchLogicIteration
[bfcDecisionLoopInterval] :: BlockFetchConfiguration -> !DiffTime

-- | Salt used when comparing peers
[bfcSalt] :: BlockFetchConfiguration -> !Int
data () => BlockFetchConsensusInterface peer header block (m :: Type -> Type)
BlockFetchConsensusInterface :: STM m (Map peer (AnchoredFragment header)) -> STM m (AnchoredFragment header) -> STM m FetchMode -> STM m (Point block -> Bool) -> (WhetherReceivingTentativeBlocks -> STM m (Point block -> block -> m ())) -> STM m MaxSlotNo -> (HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Bool) -> (HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Ordering) -> (header -> SizeInBytes) -> (header -> block -> Bool) -> (FromConsensus header -> STM m UTCTime) -> (FromConsensus block -> STM m UTCTime) -> BlockFetchConsensusInterface peer header block (m :: Type -> Type)
[readCandidateChains] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> STM m (Map peer (AnchoredFragment header))
[readCurrentChain] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> STM m (AnchoredFragment header)
[readFetchMode] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> STM m FetchMode
[readFetchedBlocks] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> STM m (Point block -> Bool)
[mkAddFetchedBlock] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> WhetherReceivingTentativeBlocks -> STM m (Point block -> block -> m ())
[readFetchedMaxSlotNo] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> STM m MaxSlotNo
[plausibleCandidateChain] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Bool
[compareCandidateChains] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> HasCallStack => AnchoredFragment header -> AnchoredFragment header -> Ordering
[blockFetchSize] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> header -> SizeInBytes
[blockMatchesHeader] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> header -> block -> Bool
[headerForgeUTCTime] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> FromConsensus header -> STM m UTCTime
[blockForgeUTCTime] :: BlockFetchConsensusInterface peer header block (m :: Type -> Type) -> FromConsensus block -> STM m UTCTime

-- | Throughout the decision making process we accumulate reasons to
--   decline to fetch any blocks. This type is used to wrap intermediate
--   and final results.
type FetchDecision result = Either FetchDecline result

-- | Tracing types for the various events that change the state (i.e.
--   <a>FetchClientStateVars</a>) for a block fetch client.
--   
--   Note that while these are all state changes, the
--   <a>AddedFetchRequest</a> occurs in the decision thread while the other
--   state changes occur in the block fetch client threads.
data TraceFetchClientState header

-- | The block fetch decision thread has added a new fetch instruction
--   consisting of one or more individual request ranges.
AddedFetchRequest :: FetchRequest header -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | Mark the point when the fetch client picks up the request added by the
--   block fetch decision thread. Note that this event can happen fewer
--   times than the <a>AddedFetchRequest</a> due to fetch request merging.
AcknowledgedFetchRequest :: FetchRequest header -> TraceFetchClientState header

-- | Mark the point when fetch request for a fragment is actually sent over
--   the wire.
SendFetchRequest :: AnchoredFragment header -> PeerGSV -> TraceFetchClientState header

-- | Mark the start of receiving a streaming batch of blocks. This will be
--   followed by one or more <a>CompletedBlockFetch</a> and a final
--   <a>CompletedFetchBatch</a>.
StartedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | Mark the completion of of receiving a single block within a streaming
--   batch of blocks.
CompletedBlockFetch :: Point header -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> NominalDiffTime -> SizeInBytes -> TraceFetchClientState header

-- | Mark the successful end of receiving a streaming batch of blocks
CompletedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | If the other peer rejects our request then we have this event instead
--   of <a>StartedFetchBatch</a> and <a>CompletedFetchBatch</a>.
RejectedFetchBatch :: ChainRange (Point header) -> PeerFetchInFlight header -> PeerFetchInFlightLimits -> PeerFetchStatus header -> TraceFetchClientState header

-- | The client is terminating. Log the number of outstanding requests.
ClientTerminating :: Int -> TraceFetchClientState header
data () => TraceLabelPeer peerid a
TraceLabelPeer :: peerid -> a -> TraceLabelPeer peerid a

-- | A registry for the threads that are executing the client side of the
--   <tt>BlockFetch</tt> protocol to communicate with our peers.
--   
--   The registry contains the shared variables we use to communicate with
--   these threads, both to track their status and to provide instructions.
--   
--   The threads add/remove themselves to/from this registry when they
--   start up and shut down.
data FetchClientRegistry peer header block m
newFetchClientRegistry :: MonadSTM m => m (FetchClientRegistry peer header block m)

-- | This is needed to start a block fetch client. It provides the required
--   <a>FetchClientContext</a>. It registers and unregisters the fetch
--   client on start and end.
--   
--   It also manages synchronisation with the corresponding chain sync
--   client.
bracketFetchClient :: forall m a peer header block version. (MonadSTM m, MonadFork m, MonadMask m, Ord peer) => FetchClientRegistry peer header block m -> version -> (version -> WhetherReceivingTentativeBlocks) -> peer -> (FetchClientContext header block m -> m a) -> m a

-- | The block fetch and chain sync clients for each peer need to
--   synchronise their startup and shutdown. This bracket operation
--   provides that synchronisation for the chain sync client.
--   
--   This must be used for the chain sync client <i>outside</i> of its own
--   state registration and deregistration.
bracketSyncWithFetchClient :: forall m a peer header block. (MonadSTM m, MonadFork m, MonadCatch m, Ord peer) => FetchClientRegistry peer header block m -> peer -> m a -> m a
bracketKeepAliveClient :: forall m a peer header block. (MonadSTM m, MonadFork m, MonadMask m, Ord peer) => FetchClientRegistry peer header block m -> peer -> (StrictTVar m (Map peer PeerGSV) -> m a) -> m a
data () => FetchMode
FetchModeBulkSync :: FetchMode
FetchModeDeadline :: FetchMode
newtype () => FromConsensus a
FromConsensus :: a -> FromConsensus a
[unFromConsensus] :: FromConsensus a -> a
data () => SizeInBytes
data () => WhetherReceivingTentativeBlocks
ReceivingTentativeBlocks :: WhetherReceivingTentativeBlocks
NotReceivingTentativeBlocks :: WhetherReceivingTentativeBlocks
instance GHC.Show.Show Ouroboros.Network.BlockFetch.BlockFetchConfiguration

module Ouroboros.Network.ExitPolicy

-- | After demoting a peer to Warm or Cold, we use a delay to re-promote it
--   back.
newtype RepromoteDelay
RepromoteDelay :: DiffTime -> RepromoteDelay
[repromoteDelay] :: RepromoteDelay -> DiffTime

-- | Deprecated in `ouroboros-network-0.11.0.0`

-- | <i>Deprecated: Use RepromoteDelay instead</i>
type ReconnectDelay = RepromoteDelay

-- | <a>ReturnPolicy</a> allows to compute reconnection delay from value
--   return by a mini-protocol. If a mini-protocol returned with an error
--   <a>epErrorDelay</a> is used.
data ExitPolicy a
ExitPolicy :: ReturnPolicy a -> RepromoteDelay -> ExitPolicy a

-- | Compute <a>RepromoteDelay</a> from a return value.
[epReturnDelay] :: ExitPolicy a -> ReturnPolicy a

-- | The delay when a mini-protocol returned with an error.
[epErrorDelay] :: ExitPolicy a -> RepromoteDelay

-- | <a>ExitPolicy</a> with 10s error delay.
stdExitPolicy :: ReturnPolicy a -> ExitPolicy a
type ReturnPolicy a = a -> RepromoteDelay
alwaysCleanReturnPolicy :: RepromoteDelay -> ExitPolicy a
instance GHC.Base.Semigroup Ouroboros.Network.ExitPolicy.RepromoteDelay
instance GHC.Real.Fractional Ouroboros.Network.ExitPolicy.RepromoteDelay
instance GHC.Num.Num Ouroboros.Network.ExitPolicy.RepromoteDelay
instance GHC.Classes.Ord Ouroboros.Network.ExitPolicy.RepromoteDelay
instance GHC.Classes.Eq Ouroboros.Network.ExitPolicy.RepromoteDelay
instance GHC.Show.Show Ouroboros.Network.ExitPolicy.RepromoteDelay

module Ouroboros.Network.KeepAlive
newtype KeepAliveInterval
KeepAliveInterval :: DiffTime -> KeepAliveInterval
[keepAliveInterval] :: KeepAliveInterval -> DiffTime
keepAliveClient :: forall m peer. (MonadTimer m, Ord peer) => Tracer m (TraceKeepAliveClient peer) -> StdGen -> ControlMessageSTM m -> peer -> StrictTVar m (Map peer PeerGSV) -> KeepAliveInterval -> KeepAliveClient m ()
keepAliveServer :: forall m. Applicative m => KeepAliveServer m ()
data TraceKeepAliveClient peer
AddSample :: peer -> DiffTime -> PeerGSV -> TraceKeepAliveClient peer
instance GHC.Show.Show peer => GHC.Show.Show (Ouroboros.Network.KeepAlive.TraceKeepAliveClient peer)

module Ouroboros.Network.PeerSelection.RootPeersDNS.DNSActions

-- | Dictionary of DNS actions vocabulary
data DNSActions resolver exception m
DNSActions :: (ResolvConf -> m (Resource m (Either (DNSorIOError exception) resolver))) -> (ResolvConf -> m (Resource m (Either (DNSorIOError exception) resolver))) -> (ResolvConf -> resolver -> Domain -> m ([DNSError], [(IP, TTL)])) -> DNSActions resolver exception m

-- | TODO: it could be useful for <tt>publicRootPeersProvider</tt>.
[dnsResolverResource] :: DNSActions resolver exception m -> ResolvConf -> m (Resource m (Either (DNSorIOError exception) resolver))

-- | <a>Resource</a> which passes the <a>Resolver</a> (or abstract resolver
--   type) through a <a>StrictTVar</a>. Better than
--   <tt>resolverResource</tt> when using in multiple threads.
--   
--   On <i>Windows</i> we use a different implementation which always
--   returns a newly initialised <a>Resolver</a> at each step. This is
--   because on <i>Windows</i> we don't have a way to check that the
--   network configuration has changed. The <tt>dns</tt> library is using
--   'GetNetworkParams@ win32 api call to get the list of default dns
--   servers.
[dnsAsyncResolverResource] :: DNSActions resolver exception m -> ResolvConf -> m (Resource m (Either (DNSorIOError exception) resolver))

-- | Like <a>lookupA</a> but also return the TTL for the results.
--   
--   DNS library timeouts do not work reliably on Windows (#1873), hence
--   the additional timeout.
[dnsLookupWithTTL] :: DNSActions resolver exception m -> ResolvConf -> resolver -> Domain -> m ([DNSError], [(IP, TTL)])

-- | IO DNSActions which resolve domain names with <a>Resolver</a>.
--   
--   The IPv4 and IPv6 addresses the node will be using should determine
--   the LookupReqs so that we can avoid lookups for address types that
--   wont be used.
--   
--   It guarantees that returned TTLs are strictly greater than 0.
ioDNSActions :: DNSLookupType -> DNSActions Resolver IOException IO
data DNSLookupType
LookupReqAOnly :: DNSLookupType
LookupReqAAAAOnly :: DNSLookupType
LookupReqAAndAAAA :: DNSLookupType

-- | Evolving resource; We use it to reinitialise the DNS library if the
--   `<i>etc</i>resolv.conf` file was modified.
--   
--   Note: <a>constantResource</a> and <a>retryResource</a> are written
--   using a simplified approach inspired by _"The Different Aspects of
--   Monads and Mixins"_, by Bruno C. d S. Oliveira, see
--   <a>https://www.youtube.com/watch?v=pfwP4hXM5hA</a>.
newtype Resource m a
Resource :: m (a, Resource m a) -> Resource m a
[withResource] :: Resource m a -> m (a, Resource m a)

-- | A <a>Resource</a> which will exhibit the given <a>Resource</a> but
--   retry it with a given delay until a success. On first success it will
--   reset the delays.
retryResource :: forall m e a. MonadDelay m => Tracer m e -> NonEmpty DiffTime -> Resource m (Either e a) -> Resource m a
constantResource :: forall m a. Applicative m => a -> Resource m a
data DNSorIOError exception
DNSError :: !DNSError -> DNSorIOError exception
IOError :: !exception -> DNSorIOError exception
instance GHC.Show.Show Ouroboros.Network.PeerSelection.RootPeersDNS.DNSActions.DNSLookupType
instance GHC.Show.Show exception => GHC.Show.Show (Ouroboros.Network.PeerSelection.RootPeersDNS.DNSActions.DNSorIOError exception)
instance GHC.Base.Functor m => GHC.Base.Functor (Ouroboros.Network.PeerSelection.RootPeersDNS.DNSActions.Resource m)
instance GHC.Exception.Type.Exception exception => GHC.Exception.Type.Exception (Ouroboros.Network.PeerSelection.RootPeersDNS.DNSActions.DNSorIOError exception)

module Ouroboros.Network.PeerSelection.RootPeersDNS.DNSSemaphore

-- | A semaphore used to limit concurrency of dns names resolution.
data DNSSemaphore m

-- | Create a <a>DNSSemaphore</a> for root and ledger peers.
newLedgerAndPublicRootDNSSemaphore :: MonadSTM m => m (DNSSemaphore m)

-- | Create a <a>DNSSemaphore</a> for local root peers.
newDNSLocalRootSemaphore :: MonadSTM m => STM m (DNSSemaphore m)

-- | Run a computation by attempting to acquire the semaphore first. On
--   termination or failure free the semaphore
withDNSSemaphore :: (MonadSTM m, MonadThrow m) => DNSSemaphore m -> m a -> m a

module Ouroboros.Network.PeerSelection.LedgerPeers

-- | Only use the ledger after the given slot number.
data UseLedgerAfter
DontUseLedger :: UseLedgerAfter
UseLedgerAfter :: SlotNo -> UseLedgerAfter

-- | Identifies a peer as coming from ledger or not
data IsLedgerPeer

-- | a ledger peer.
IsLedgerPeer :: IsLedgerPeer
IsNotLedgerPeer :: IsLedgerPeer
newtype NumberOfPeers
NumberOfPeers :: Word16 -> NumberOfPeers
[getNumberOfPeers] :: NumberOfPeers -> Word16
newtype LedgerPeersConsensusInterface m
LedgerPeersConsensusInterface :: (SlotNo -> STM m (Maybe [(PoolStake, NonEmpty RelayAccessPoint)])) -> LedgerPeersConsensusInterface m
[lpGetPeers] :: LedgerPeersConsensusInterface m -> SlotNo -> STM m (Maybe [(PoolStake, NonEmpty RelayAccessPoint)])

-- | Which ledger peers to pick.
data LedgerPeersKind
AllLedgerPeers :: LedgerPeersKind
BigLedgerPeers :: LedgerPeersKind

-- | Convert a list of pools with stake to a Map keyed on the accumulated
--   stake. Consensus provides a list of pairs of relative stake and
--   corresponding relays for all usable registered pools. By creating a
--   Map keyed on the <a>AccPoolStake</a> that is the sum of the pool's
--   relative stake and the stake of all preceding pools we can support
--   weighted random selection in O(log n) time by taking advantage of
--   Map.lookupGE (returns the smallest key greater or equal to the
--   provided value).
accPoolStake :: [(PoolStake, NonEmpty RelayAccessPoint)] -> Map AccPoolStake (PoolStake, NonEmpty RelayAccessPoint)

-- | Convert a list of pools with stake to a Map keyed on the accumulated
--   stake which only contains big ledger peers, e.g. largest ledger peers
--   which cumulatively control 90% of stake.
accBigPoolStake :: [(PoolStake, NonEmpty RelayAccessPoint)] -> Map AccPoolStake (PoolStake, NonEmpty RelayAccessPoint)

-- | The total accumulated stake of big ledger peers.
bigLedgerPeerQuota :: AccPoolStake

-- | For a LedgerPeers worker thread and submit request and receive
--   responses.
withLedgerPeers :: forall peerAddr resolver exception m a. (MonadAsync m, MonadThrow m, MonadMonotonicTime m, Exception exception, Ord peerAddr) => StdGen -> DNSSemaphore m -> (IP -> PortNumber -> peerAddr) -> Tracer m TraceLedgerPeers -> STM m UseLedgerAfter -> LedgerPeersConsensusInterface m -> DNSActions resolver exception m -> ((NumberOfPeers -> LedgerPeersKind -> m (Maybe (Set peerAddr, DiffTime))) -> Async m Void -> m a) -> m a

-- | Only use the ledger after the given slot number.
data UseLedgerAfter
DontUseLedger :: UseLedgerAfter
UseLedgerAfter :: SlotNo -> UseLedgerAfter
isLedgerPeersEnabled :: UseLedgerAfter -> Bool
newtype NumberOfPeers
NumberOfPeers :: Word16 -> NumberOfPeers
[getNumberOfPeers] :: NumberOfPeers -> Word16

-- | Identifies a peer as coming from ledger or not
data IsLedgerPeer

-- | a ledger peer.
IsLedgerPeer :: IsLedgerPeer
IsNotLedgerPeer :: IsLedgerPeer

-- | Trace LedgerPeers events.
data TraceLedgerPeers

-- | Trace for a significant ledger peer picked with accumulated and
--   relative stake of its pool.
PickedBigLedgerPeer :: RelayAccessPoint -> AccPoolStake -> PoolStake -> TraceLedgerPeers

-- | Trace for a ledger peer picked with accumulated and relative stake of
--   its pool.
PickedLedgerPeer :: RelayAccessPoint -> AccPoolStake -> PoolStake -> TraceLedgerPeers
PickedBigLedgerPeers :: NumberOfPeers -> [RelayAccessPoint] -> TraceLedgerPeers

-- | Trace for the number of peers and we wanted to pick and the list of
--   peers picked.
PickedLedgerPeers :: NumberOfPeers -> [RelayAccessPoint] -> TraceLedgerPeers

-- | Trace for fetching a new list of peers from the ledger. The first Int
--   is the number of ledger peers returned the latter is the number of big
--   ledger peers.
FetchingNewLedgerState :: Int -> Int -> TraceLedgerPeers
TraceLedgerPeersDomains :: [DomainAccessPoint] -> TraceLedgerPeers
TraceLedgerPeersResult :: Domain -> [(IP, TTL)] -> TraceLedgerPeers
TraceLedgerPeersFailure :: Domain -> DNSError -> TraceLedgerPeers

-- | Trace for when getting peers from the ledger is disabled, that is
--   DontUseLedger.
DisabledLedgerPeers :: TraceLedgerPeers

-- | Trace UseLedgerAfter value
TraceUseLedgerAfter :: UseLedgerAfter -> TraceLedgerPeers
WaitingOnRequest :: TraceLedgerPeers
RequestForPeers :: NumberOfPeers -> TraceLedgerPeers
ReusingLedgerState :: Int -> DiffTime -> TraceLedgerPeers
FallingBackToPublicRootPeers :: TraceLedgerPeers
NotEnoughBigLedgerPeers :: NumberOfPeers -> Int -> TraceLedgerPeers
NotEnoughLedgerPeers :: NumberOfPeers -> Int -> TraceLedgerPeers

-- | Provides DNS resolution functionality.
--   
--   Concurrently resolve DNS names, respecting the
--   <tt>maxDNSConcurrency</tt> limit.
resolveLedgerPeers :: forall m peerAddr resolver exception. (Ord peerAddr, MonadThrow m, MonadAsync m, Exception exception) => Tracer m TraceLedgerPeers -> (IP -> PortNumber -> peerAddr) -> DNSSemaphore m -> ResolvConf -> DNSActions resolver exception m -> [DomainAccessPoint] -> m (Map DomainAccessPoint (Set peerAddr))
instance GHC.Show.Show Ouroboros.Network.PeerSelection.LedgerPeers.LedgerPeersKind

module Ouroboros.Network.PeerSelection.RootPeersDNS.PublicRootPeers

publicRootPeersProvider :: forall peerAddr resolver exception a m. (MonadThrow m, MonadAsync m, Exception exception, Ord peerAddr) => Tracer m TracePublicRootPeers -> (IP -> PortNumber -> peerAddr) -> DNSSemaphore m -> ResolvConf -> STM m (Map RelayAccessPoint PeerAdvertise) -> DNSActions resolver exception m -> ((Int -> m (Map peerAddr PeerAdvertise, DiffTime)) -> m a) -> m a
data TracePublicRootPeers
TracePublicRootRelayAccessPoint :: Map RelayAccessPoint PeerAdvertise -> TracePublicRootPeers
TracePublicRootDomains :: [DomainAccessPoint] -> TracePublicRootPeers
TracePublicRootResult :: Domain -> [(IP, TTL)] -> TracePublicRootPeers
TracePublicRootFailure :: Domain -> DNSError -> TracePublicRootPeers
instance GHC.Show.Show Ouroboros.Network.PeerSelection.RootPeersDNS.PublicRootPeers.TracePublicRootPeers

module Ouroboros.Network.PeerSelection.State.EstablishedPeers

-- | The set of established peers. To a first approximation it can be
--   thought of as a <a>Set</a> of <tt>peeraddr</tt>.
--   
--   It has one special feature:
--   
--   <ul>
--   <li>It tracks which peers we are permitted to ask for peers now, or
--   for peers we cannot issue share requests with now the time at which we
--   would next be allowed to do so.</li>
--   </ul>
data EstablishedPeers peeraddr peerconn
empty :: EstablishedPeers peeraddr perconn

-- | <i>O(1)</i>
toMap :: EstablishedPeers peeraddr peerconn -> Map peeraddr peerconn

-- | <i>O(n)</i>
toSet :: EstablishedPeers peeraddr peerconn -> Set peeraddr

-- | Map of established peers that are either active or ready to be
--   promoted to active.
--   
--   <i>O(n log m), for n not-ready peers, and m established peers</i>
readyPeers :: Ord peeraddr => EstablishedPeers peeraddr peerconn -> Set peeraddr

-- | The number of established peers. The size of <a>allPeers</a>
--   
--   <i>O(1)</i>
size :: EstablishedPeers peeraddr peerconn -> Int

-- | The number of ready peers. The size of <a>readyPeers</a>
--   
--   <i>O(1)</i>
sizeReady :: EstablishedPeers peeraddr peerconn -> Int
member :: Ord peeraddr => peeraddr -> EstablishedPeers peeraddr peerconn -> Bool

-- | Insert a peer into <a>EstablishedPeers</a>.
insert :: Ord peeraddr => peeraddr -> peerconn -> Time -> EstablishedPeers peeraddr peerconn -> EstablishedPeers peeraddr peerconn
delete :: Ord peeraddr => peeraddr -> EstablishedPeers peeraddr peerconn -> EstablishedPeers peeraddr peerconn

-- | Bulk delete of peers from 'EstablishedPeers.
deletePeers :: Ord peeraddr => Set peeraddr -> EstablishedPeers peeraddr peerconn -> EstablishedPeers peeraddr peerconn
setCurrentTime :: Ord peeraddr => Time -> EstablishedPeers peeraddr peerconn -> EstablishedPeers peeraddr peerconn
setActivateTimes :: Ord peeraddr => Map peeraddr Time -> EstablishedPeers peeraddr peerconn -> EstablishedPeers peeraddr peerconn

-- | Find smallest activation time for a peer belonging to a given set.
minActivateTime :: Ord peeraddr => EstablishedPeers peeraddr peerconn -> (peeraddr -> Bool) -> Maybe Time

-- | The first time that a peer will become available for peer sharing. If
--   peers are already available for peer share, or there are no peers at
--   all then the result is <tt>Nothing</tt>.
minPeerShareTime :: Ord peeraddr => EstablishedPeers peeraddr peercon -> Maybe Time
setPeerShareTime :: Ord peeraddr => Set peeraddr -> Time -> EstablishedPeers peeraddr peercon -> EstablishedPeers peeraddr peercon

-- | The subset of established peers that we would be allowed to peer share
--   with now. This is because we have not peer shared with them recently.
--   
--   NOTE that this is the set of available peers one would be able to
--   perform peer sharing _now_, it doesn't mean they are 100% eligible.
--   This will depend on other factors like the peer's <tt>PeerSharing</tt>
--   value.
availableForPeerShare :: EstablishedPeers peeraddr peerconn -> Set peeraddr
invariant :: Ord peeraddr => EstablishedPeers peeraddr peerconn -> Bool
instance GHC.Base.Functor (Ouroboros.Network.PeerSelection.State.EstablishedPeers.EstablishedPeers peeraddr)
instance (GHC.Show.Show peeraddr, GHC.Show.Show peerconn) => GHC.Show.Show (Ouroboros.Network.PeerSelection.State.EstablishedPeers.EstablishedPeers peeraddr peerconn)

module Ouroboros.Network.PeerSelection.State.KnownPeers

-- | The set of known peers. To a first approximation it can be thought of
--   as a <a>Set</a> of <tt>peeraddr</tt>.
--   
--   It has one special feature:
--   
--   <ul>
--   <li>It tracks the subset of peers that we are happy to publish in
--   reply to peer share requests to our node. It supports random sampling
--   from this set.</li>
--   </ul>
data KnownPeers peeraddr
invariant :: Ord peeraddr => KnownPeers peeraddr -> Bool
alterKnownPeerInfo :: (Maybe PeerSharing, Maybe PeerAdvertise, Maybe IsLedgerPeer) -> Maybe KnownPeerInfo -> Maybe KnownPeerInfo
empty :: KnownPeers peeraddr
size :: KnownPeers peeraddr -> Int

-- | This inserts a map of peers with its respective peer sharing, peer
--   advertise and ledger flags into the known peers set.
--   
--   Please note that if in the map there's an entry for a peer already
--   present in the known peers set, then its values will only be
--   overwritten if they are a <a>Just</a>. Otherwise the current
--   information will be preserved. On the other hand if there's an entry
--   for a peer that isn't a member of the known peer set, the
--   <a>Nothing</a> values will default to <tt>NoPeerSharing</tt>,
--   <a>DoNotAdvertisePeer</a> and <a>IsNotLedgerPeer</a>, respectively,
--   unless a <a>Just</a> value is used.
insert :: Ord peeraddr => Map peeraddr (Maybe PeerSharing, Maybe PeerAdvertise, Maybe IsLedgerPeer) -> KnownPeers peeraddr -> KnownPeers peeraddr
alter :: Ord peeraddr => (Maybe KnownPeerInfo -> Maybe KnownPeerInfo) -> Set peeraddr -> KnownPeers peeraddr -> KnownPeers peeraddr
delete :: Ord peeraddr => Set peeraddr -> KnownPeers peeraddr -> KnownPeers peeraddr

-- | <i>O(n)</i>
toSet :: KnownPeers peeraddr -> Set peeraddr
member :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> Bool
setCurrentTime :: Ord peeraddr => Time -> KnownPeers peeraddr -> KnownPeers peeraddr
incrementFailCount :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> (Int, KnownPeers peeraddr)
resetFailCount :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> KnownPeers peeraddr
lookupFailCount :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> Maybe Int
lookupTepidFlag :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> Maybe Bool
setTepidFlag :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> KnownPeers peeraddr
clearTepidFlag :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> KnownPeers peeraddr
setSuccessfulConnectionFlag :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> KnownPeers peeraddr
minConnectTime :: Ord peeraddr => KnownPeers peeraddr -> (peeraddr -> Bool) -> Maybe Time
setConnectTimes :: Ord peeraddr => Map peeraddr Time -> KnownPeers peeraddr -> KnownPeers peeraddr

-- | The subset of known peers that we would be allowed to try to establish
--   a connection to now. This is because we have not connected with them
--   before or because any failure backoff time has expired.
availableToConnect :: KnownPeers peeraddr -> Set peeraddr
canPeerShareRequest :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> Bool

-- | Filter peers available for Peer Sharing requests, according to their
--   <a>PeerSharing</a> information
getPeerSharingRequestPeers :: Ord peeraddr => Set peeraddr -> KnownPeers peeraddr -> Set peeraddr
canSharePeers :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> Bool

-- | Filter peers available for Peer Sharing replies, according to their
--   <a>PeerAdvertise</a> information
getPeerSharingResponsePeers :: KnownPeers peeraddr -> Set peeraddr

-- | Select a random subset of the known peers that are available to
--   publish.
--   
--   The selection is done in such a way that when the same initial PRNG
--   state is used, the selected set does not significantly vary with small
--   perturbations in the set of published peers.
--   
--   The intention of this selection method is that the selection should
--   give approximately the same replies to the same peers over the course
--   of multiple requests from the same peer. This is to deliberately slow
--   the rate at which peers can discover and map out the entire network.
--   
--   Checks the KnownPeers Set for known ledger peers.
--   
--   This is used in Peer Selection Governor to filter out the known-to-te
--   ledger peers from the share result set.
isKnownLedgerPeer :: Ord peeraddr => peeraddr -> KnownPeers peeraddr -> Bool
instance GHC.Show.Show Ouroboros.Network.PeerSelection.State.KnownPeers.KnownPeerInfo
instance GHC.Classes.Eq Ouroboros.Network.PeerSelection.State.KnownPeers.KnownPeerInfo
instance GHC.Show.Show peeraddr => GHC.Show.Show (Ouroboros.Network.PeerSelection.State.KnownPeers.KnownPeers peeraddr)
instance GHC.Classes.Ord peeraddr => GHC.Classes.Eq (Ouroboros.Network.PeerSelection.State.KnownPeers.KnownPeers peeraddr)

module Ouroboros.Network.PeerSelection.State.LocalRootPeers
data LocalRootPeers peeraddr
LocalRootPeers :: Map peeraddr PeerAdvertise -> [(HotValency, WarmValency, Set peeraddr)] -> LocalRootPeers peeraddr

-- | Newtype wrapper representing hot valency value from local root group
--   configuration
newtype HotValency
HotValency :: Int -> HotValency
[getHotValency] :: HotValency -> Int

-- | Newtype wrapper representing warm valency value from local root group
--   configuration
newtype WarmValency
WarmValency :: Int -> WarmValency
[getWarmValency] :: WarmValency -> Int
invariant :: Ord peeraddr => LocalRootPeers peeraddr -> Bool
empty :: LocalRootPeers peeraddr
null :: LocalRootPeers peeraddr -> Bool
size :: LocalRootPeers peeraddr -> Int
member :: Ord peeraddr => peeraddr -> LocalRootPeers peeraddr -> Bool
hotTarget :: LocalRootPeers peeraddr -> HotValency
warmTarget :: LocalRootPeers peeraddr -> WarmValency

-- | The local root peers info has some invariants that are not directly
--   enforced in the types, and the config comes from an external source.
--   Of course it's good to validate that at source, but here we need to
--   not fail if we're given imperfect data.
--   
--   So what we do is bash it until it is valid. We don't need to be too
--   careful about how we do it, it's ok to be brutal. We should however
--   make sure we trace a warning about dodgy config.
fromGroups :: Ord peeraddr => [(HotValency, WarmValency, Map peeraddr PeerAdvertise)] -> LocalRootPeers peeraddr

-- | Inverse of <a>fromGroups</a>, for the subset of inputs to
--   <a>fromGroups</a> that satisfy the invariant.
toGroups :: Ord peeraddr => LocalRootPeers peeraddr -> [(HotValency, WarmValency, Map peeraddr PeerAdvertise)]
toGroupSets :: LocalRootPeers peeraddr -> [(HotValency, WarmValency, Set peeraddr)]
toMap :: LocalRootPeers peeraddr -> Map peeraddr PeerAdvertise
keysSet :: LocalRootPeers peeraddr -> Set peeraddr

-- | Limit the size of the root peers collection to fit within given
--   bounds.
--   
--   The governor needs to be able to do this to enforce its invariant
--   that:
--   
--   <pre>
--   LocalRootPeers.size localRootPeers &lt;= targetNumberOfKnownPeers
--   </pre>
--   
--   It needs to be able to <i>establish</i> that invariant given arbitrary
--   configuration for local root peers. It makes sense to do it this way
--   rather than just enforce that local root peers config fits the
--   invariant because the invariant depends on both the targets and the
--   local root peers config and these can both vary dynamically and
--   independently.
--   
--   It is unlikely in practice that there are so many local root peers
--   configured that it goes over this targets, so it's ok to resolve it
--   pretty arbitrarily. We just take the local roots in left to right
--   order up to the limit. So we have the property that
--   
--   <pre>
--   LocalRootPeers.size (LocalRootPeers.clampToLimit sz lrps)
--    == min sz (LocalRootPeers.size lrps)
--   </pre>
clampToLimit :: Ord peeraddr => Int -> LocalRootPeers peeraddr -> LocalRootPeers peeraddr
instance GHC.Num.Num Ouroboros.Network.PeerSelection.State.LocalRootPeers.HotValency
instance GHC.Classes.Ord Ouroboros.Network.PeerSelection.State.LocalRootPeers.HotValency
instance GHC.Classes.Eq Ouroboros.Network.PeerSelection.State.LocalRootPeers.HotValency
instance GHC.Show.Show Ouroboros.Network.PeerSelection.State.LocalRootPeers.HotValency
instance GHC.Num.Num Ouroboros.Network.PeerSelection.State.LocalRootPeers.WarmValency
instance GHC.Classes.Ord Ouroboros.Network.PeerSelection.State.LocalRootPeers.WarmValency
instance GHC.Classes.Eq Ouroboros.Network.PeerSelection.State.LocalRootPeers.WarmValency
instance GHC.Show.Show Ouroboros.Network.PeerSelection.State.LocalRootPeers.WarmValency
instance GHC.Classes.Eq peeraddr => GHC.Classes.Eq (Ouroboros.Network.PeerSelection.State.LocalRootPeers.LocalRootPeers peeraddr)
instance (GHC.Show.Show peeraddr, GHC.Classes.Ord peeraddr) => GHC.Show.Show (Ouroboros.Network.PeerSelection.State.LocalRootPeers.LocalRootPeers peeraddr)

module Ouroboros.Network.PeerSelection.RootPeersDNS.LocalRootPeers

-- | Resolve <tt>RelayAddress</tt>-es of local root peers using dns if
--   needed. Local roots are provided wrapped in a <a>StrictTVar</a>, which
--   value might change (re-read form a config file). The resolved dns
--   names are available through the output <a>StrictTVar</a>.
localRootPeersProvider :: forall m peerAddr resolver exception. (Alternative (STM m), MonadAsync m, MonadDelay m, MonadThrow m, Ord peerAddr) => Tracer m (TraceLocalRootPeers peerAddr exception) -> (IP -> PortNumber -> peerAddr) -> ResolvConf -> DNSActions resolver exception m -> STM m [(HotValency, WarmValency, Map RelayAccessPoint PeerAdvertise)] -> StrictTVar m [(HotValency, WarmValency, Map peerAddr PeerAdvertise)] -> m Void
data TraceLocalRootPeers peerAddr exception

-- | <a>Int</a> is the configured valency for the local producer groups
TraceLocalRootDomains :: [(HotValency, WarmValency, Map RelayAccessPoint PeerAdvertise)] -> TraceLocalRootPeers peerAddr exception
TraceLocalRootWaiting :: DomainAccessPoint -> DiffTime -> TraceLocalRootPeers peerAddr exception
TraceLocalRootResult :: DomainAccessPoint -> [(IP, TTL)] -> TraceLocalRootPeers peerAddr exception

-- | This traces the results of the local root peer provider
TraceLocalRootGroups :: [(HotValency, WarmValency, Map peerAddr PeerAdvertise)] -> TraceLocalRootPeers peerAddr exception

-- | This traces the results of the domain name resolution
TraceLocalRootDNSMap :: Map DomainAccessPoint [peerAddr] -> TraceLocalRootPeers peerAddr exception
TraceLocalRootReconfigured :: [(HotValency, WarmValency, Map RelayAccessPoint PeerAdvertise)] -> [(HotValency, WarmValency, Map RelayAccessPoint PeerAdvertise)] -> TraceLocalRootPeers peerAddr exception
TraceLocalRootFailure :: DomainAccessPoint -> DNSorIOError exception -> TraceLocalRootPeers peerAddr exception
TraceLocalRootError :: DomainAccessPoint -> SomeException -> TraceLocalRootPeers peerAddr exception
instance (GHC.Show.Show peerAddr, GHC.Show.Show exception) => GHC.Show.Show (Ouroboros.Network.PeerSelection.RootPeersDNS.LocalRootPeers.TraceLocalRootPeers peerAddr exception)

module Ouroboros.Network.PeerSelection.Types

-- | Where did this peer come from? Policy functions can choose to treat
--   peers differently depending on where we found them from.
data PeerSource
PeerSourceLocalRoot :: PeerSource
PeerSourcePublicRoot :: PeerSource
PeerSourcePeerShare :: PeerSource
data PeerStatus

-- | Peer is in true cold which means no connection to exists and the
--   outbound governor is safe to promote it.
PeerCold :: PeerStatus

-- | Peer is in cold state but its connection still lingers. I.e. it is
--   still in progress to be fully demoted.
--   
--   Note: The `PeerCooling -&gt; PeerCold` state transition is an
--   `outbound-governor` reflection of the connection-manager's
--   `TerminatingSt -&gt; TerminatedSt` state transition (our version of
--   tcp's <tt>TimeWait</tt>). It is only triggered in case of a clean
--   connection shutdown, not in the case of errors.
PeerCooling :: PeerStatus
PeerWarm :: PeerStatus
PeerHot :: PeerStatus
instance GHC.Enum.Enum Ouroboros.Network.PeerSelection.Types.PeerSource
instance GHC.Show.Show Ouroboros.Network.PeerSelection.Types.PeerSource
instance GHC.Classes.Ord Ouroboros.Network.PeerSelection.Types.PeerSource
instance GHC.Classes.Eq Ouroboros.Network.PeerSelection.Types.PeerSource
instance GHC.Show.Show Ouroboros.Network.PeerSelection.Types.PeerStatus
instance GHC.Classes.Ord Ouroboros.Network.PeerSelection.Types.PeerStatus
instance GHC.Classes.Eq Ouroboros.Network.PeerSelection.Types.PeerStatus

module Ouroboros.Network.PeerSharing

-- | Request and Result queue for the peer sharing client implementation.
--   
--   Although Peer Sharing is a request-response protocol we can not run it
--   as one, i.e. starting and terminating the protocol on demand since
--   protocol termination as a different semantics. We have to keep the
--   client and server protocol sides running and only issue the requests
--   on demand.
--   
--   A workaround to this is to implement the client side with the help of
--   a PeerSharingController which contains two queues: request and result.
--   The client side will be waiting to receive a <a>PeerSharingAmount</a>
--   from the request queue and as soon as it gets something it will send a
--   <a>SendMsgShareRequest</a> and wait for a response before writing it
--   to the result queue.
newtype PeerSharingController peer m
PeerSharingController :: StrictTMVar m (PeerSharingAmount, MVar m [peer]) -> PeerSharingController peer m

-- | Depth 1 mailbox that contains a locally scoped result queue
[requestQueue] :: PeerSharingController peer m -> StrictTMVar m (PeerSharingAmount, MVar m [peer])

-- | Peer Sharing Registry is a registry that stores a
--   <a>PeerSharingController</a> for every peer that we connect to.
--   
--   <a>bracketPeerSharingClient</a> should be used.
newtype PeerSharingRegistry peer m
PeerSharingRegistry :: StrictTVar m (Map peer (PeerSharingController peer m)) -> PeerSharingRegistry peer m
[getPeerSharingRegistry] :: PeerSharingRegistry peer m -> StrictTVar m (Map peer (PeerSharingController peer m))
newPeerSharingRegistry :: (MonadSTM m, Ord peer) => m (PeerSharingRegistry peer m)
bracketPeerSharingClient :: (Ord peer, MonadSTM m, MonadThrow m) => PeerSharingRegistry peer m -> peer -> (PeerSharingController peer m -> m a) -> m a
peerSharingClient :: (Alternative (STM m), MonadMVar m, MonadSTM m) => ControlMessageSTM m -> PeerSharingController peer m -> m (PeerSharingClient peer m ())
peerSharingServer :: Monad m => (PeerSharingAmount -> m [peer]) -> PeerSharingServer peer m

module Ouroboros.Network.PeerSelection.PeerSelectionActions
withPeerSelectionActions :: forall peeraddr peerconn resolver exception m a. (Alternative (STM m), MonadAsync m, MonadDelay m, MonadThrow m, MonadMVar m, Ord peeraddr, Exception exception) => Tracer m (TraceLocalRootPeers peeraddr exception) -> Tracer m TracePublicRootPeers -> Tracer m TraceLedgerPeers -> (IP -> PortNumber -> peeraddr) -> DNSActions resolver exception m -> STM m PeerSelectionTargets -> STM m [(HotValency, WarmValency, Map RelayAccessPoint PeerAdvertise)] -> STM m (Map RelayAccessPoint PeerAdvertise) -> PeerSharing -> (peerconn -> PeerSharing) -> STM m (Map peeraddr (PeerSharingController peeraddr m)) -> STM m (peeraddr, PeerSharing) -> PeerStateActions peeraddr peerconn m -> StdGen -> LedgerPeersConsensusInterface m -> STM m UseLedgerAfter -> ((Async m Void, Async m Void) -> PeerSelectionActions peeraddr peerconn m -> m a) -> m a

-- | Adjustable targets for the peer selection mechanism.
--   
--   These are used by the peer selection governor as targets. They are
--   used by the peer churn governor loop as knobs to adjust, to influence
--   the peer selection governor.
--   
--   The <i>known</i>, <i>established</i> and <i>active</i> peer targets
--   are targets both from below and from above: the governor will attempt
--   to grow or shrink the sets to hit these targets.
--   
--   Unlike the other targets, the <i>root</i> peer target is "one sided",
--   it is only a target from below. The governor does not try to shrink
--   the root set to hit it, it simply stops looking for more.
--   
--   There is also an implicit target that enough local root peers are
--   selected as active. This comes from the configuration for local roots,
--   and is not an independently adjustable target.
data PeerSelectionTargets
PeerSelectionTargets :: !Int -> !Int -> !Int -> !Int -> !Int -> !Int -> !Int -> PeerSelectionTargets
[targetNumberOfRootPeers] :: PeerSelectionTargets -> !Int

-- | The target number of all known peers. This includes ledger, big ledger
--   peers.
[targetNumberOfKnownPeers] :: PeerSelectionTargets -> !Int

-- | The target number of established peers (does not include big ledger
--   peers).
--   
--   The target includes root peers, local root peers, ledger peers and big
--   ledger peers.
[targetNumberOfEstablishedPeers] :: PeerSelectionTargets -> !Int

-- | The target number of active peers (does not include big ledger peers).
--   
--   The
[targetNumberOfActivePeers] :: PeerSelectionTargets -> !Int

-- | Target number of known big ledger peers.
--   
--   This target is independent of <a>targetNumberOfKnownPeers</a>. The
--   total number of known peers will be sum of the two targets.
[targetNumberOfKnownBigLedgerPeers] :: PeerSelectionTargets -> !Int

-- | Target number of established big ledger peers.
--   
--   This target is independent of <a>targetNumberOfEstablishedPeers</a>.
--   The total number of established peers will be sum of the two targets
--   and local root peers.
[targetNumberOfEstablishedBigLedgerPeers] :: PeerSelectionTargets -> !Int

-- | Target number of active big ledger peers.
--   
--   This target is independent of <a>targetNumberOfActivePeers</a>. The
--   total number of active peers will be sum of the two targets and active
--   local root peers.
[targetNumberOfActiveBigLedgerPeers] :: PeerSelectionTargets -> !Int
data () => PeerAdvertise
DoNotAdvertisePeer :: PeerAdvertise
DoAdvertisePeer :: PeerAdvertise

module Ouroboros.Network.Tracers

-- | IP subscription tracers.
data NetworkSubscriptionTracers withIPList addr vNumber
NetworkSubscriptionTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> Tracer IO (WithAddr addr ErrorPolicyTrace) -> Tracer IO (withIPList (SubscriptionTrace addr)) -> NetworkSubscriptionTracers withIPList addr vNumber

-- | low level mux-network tracer, which logs mux sdu (send and received)
--   and other low level multiplexing events.
[nsMuxTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)

-- | handshake protocol tracer; it is important for analysing version
--   negotation mismatches.
[nsHandshakeTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))

-- | error policy tracer; must not be <a>nullTracer</a>, otherwise all the
--   exceptions which are not matched by any error policy will be caught
--   and not logged or rethrown.
[nsErrorPolicyTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithAddr addr ErrorPolicyTrace)

-- | subscription tracers; it is infrequent it should not be
--   <a>nullTracer</a> by default.
[nsSubscriptionTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (withIPList (SubscriptionTrace addr))
type NetworkIPSubscriptionTracers addr vNumber = NetworkSubscriptionTracers WithIPList addr vNumber
nullNetworkSubscriptionTracers :: NetworkSubscriptionTracers withIPList addr vNumber

-- | DNS subscription tracers.
data NetworkDNSSubscriptionTracers vNumber addr
NetworkDNSSubscriptionTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> Tracer IO (WithAddr addr ErrorPolicyTrace) -> Tracer IO (WithDomainName (SubscriptionTrace addr)) -> Tracer IO (WithDomainName DnsTrace) -> NetworkDNSSubscriptionTracers vNumber addr

-- | low level mux-network tracer, which logs mux sdu (send and received)
--   and other low level multiplexing events.
[ndstMuxTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)

-- | handshake protocol tracer; it is important for analysing version
--   negotation mismatches.
[ndstHandshakeTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))

-- | error policy tracer; must not be <a>nullTracer</a>, otherwise all the
--   exceptions which are not matched by any error policy will be caught
--   and not logged or rethrown.
[ndstErrorPolicyTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithAddr addr ErrorPolicyTrace)

-- | subscription tracer; it is infrequent it should not be
--   <a>nullTracer</a> by default.
[ndstSubscriptionTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithDomainName (SubscriptionTrace addr))

-- | dns resolver tracer; it is infrequent it should not be
--   <a>nullTracer</a> by default.
[ndstDnsTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithDomainName DnsTrace)
nullNetworkDNSSubscriptionTracers :: NetworkDNSSubscriptionTracers vNumber peerid


-- | This is the starting point for a module that will bring together the
--   overall node to client protocol, as a collection of mini-protocols.
module Ouroboros.Network.NodeToClient

-- | Make an <a>OuroborosApplication</a> for the bundle of mini-protocols
--   that make up the overall node-to-client protocol.
--   
--   This function specifies the wire format protocol numbers as well as
--   the protocols that run for each <a>NodeToClientVersion</a>.
--   
--   They are chosen to not overlap with the node to node protocol numbers.
--   This is not essential for correctness, but is helpful to allow a
--   single shared implementation of tools that can analyse both protocols,
--   e.g. wireshark plugins.
nodeToClientProtocols :: NodeToClientProtocols appType addr bytes m a b -> NodeToClientVersion -> OuroborosApplicationWithMinimalCtx appType addr bytes m a b

-- | Record of node-to-client mini protocols.
data NodeToClientProtocols appType ntcAddr bytes m a b
NodeToClientProtocols :: RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b -> RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b -> RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b -> RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b -> NodeToClientProtocols appType ntcAddr bytes m a b

-- | local chain-sync mini-protocol
[localChainSyncProtocol] :: NodeToClientProtocols appType ntcAddr bytes m a b -> RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b

-- | local tx-submission mini-protocol
[localTxSubmissionProtocol] :: NodeToClientProtocols appType ntcAddr bytes m a b -> RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b

-- | local state-query mini-protocol
[localStateQueryProtocol] :: NodeToClientProtocols appType ntcAddr bytes m a b -> RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b

-- | local tx-monitor mini-protocol
[localTxMonitorProtocol] :: NodeToClientProtocols appType ntcAddr bytes m a b -> RunMiniProtocolWithMinimalCtx appType ntcAddr bytes m a b
data () => NodeToClientVersion
NodeToClientV_9 :: NodeToClientVersion
NodeToClientV_10 :: NodeToClientVersion
NodeToClientV_11 :: NodeToClientVersion
NodeToClientV_12 :: NodeToClientVersion
NodeToClientV_13 :: NodeToClientVersion
NodeToClientV_14 :: NodeToClientVersion
NodeToClientV_15 :: NodeToClientVersion
NodeToClientV_16 :: NodeToClientVersion
data () => NodeToClientVersionData
NodeToClientVersionData :: !NetworkMagic -> !Bool -> NodeToClientVersionData
[networkMagic] :: NodeToClientVersionData -> !NetworkMagic
[query] :: NodeToClientVersionData -> !Bool
data () => NetworkConnectTracers addr vNumber
NetworkConnectTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> NetworkConnectTracers addr vNumber
[nctMuxTracer] :: NetworkConnectTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)
[nctHandshakeTracer] :: NetworkConnectTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))
nullNetworkConnectTracers :: NetworkConnectTracers addr vNumber

-- | A specialised version of <a>connectToNode</a>. It is a general purpose
--   function which can connect using any version of the protocol. This is
--   mostly useful for future enhancements.
connectTo :: LocalSnocket -> NetworkConnectTracers LocalAddress NodeToClientVersion -> Versions NodeToClientVersion NodeToClientVersionData (OuroborosApplicationWithMinimalCtx InitiatorMode LocalAddress ByteString IO a b) -> FilePath -> IO ()
data () => NetworkServerTracers addr vNumber
NetworkServerTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> Tracer IO (WithAddr addr ErrorPolicyTrace) -> Tracer IO AcceptConnectionsPolicyTrace -> NetworkServerTracers addr vNumber
[nstMuxTracer] :: NetworkServerTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)
[nstHandshakeTracer] :: NetworkServerTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))
[nstErrorPolicyTracer] :: NetworkServerTracers addr vNumber -> Tracer IO (WithAddr addr ErrorPolicyTrace)
[nstAcceptPolicyTracer] :: NetworkServerTracers addr vNumber -> Tracer IO AcceptConnectionsPolicyTrace
nullNetworkServerTracers :: NetworkServerTracers addr vNumber
data () => NetworkMutableState addr
NetworkMutableState :: ConnectionTable IO addr -> StrictTVar IO (PeerStates IO addr) -> NetworkMutableState addr
[nmsConnectionTable] :: NetworkMutableState addr -> ConnectionTable IO addr
[nmsPeerStates] :: NetworkMutableState addr -> StrictTVar IO (PeerStates IO addr)
newNetworkMutableState :: IO (NetworkMutableState addr)
newNetworkMutableStateSTM :: STM (NetworkMutableState addr)
cleanNetworkMutableState :: NetworkMutableState addr -> IO ()

-- | A specialised version of <a>withServerNode</a>.
--   
--   Comments to <a>withServer</a> apply here as well.
withServer :: LocalSnocket -> NetworkServerTracers LocalAddress NodeToClientVersion -> NetworkMutableState LocalAddress -> LocalSocket -> Versions NodeToClientVersion NodeToClientVersionData (OuroborosApplicationWithMinimalCtx ResponderMode LocalAddress ByteString IO a b) -> ErrorPolicies -> IO Void
type NetworkClientSubcriptionTracers = NetworkSubscriptionTracers Identity LocalAddress NodeToClientVersion

-- | IP subscription tracers.
data NetworkSubscriptionTracers withIPList addr vNumber
NetworkSubscriptionTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> Tracer IO (WithAddr addr ErrorPolicyTrace) -> Tracer IO (withIPList (SubscriptionTrace addr)) -> NetworkSubscriptionTracers withIPList addr vNumber

-- | low level mux-network tracer, which logs mux sdu (send and received)
--   and other low level multiplexing events.
[nsMuxTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)

-- | handshake protocol tracer; it is important for analysing version
--   negotation mismatches.
[nsHandshakeTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))

-- | error policy tracer; must not be <a>nullTracer</a>, otherwise all the
--   exceptions which are not matched by any error policy will be caught
--   and not logged or rethrown.
[nsErrorPolicyTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithAddr addr ErrorPolicyTrace)

-- | subscription tracers; it is infrequent it should not be
--   <a>nullTracer</a> by default.
[nsSubscriptionTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (withIPList (SubscriptionTrace addr))
data () => ClientSubscriptionParams a
ClientSubscriptionParams :: !LocalAddress -> !Maybe DiffTime -> !ErrorPolicies -> ClientSubscriptionParams a
[cspAddress] :: ClientSubscriptionParams a -> !LocalAddress
[cspConnectionAttemptDelay] :: ClientSubscriptionParams a -> !Maybe DiffTime
[cspErrorPolicies] :: ClientSubscriptionParams a -> !ErrorPolicies

-- | <a>ncSubscriptionWorker</a> which starts given application versions on
--   each established connection.
ncSubscriptionWorker :: forall mode x y. HasInitiator mode ~ True => LocalSnocket -> NetworkClientSubcriptionTracers -> NetworkMutableState LocalAddress -> ClientSubscriptionParams () -> Versions NodeToClientVersion NodeToClientVersionData (OuroborosApplicationWithMinimalCtx mode LocalAddress ByteString IO x y) -> IO Void
chainSyncPeerNull :: forall (header :: Type) (point :: Type) (tip :: Type) m a. MonadDelay m => Peer (ChainSync header point tip) AsClient StIdle m a
localStateQueryPeerNull :: forall (block :: Type) (point :: Type) (query :: Type -> Type) m a. MonadDelay m => Peer (LocalStateQuery block point query) AsClient StIdle m a
localTxSubmissionPeerNull :: forall (tx :: Type) (reject :: Type) m a. MonadDelay m => Peer (LocalTxSubmission tx reject) AsClient StIdle m a
localTxMonitorPeerNull :: forall (txid :: Type) (tx :: Type) (slot :: Type) m a. MonadDelay m => Peer (LocalTxMonitor txid tx slot) AsClient StIdle m a
newtype () => IOManager
IOManager :: (forall hole. () => hole -> IO ()) -> IOManager
[associateWithIOManager] :: IOManager -> forall hole. () => hole -> IO ()
type AssociateWithIOCP = IOManager
withIOManager :: WithIOManager
type LocalSnocket = Snocket IO LocalSocket LocalAddress
localSnocket :: IOManager -> LocalSnocket
newtype () => LocalSocket
LocalSocket :: LocalHandle -> LocalSocket
[getLocalHandle] :: LocalSocket -> LocalHandle
newtype () => LocalAddress
LocalAddress :: FilePath -> LocalAddress
[getFilePath] :: LocalAddress -> FilePath
newtype () => Versions vNum vData r
Versions :: Map vNum (Version vData r) -> Versions vNum vData r
[getVersions] :: Versions vNum vData r -> Map vNum (Version vData r)

-- | <a>Versions</a> containing a single version of
--   <a>nodeToClientProtocols</a>.
versionedNodeToClientProtocols :: NodeToClientVersion -> NodeToClientVersionData -> NodeToClientProtocols appType LocalAddress bytes m a b -> Versions NodeToClientVersion NodeToClientVersionData (OuroborosApplicationWithMinimalCtx appType LocalAddress bytes m a b)
simpleSingletonVersions :: vNum -> vData -> r -> Versions vNum vData r
foldMapVersions :: (Ord vNum, Foldable f, HasCallStack) => (x -> Versions vNum extra r) -> f x -> Versions vNum extra r
combineVersions :: (Ord vNum, Foldable f, HasCallStack) => f (Versions vNum extra r) -> Versions vNum extra r
nodeToClientHandshakeCodec :: forall (m :: Type -> Type). MonadST m => Codec (Handshake NodeToClientVersion Term) DeserialiseFailure m ByteString
nodeToClientVersionCodec :: CodecCBORTerm (Text, Maybe Int) NodeToClientVersion
nodeToClientCodecCBORTerm :: NodeToClientVersion -> CodecCBORTerm Text NodeToClientVersionData
data () => ConnectionId addr
ConnectionId :: !addr -> !addr -> ConnectionId addr
[localAddress] :: ConnectionId addr -> !addr
[remoteAddress] :: ConnectionId addr -> !addr
newtype () => MinimalInitiatorContext addr
MinimalInitiatorContext :: ConnectionId addr -> MinimalInitiatorContext addr
[micConnectionId] :: MinimalInitiatorContext addr -> ConnectionId addr
newtype () => ResponderContext addr
ResponderContext :: ConnectionId addr -> ResponderContext addr
[rcConnectionId] :: ResponderContext addr -> ConnectionId addr
type LocalConnectionId = ConnectionId LocalAddress
data () => ErrorPolicies
ErrorPolicies :: [ErrorPolicy] -> [ErrorPolicy] -> ErrorPolicies
[epAppErrorPolicies] :: ErrorPolicies -> [ErrorPolicy]
[epConErrorPolicies] :: ErrorPolicies -> [ErrorPolicy]

-- | <a>ErrorPolicies</a> for client application. Additional rules can be
--   added by means of a <a>Semigroup</a> instance of <a>ErrorPolicies</a>.
--   
--   This error policies will try to preserve <tt>subscriptionWorker</tt>,
--   e.g. if the connect function throws an <a>IOException</a> we will
--   suspend it for a <tt>shortDelay</tt>, and try to re-connect.
--   
--   This allows to recover from a situation where a node temporarily
--   shutsdown, or running a client application which is subscribed two
--   more than one node (possibly over network).
networkErrorPolicies :: ErrorPolicies
nullErrorPolicies :: ErrorPolicies
data () => ErrorPolicy
[ErrorPolicy] :: forall e. Exception e => (e -> Maybe (SuspendDecision DiffTime)) -> ErrorPolicy
data () => ErrorPolicyTrace
ErrorPolicySuspendPeer :: Maybe (ConnectionOrApplicationExceptionTrace SomeException) -> DiffTime -> DiffTime -> ErrorPolicyTrace
ErrorPolicySuspendConsumer :: Maybe (ConnectionOrApplicationExceptionTrace SomeException) -> DiffTime -> ErrorPolicyTrace
ErrorPolicyLocalNodeError :: ConnectionOrApplicationExceptionTrace SomeException -> ErrorPolicyTrace
ErrorPolicyResumePeer :: ErrorPolicyTrace
ErrorPolicyKeepSuspended :: ErrorPolicyTrace
ErrorPolicyResumeConsumer :: ErrorPolicyTrace
ErrorPolicyResumeProducer :: ErrorPolicyTrace
ErrorPolicyUnhandledApplicationException :: SomeException -> ErrorPolicyTrace
ErrorPolicyUnhandledConnectionException :: SomeException -> ErrorPolicyTrace
ErrorPolicyAcceptException :: IOException -> ErrorPolicyTrace
data () => WithAddr addr a
WithAddr :: addr -> a -> WithAddr addr a
[wiaAddr] :: WithAddr addr a -> addr
[wiaEvent] :: WithAddr addr a -> a
data () => SuspendDecision t
SuspendPeer :: !t -> !t -> SuspendDecision t
SuspendConsumer :: !t -> SuspendDecision t
Throw :: SuspendDecision t
data () => TraceSendRecv ps
[TraceSendMsg] :: forall ps. AnyMessageAndAgency ps -> TraceSendRecv ps
[TraceRecvMsg] :: forall ps. AnyMessageAndAgency ps -> TraceSendRecv ps
data () => ProtocolLimitFailure
data () => Handshake (vNumber :: k) (vParams :: k1)
data () => LocalAddresses addr
LocalAddresses :: Maybe addr -> Maybe addr -> Maybe addr -> LocalAddresses addr
[laIpv4] :: LocalAddresses addr -> Maybe addr
[laIpv6] :: LocalAddresses addr -> Maybe addr
[laUnix] :: LocalAddresses addr -> Maybe addr
data () => SubscriptionTrace addr
SubscriptionTraceConnectStart :: addr -> SubscriptionTrace addr
SubscriptionTraceConnectEnd :: addr -> ConnectResult -> SubscriptionTrace addr
SubscriptionTraceSocketAllocationException :: addr -> e -> SubscriptionTrace addr
SubscriptionTraceConnectException :: addr -> e -> SubscriptionTrace addr
SubscriptionTraceApplicationException :: addr -> e -> SubscriptionTrace addr
SubscriptionTraceTryConnectToPeer :: addr -> SubscriptionTrace addr
SubscriptionTraceSkippingPeer :: addr -> SubscriptionTrace addr
SubscriptionTraceSubscriptionRunning :: SubscriptionTrace addr
SubscriptionTraceSubscriptionWaiting :: Int -> SubscriptionTrace addr
SubscriptionTraceSubscriptionFailed :: SubscriptionTrace addr
SubscriptionTraceSubscriptionWaitingNewConnection :: DiffTime -> SubscriptionTrace addr
SubscriptionTraceStart :: Int -> SubscriptionTrace addr
SubscriptionTraceRestart :: DiffTime -> Int -> Int -> SubscriptionTrace addr
SubscriptionTraceConnectionExist :: addr -> SubscriptionTrace addr
SubscriptionTraceUnsupportedRemoteAddr :: addr -> SubscriptionTrace addr
SubscriptionTraceMissingLocalAddress :: SubscriptionTrace addr
SubscriptionTraceAllocateSocket :: addr -> SubscriptionTrace addr
SubscriptionTraceCloseSocket :: addr -> SubscriptionTrace addr
type HandshakeTr ntcAddr ntcVersion = WithMuxBearer (ConnectionId ntcAddr) (TraceSendRecv (Handshake ntcVersion Term))

module Ouroboros.Network.TxSubmission.Mempool.Reader

-- | The consensus layer functionality that the inbound and outbound side
--   of the tx submission logic requires.
--   
--   This is provided to the tx submission logic by the consensus layer.
data TxSubmissionMempoolReader txid tx idx m
TxSubmissionMempoolReader :: STM m (MempoolSnapshot txid tx idx) -> idx -> TxSubmissionMempoolReader txid tx idx m

-- | In STM, grab a snapshot of the contents of the mempool. This allows
--   further pure queries on the snapshot.
[mempoolGetSnapshot] :: TxSubmissionMempoolReader txid tx idx m -> STM m (MempoolSnapshot txid tx idx)

-- | <a>mempoolTxIdsAfter</a> with <a>mempoolZeroIdx</a> is expected to
--   give all txs currently in the mempool.
[mempoolZeroIdx] :: TxSubmissionMempoolReader txid tx idx m -> idx

-- | A pure snapshot of the contents of the mempool. It allows fetching
--   information about transactions in the mempool, and fetching individual
--   transactions.
--   
--   This uses a transaction sequence number type for identifying
--   transactions within the mempool sequence. The sequence number is local
--   to this mempool, unlike the transaction hash. This allows us to ask
--   for all transactions after a known sequence number, to get new
--   transactions. It is also used to look up individual transactions.
--   
--   Note that it is expected that <a>mempoolLookupTx</a> will often return
--   <a>Nothing</a> even for tx sequence numbers returned in previous
--   snapshots. This happens when the transaction has been removed from the
--   mempool between snapshots.
data MempoolSnapshot txid tx idx
MempoolSnapshot :: (idx -> [(txid, idx, TxSizeInBytes)]) -> (idx -> Maybe tx) -> (txid -> Bool) -> MempoolSnapshot txid tx idx
[mempoolTxIdsAfter] :: MempoolSnapshot txid tx idx -> idx -> [(txid, idx, TxSizeInBytes)]
[mempoolLookupTx] :: MempoolSnapshot txid tx idx -> idx -> Maybe tx
[mempoolHasTx] :: MempoolSnapshot txid tx idx -> txid -> Bool
mapMempoolSnapshot :: (tx -> tx') -> MempoolSnapshot txid tx idx -> MempoolSnapshot txid tx' idx
mapTxSubmissionMempoolReader :: MonadSTM m => (tx -> tx') -> TxSubmissionMempoolReader txid tx idx m -> TxSubmissionMempoolReader txid tx' idx m

module Ouroboros.Network.TxSubmission.Inbound
txSubmissionInbound :: forall txid tx idx m. (Ord txid, NoThunks txid, NoThunks tx, MonadSTM m, MonadThrow m) => Tracer m (TraceTxSubmissionInbound txid tx) -> Word16 -> TxSubmissionMempoolReader txid tx idx m -> TxSubmissionMempoolWriter txid tx idx m -> NodeToNodeVersion -> TxSubmissionServerPipelined txid tx m ()

-- | The consensus layer functionality that the inbound side of the tx
--   submission logic requires.
--   
--   This is provided to the tx submission logic by the consensus layer.
data TxSubmissionMempoolWriter txid tx idx m
TxSubmissionMempoolWriter :: (tx -> txid) -> ([tx] -> m [txid]) -> TxSubmissionMempoolWriter txid tx idx m

-- | Compute the transaction id from a transaction.
--   
--   This is used in the protocol handler to verify a full transaction
--   matches a previously given transaction id.
[txId] :: TxSubmissionMempoolWriter txid tx idx m -> tx -> txid

-- | Supply a batch of transactions to the mempool. They are either
--   accepted or rejected individually, but in the order supplied.
--   
--   The <tt>txid</tt>s of all transactions that were added successfully
--   are returned.
[mempoolAddTxs] :: TxSubmissionMempoolWriter txid tx idx m -> [tx] -> m [txid]
data TraceTxSubmissionInbound txid tx

-- | Number of transactions just about to be inserted.
TraceTxSubmissionCollected :: Int -> TraceTxSubmissionInbound txid tx

-- | Just processed transaction pass/fail breakdown.
TraceTxSubmissionProcessed :: ProcessedTxCount -> TraceTxSubmissionInbound txid tx

-- | Server received <tt>MsgDone</tt>
TraceTxInboundTerminated :: TraceTxSubmissionInbound txid tx
TraceTxInboundCanRequestMoreTxs :: Int -> TraceTxSubmissionInbound txid tx
TraceTxInboundCannotRequestMoreTxs :: Int -> TraceTxSubmissionInbound txid tx
data TxSubmissionProtocolError
ProtocolErrorTxNotRequested :: TxSubmissionProtocolError
ProtocolErrorTxIdsNotRequested :: TxSubmissionProtocolError
data ProcessedTxCount
ProcessedTxCount :: Int -> Int -> ProcessedTxCount

-- | Just accepted this many transactions.
[ptxcAccepted] :: ProcessedTxCount -> Int

-- | Just rejected this many transactions.
[ptxcRejected] :: ProcessedTxCount -> Int
instance GHC.Show.Show Ouroboros.Network.TxSubmission.Inbound.ProcessedTxCount
instance GHC.Classes.Eq Ouroboros.Network.TxSubmission.Inbound.ProcessedTxCount
instance GHC.Show.Show (Ouroboros.Network.TxSubmission.Inbound.TraceTxSubmissionInbound txid tx)
instance GHC.Classes.Eq (Ouroboros.Network.TxSubmission.Inbound.TraceTxSubmissionInbound txid tx)
instance GHC.Show.Show Ouroboros.Network.TxSubmission.Inbound.TxSubmissionProtocolError
instance GHC.Generics.Generic (Ouroboros.Network.TxSubmission.Inbound.ServerState txid tx)
instance (GHC.Show.Show txid, GHC.Show.Show tx) => GHC.Show.Show (Ouroboros.Network.TxSubmission.Inbound.ServerState txid tx)
instance (NoThunks.Class.NoThunks txid, NoThunks.Class.NoThunks tx) => NoThunks.Class.NoThunks (Ouroboros.Network.TxSubmission.Inbound.ServerState txid tx)
instance GHC.Exception.Type.Exception Ouroboros.Network.TxSubmission.Inbound.TxSubmissionProtocolError

module Ouroboros.Network.TxSubmission.Outbound
txSubmissionOutbound :: forall txid tx idx m. (Ord txid, Ord idx, MonadSTM m, MonadThrow m) => Tracer m (TraceTxSubmissionOutbound txid tx) -> Word16 -> TxSubmissionMempoolReader txid tx idx m -> NodeToNodeVersion -> ControlMessageSTM m -> TxSubmissionClient txid tx m ()
data TraceTxSubmissionOutbound txid tx

-- | The IDs of the transactions requested.
TraceTxSubmissionOutboundRecvMsgRequestTxs :: [txid] -> TraceTxSubmissionOutbound txid tx

-- | The transactions to be sent in the response.
TraceTxSubmissionOutboundSendMsgReplyTxs :: [tx] -> TraceTxSubmissionOutbound txid tx
TraceControlMessage :: ControlMessage -> TraceTxSubmissionOutbound txid tx
data TxSubmissionProtocolError
ProtocolErrorAckedTooManyTxids :: TxSubmissionProtocolError
ProtocolErrorRequestedNothing :: TxSubmissionProtocolError
ProtocolErrorRequestedTooManyTxids :: Word16 -> Word16 -> TxSubmissionProtocolError
ProtocolErrorRequestBlocking :: TxSubmissionProtocolError
ProtocolErrorRequestNonBlocking :: TxSubmissionProtocolError
ProtocolErrorRequestedUnavailableTx :: TxSubmissionProtocolError
instance (GHC.Show.Show txid, GHC.Show.Show tx) => GHC.Show.Show (Ouroboros.Network.TxSubmission.Outbound.TraceTxSubmissionOutbound txid tx)
instance GHC.Show.Show Ouroboros.Network.TxSubmission.Outbound.TxSubmissionProtocolError
instance GHC.Exception.Type.Exception Ouroboros.Network.TxSubmission.Outbound.TxSubmissionProtocolError


-- | This is the starting point for a module that will bring together the
--   overall node to node protocol, as a collection of mini-protocols.
module Ouroboros.Network.NodeToNode

-- | Make an <a>OuroborosApplication</a> for the bundle of mini-protocols
--   that make up the overall node-to-node protocol.
--   
--   This function specifies the wire format protocol numbers.
--   
--   The application specific protocol numbers start from 2. The
--   <tt><a>MiniProtocolNum</a> 0</tt> is reserved for the <a>Handshake</a>
--   protocol, while <tt><a>MiniProtocolNum</a> 1</tt> is reserved for
--   DeltaQ messages. <a>Handshake</a> protocol is not included in
--   <a>NodeToNodeProtocols</a> as it runs before mux is started but it
--   reusing <tt>MuxBearer</tt> to send and receive messages. Only when the
--   handshake protocol succeeds, we will know which protocols to run /
--   multiplex.
--   
--   These are chosen to not overlap with the node to client protocol
--   numbers (and the handshake protocol number). This is not essential for
--   correctness, but is helpful to allow a single shared implementation of
--   tools that can analyse both protocols, e.g. wireshark plugins.
nodeToNodeProtocols :: MiniProtocolParameters -> NodeToNodeProtocols muxMode initiatorCtx responderCtx bytes m a b -> NodeToNodeVersion -> PeerSharing -> OuroborosBundle muxMode initiatorCtx responderCtx bytes m a b
data NodeToNodeProtocols appType initiatorCtx responderCtx bytes m a b
NodeToNodeProtocols :: RunMiniProtocol appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b -> NodeToNodeProtocols appType initiatorCtx responderCtx bytes m a b

-- | chain-sync mini-protocol
[chainSyncProtocol] :: NodeToNodeProtocols appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b

-- | block-fetch mini-protocol
[blockFetchProtocol] :: NodeToNodeProtocols appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b

-- | tx-submission mini-protocol
[txSubmissionProtocol] :: NodeToNodeProtocols appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b

-- | keep-alive mini-protocol
[keepAliveProtocol] :: NodeToNodeProtocols appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b

-- | peer sharing mini-protocol
[peerSharingProtocol] :: NodeToNodeProtocols appType initiatorCtx responderCtx bytes m a b -> RunMiniProtocol appType initiatorCtx responderCtx bytes m a b
type NodeToNodeProtocolsWithExpandedCtx appType ntnAddr bytes m a b = NodeToNodeProtocols appType (ExpandedInitiatorContext ntnAddr m) (ResponderContext ntnAddr) bytes m a b
type NodeToNodeProtocolsWithMinimalCtx appType ntnAddr bytes m a b = NodeToNodeProtocols appType (MinimalInitiatorContext ntnAddr) (ResponderContext ntnAddr) bytes m a b
data MiniProtocolParameters
MiniProtocolParameters :: !Word16 -> !Word16 -> !Word16 -> !Word16 -> MiniProtocolParameters

-- | high threshold for pipelining (we will never exceed that many messages
--   pipelined).
[chainSyncPipeliningHighMark] :: MiniProtocolParameters -> !Word16

-- | low threshold: if we hit the <a>chainSyncPipeliningHighMark</a> we
--   will listen for responses until there are at most
--   <a>chainSyncPipeliningLowMark</a> pipelined message
--   
--   Must be smaller than <a>chainSyncPipeliningHighMark</a>.
--   
--   Note: <a>chainSyncPipeliningLowMark</a> and
--   <a>chainSyncPipeliningLowMark</a> are passed to
--   <tt>pipelineDecisionLowHighMark</tt>.
[chainSyncPipeliningLowMark] :: MiniProtocolParameters -> !Word16

-- | maximal number of pipelined messages in 'block-fetch' mini-protocol.
[blockFetchPipeliningMax] :: MiniProtocolParameters -> !Word16

-- | maximal number of unacked tx (pipelining is bounded by twice this
--   number)
[txSubmissionMaxUnacked] :: MiniProtocolParameters -> !Word16
chainSyncProtocolLimits :: MiniProtocolParameters -> MiniProtocolLimits
blockFetchProtocolLimits :: MiniProtocolParameters -> MiniProtocolLimits
txSubmissionProtocolLimits :: MiniProtocolParameters -> MiniProtocolLimits
keepAliveProtocolLimits :: MiniProtocolParameters -> MiniProtocolLimits
defaultMiniProtocolParameters :: MiniProtocolParameters
data () => NodeToNodeVersion
NodeToNodeV_7 :: NodeToNodeVersion
NodeToNodeV_8 :: NodeToNodeVersion
NodeToNodeV_9 :: NodeToNodeVersion
NodeToNodeV_10 :: NodeToNodeVersion
NodeToNodeV_11 :: NodeToNodeVersion
NodeToNodeV_12 :: NodeToNodeVersion
NodeToNodeV_13 :: NodeToNodeVersion
data () => NodeToNodeVersionData
NodeToNodeVersionData :: !NetworkMagic -> !DiffusionMode -> !PeerSharing -> !Bool -> NodeToNodeVersionData
[networkMagic] :: NodeToNodeVersionData -> !NetworkMagic
[diffusionMode] :: NodeToNodeVersionData -> !DiffusionMode
[peerSharing] :: NodeToNodeVersionData -> !PeerSharing
[query] :: NodeToNodeVersionData -> !Bool
data () => NetworkConnectTracers addr vNumber
NetworkConnectTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> NetworkConnectTracers addr vNumber
[nctMuxTracer] :: NetworkConnectTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)
[nctHandshakeTracer] :: NetworkConnectTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))
nullNetworkConnectTracers :: NetworkConnectTracers addr vNumber

-- | A specialised version of <tt><a>connectToNode</a></tt>.
connectTo :: Snocket IO Socket SockAddr -> NetworkConnectTracers SockAddr NodeToNodeVersion -> Versions NodeToNodeVersion NodeToNodeVersionData (OuroborosApplicationWithMinimalCtx InitiatorMode SockAddr ByteString IO a b) -> Maybe SockAddr -> SockAddr -> IO ()
data () => NetworkServerTracers addr vNumber
NetworkServerTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> Tracer IO (WithAddr addr ErrorPolicyTrace) -> Tracer IO AcceptConnectionsPolicyTrace -> NetworkServerTracers addr vNumber
[nstMuxTracer] :: NetworkServerTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)
[nstHandshakeTracer] :: NetworkServerTracers addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))
[nstErrorPolicyTracer] :: NetworkServerTracers addr vNumber -> Tracer IO (WithAddr addr ErrorPolicyTrace)
[nstAcceptPolicyTracer] :: NetworkServerTracers addr vNumber -> Tracer IO AcceptConnectionsPolicyTrace
nullNetworkServerTracers :: NetworkServerTracers addr vNumber
data () => NetworkMutableState addr
NetworkMutableState :: ConnectionTable IO addr -> StrictTVar IO (PeerStates IO addr) -> NetworkMutableState addr
[nmsConnectionTable] :: NetworkMutableState addr -> ConnectionTable IO addr
[nmsPeerStates] :: NetworkMutableState addr -> StrictTVar IO (PeerStates IO addr)
data () => AcceptedConnectionsLimit
AcceptedConnectionsLimit :: !Word32 -> !Word32 -> !DiffTime -> AcceptedConnectionsLimit
[acceptedConnectionsHardLimit] :: AcceptedConnectionsLimit -> !Word32
[acceptedConnectionsSoftLimit] :: AcceptedConnectionsLimit -> !Word32
[acceptedConnectionsDelay] :: AcceptedConnectionsLimit -> !DiffTime
newNetworkMutableState :: IO (NetworkMutableState addr)
newNetworkMutableStateSTM :: STM (NetworkMutableState addr)
cleanNetworkMutableState :: NetworkMutableState addr -> IO ()

-- | A specialised version of <tt><a>withServerNode</a></tt>. It forks a
--   thread which runs an accept loop (server thread):
--   
--   <ul>
--   <li>when the server thread throws an exception the main thread
--   rethrows it (by <a>wait</a>)</li>
--   <li>when an async exception is thrown to kill the main thread the
--   server thread will be cancelled as well (by <tt>withAsync</tt>)</li>
--   </ul>
withServer :: SocketSnocket -> NetworkServerTracers SockAddr NodeToNodeVersion -> NetworkMutableState SockAddr -> AcceptedConnectionsLimit -> Socket -> Versions NodeToNodeVersion NodeToNodeVersionData (OuroborosApplicationWithMinimalCtx ResponderMode SockAddr ByteString IO a b) -> ErrorPolicies -> IO Void
data () => PeerAdvertise
DoNotAdvertisePeer :: PeerAdvertise
DoAdvertisePeer :: PeerAdvertise

-- | Adjustable targets for the peer selection mechanism.
--   
--   These are used by the peer selection governor as targets. They are
--   used by the peer churn governor loop as knobs to adjust, to influence
--   the peer selection governor.
--   
--   The <i>known</i>, <i>established</i> and <i>active</i> peer targets
--   are targets both from below and from above: the governor will attempt
--   to grow or shrink the sets to hit these targets.
--   
--   Unlike the other targets, the <i>root</i> peer target is "one sided",
--   it is only a target from below. The governor does not try to shrink
--   the root set to hit it, it simply stops looking for more.
--   
--   There is also an implicit target that enough local root peers are
--   selected as active. This comes from the configuration for local roots,
--   and is not an independently adjustable target.
data PeerSelectionTargets
PeerSelectionTargets :: !Int -> !Int -> !Int -> !Int -> !Int -> !Int -> !Int -> PeerSelectionTargets
[targetNumberOfRootPeers] :: PeerSelectionTargets -> !Int

-- | The target number of all known peers. This includes ledger, big ledger
--   peers.
[targetNumberOfKnownPeers] :: PeerSelectionTargets -> !Int

-- | The target number of established peers (does not include big ledger
--   peers).
--   
--   The target includes root peers, local root peers, ledger peers and big
--   ledger peers.
[targetNumberOfEstablishedPeers] :: PeerSelectionTargets -> !Int

-- | The target number of active peers (does not include big ledger peers).
--   
--   The
[targetNumberOfActivePeers] :: PeerSelectionTargets -> !Int

-- | Target number of known big ledger peers.
--   
--   This target is independent of <a>targetNumberOfKnownPeers</a>. The
--   total number of known peers will be sum of the two targets.
[targetNumberOfKnownBigLedgerPeers] :: PeerSelectionTargets -> !Int

-- | Target number of established big ledger peers.
--   
--   This target is independent of <a>targetNumberOfEstablishedPeers</a>.
--   The total number of established peers will be sum of the two targets
--   and local root peers.
[targetNumberOfEstablishedBigLedgerPeers] :: PeerSelectionTargets -> !Int

-- | Target number of active big ledger peers.
--   
--   This target is independent of <a>targetNumberOfActivePeers</a>. The
--   total number of active peers will be sum of the two targets and active
--   local root peers.
[targetNumberOfActiveBigLedgerPeers] :: PeerSelectionTargets -> !Int
data () => IPSubscriptionTarget
IPSubscriptionTarget :: ![SockAddr] -> !Int -> IPSubscriptionTarget
[ispIps] :: IPSubscriptionTarget -> ![SockAddr]
[ispValency] :: IPSubscriptionTarget -> !Int
type NetworkIPSubscriptionTracers addr vNumber = NetworkSubscriptionTracers WithIPList addr vNumber

-- | IP subscription tracers.
data NetworkSubscriptionTracers withIPList addr vNumber
NetworkSubscriptionTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> Tracer IO (WithAddr addr ErrorPolicyTrace) -> Tracer IO (withIPList (SubscriptionTrace addr)) -> NetworkSubscriptionTracers withIPList addr vNumber

-- | low level mux-network tracer, which logs mux sdu (send and received)
--   and other low level multiplexing events.
[nsMuxTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)

-- | handshake protocol tracer; it is important for analysing version
--   negotation mismatches.
[nsHandshakeTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))

-- | error policy tracer; must not be <a>nullTracer</a>, otherwise all the
--   exceptions which are not matched by any error policy will be caught
--   and not logged or rethrown.
[nsErrorPolicyTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (WithAddr addr ErrorPolicyTrace)

-- | subscription tracers; it is infrequent it should not be
--   <a>nullTracer</a> by default.
[nsSubscriptionTracer] :: NetworkSubscriptionTracers withIPList addr vNumber -> Tracer IO (withIPList (SubscriptionTrace addr))
nullNetworkSubscriptionTracers :: NetworkSubscriptionTracers withIPList addr vNumber
data () => SubscriptionParams a target
SubscriptionParams :: LocalAddresses SockAddr -> (SockAddr -> Maybe DiffTime) -> ErrorPolicies -> target -> SubscriptionParams a target
[spLocalAddresses] :: SubscriptionParams a target -> LocalAddresses SockAddr
[spConnectionAttemptDelay] :: SubscriptionParams a target -> SockAddr -> Maybe DiffTime
[spErrorPolicies] :: SubscriptionParams a target -> ErrorPolicies
[spSubscriptionTarget] :: SubscriptionParams a target -> target
type IPSubscriptionParams a = SubscriptionParams a IPSubscriptionTarget

-- | <a>ipSubscriptionWorker</a> which starts given application versions on
--   each established connection.
ipSubscriptionWorker :: forall mode x y. HasInitiator mode ~ True => SocketSnocket -> NetworkIPSubscriptionTracers SockAddr NodeToNodeVersion -> NetworkMutableState SockAddr -> IPSubscriptionParams () -> Versions NodeToNodeVersion NodeToNodeVersionData (OuroborosApplicationWithMinimalCtx mode SockAddr ByteString IO x y) -> IO Void
data () => DnsSubscriptionTarget
DnsSubscriptionTarget :: !Domain -> !PortNumber -> !Int -> DnsSubscriptionTarget
[dstDomain] :: DnsSubscriptionTarget -> !Domain
[dstPort] :: DnsSubscriptionTarget -> !PortNumber
[dstValency] :: DnsSubscriptionTarget -> !Int
type DnsSubscriptionParams a = SubscriptionParams a DnsSubscriptionTarget

-- | DNS subscription tracers.
data NetworkDNSSubscriptionTracers vNumber addr
NetworkDNSSubscriptionTracers :: Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace) -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term))) -> Tracer IO (WithAddr addr ErrorPolicyTrace) -> Tracer IO (WithDomainName (SubscriptionTrace addr)) -> Tracer IO (WithDomainName DnsTrace) -> NetworkDNSSubscriptionTracers vNumber addr

-- | low level mux-network tracer, which logs mux sdu (send and received)
--   and other low level multiplexing events.
[ndstMuxTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithMuxBearer (ConnectionId addr) MuxTrace)

-- | handshake protocol tracer; it is important for analysing version
--   negotation mismatches.
[ndstHandshakeTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithMuxBearer (ConnectionId addr) (TraceSendRecv (Handshake vNumber Term)))

-- | error policy tracer; must not be <a>nullTracer</a>, otherwise all the
--   exceptions which are not matched by any error policy will be caught
--   and not logged or rethrown.
[ndstErrorPolicyTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithAddr addr ErrorPolicyTrace)

-- | subscription tracer; it is infrequent it should not be
--   <a>nullTracer</a> by default.
[ndstSubscriptionTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithDomainName (SubscriptionTrace addr))

-- | dns resolver tracer; it is infrequent it should not be
--   <a>nullTracer</a> by default.
[ndstDnsTracer] :: NetworkDNSSubscriptionTracers vNumber addr -> Tracer IO (WithDomainName DnsTrace)
nullNetworkDNSSubscriptionTracers :: NetworkDNSSubscriptionTracers vNumber peerid

-- | <a>dnsSubscriptionWorker</a> which starts given application versions
--   on each established connection.
dnsSubscriptionWorker :: forall mode x y. HasInitiator mode ~ True => SocketSnocket -> NetworkDNSSubscriptionTracers NodeToNodeVersion SockAddr -> NetworkMutableState SockAddr -> DnsSubscriptionParams () -> Versions NodeToNodeVersion NodeToNodeVersionData (OuroborosApplicationWithMinimalCtx mode SockAddr ByteString IO x y) -> IO Void
newtype () => Versions vNum vData r
Versions :: Map vNum (Version vData r) -> Versions vNum vData r
[getVersions] :: Versions vNum vData r -> Map vNum (Version vData r)
data () => DiffusionMode
InitiatorOnlyDiffusionMode :: DiffusionMode
InitiatorAndResponderDiffusionMode :: DiffusionMode
simpleSingletonVersions :: vNum -> vData -> r -> Versions vNum vData r
foldMapVersions :: (Ord vNum, Foldable f, HasCallStack) => (x -> Versions vNum extra r) -> f x -> Versions vNum extra r
combineVersions :: (Ord vNum, Foldable f, HasCallStack) => f (Versions vNum extra r) -> Versions vNum extra r
nodeToNodeHandshakeCodec :: forall (m :: Type -> Type). MonadST m => Codec (Handshake NodeToNodeVersion Term) DeserialiseFailure m ByteString
nodeToNodeVersionCodec :: CodecCBORTerm (Text, Maybe Int) NodeToNodeVersion
nodeToNodeCodecCBORTerm :: NodeToNodeVersion -> CodecCBORTerm Text NodeToNodeVersionData
data () => ExpandedInitiatorContext addr (m :: Type -> Type)
ExpandedInitiatorContext :: !ConnectionId addr -> !ControlMessageSTM m -> !IsBigLedgerPeer -> ExpandedInitiatorContext addr (m :: Type -> Type)
[eicConnectionId] :: ExpandedInitiatorContext addr (m :: Type -> Type) -> !ConnectionId addr
[eicControlMessage] :: ExpandedInitiatorContext addr (m :: Type -> Type) -> !ControlMessageSTM m
[eicIsBigLedgerPeer] :: ExpandedInitiatorContext addr (m :: Type -> Type) -> !IsBigLedgerPeer
newtype () => MinimalInitiatorContext addr
MinimalInitiatorContext :: ConnectionId addr -> MinimalInitiatorContext addr
[micConnectionId] :: MinimalInitiatorContext addr -> ConnectionId addr
newtype () => ResponderContext addr
ResponderContext :: ConnectionId addr -> ResponderContext addr
[rcConnectionId] :: ResponderContext addr -> ConnectionId addr
data () => ConnectionId addr
ConnectionId :: !addr -> !addr -> ConnectionId addr
[localAddress] :: ConnectionId addr -> !addr
[remoteAddress] :: ConnectionId addr -> !addr
data () => ControlMessage
Continue :: ControlMessage
Quiesce :: ControlMessage
Terminate :: ControlMessage
type ControlMessageSTM (m :: Type -> Type) = STM m ControlMessage
type RemoteAddress = SockAddr
type RemoteConnectionId = ConnectionId RemoteAddress
data () => IsBigLedgerPeer
IsBigLedgerPeer :: IsBigLedgerPeer
IsNotBigLedgerPeer :: IsBigLedgerPeer
data () => ProtocolLimitFailure
data () => Handshake (vNumber :: k) (vParams :: k1)
data () => LocalAddresses addr
LocalAddresses :: Maybe addr -> Maybe addr -> Maybe addr -> LocalAddresses addr
[laIpv4] :: LocalAddresses addr -> Maybe addr
[laIpv6] :: LocalAddresses addr -> Maybe addr
[laUnix] :: LocalAddresses addr -> Maybe addr
data () => Socket
data () => ExceptionInHandler
[ExceptionInHandler] :: forall peerAddr. (Typeable peerAddr, Show peerAddr) => !peerAddr -> !SomeException -> ExceptionInHandler
data () => ErrorPolicies
ErrorPolicies :: [ErrorPolicy] -> [ErrorPolicy] -> ErrorPolicies
[epAppErrorPolicies] :: ErrorPolicies -> [ErrorPolicy]
[epConErrorPolicies] :: ErrorPolicies -> [ErrorPolicy]

-- | A minimal error policy for remote peers, which only handles exceptions
--   raised by `ouroboros-network`.
remoteNetworkErrorPolicy :: ErrorPolicies

-- | Error policy for local clients. This is equivalent to
--   <a>nullErrorPolicies</a>, but explicit in the errors which can be
--   caught.
--   
--   We are very permissive here, and very strict in the
--   <a>networkErrorPolicy</a>. After any failure the client will be killed
--   and not penalised by this policy. This allows to restart the local
--   client without a delay.
localNetworkErrorPolicy :: ErrorPolicies
nullErrorPolicies :: ErrorPolicies
data () => ErrorPolicy
[ErrorPolicy] :: forall e. Exception e => (e -> Maybe (SuspendDecision DiffTime)) -> ErrorPolicy
data () => SuspendDecision t
SuspendPeer :: !t -> !t -> SuspendDecision t
SuspendConsumer :: !t -> SuspendDecision t
Throw :: SuspendDecision t
data () => AcceptConnectionsPolicyTrace
ServerTraceAcceptConnectionRateLimiting :: DiffTime -> Int -> AcceptConnectionsPolicyTrace
ServerTraceAcceptConnectionHardLimit :: Word32 -> AcceptConnectionsPolicyTrace
ServerTraceAcceptConnectionResume :: Int -> AcceptConnectionsPolicyTrace
data () => TraceSendRecv ps
[TraceSendMsg] :: forall ps. AnyMessageAndAgency ps -> TraceSendRecv ps
[TraceRecvMsg] :: forall ps. AnyMessageAndAgency ps -> TraceSendRecv ps
data () => SubscriptionTrace addr
SubscriptionTraceConnectStart :: addr -> SubscriptionTrace addr
SubscriptionTraceConnectEnd :: addr -> ConnectResult -> SubscriptionTrace addr
SubscriptionTraceSocketAllocationException :: addr -> e -> SubscriptionTrace addr
SubscriptionTraceConnectException :: addr -> e -> SubscriptionTrace addr
SubscriptionTraceApplicationException :: addr -> e -> SubscriptionTrace addr
SubscriptionTraceTryConnectToPeer :: addr -> SubscriptionTrace addr
SubscriptionTraceSkippingPeer :: addr -> SubscriptionTrace addr
SubscriptionTraceSubscriptionRunning :: SubscriptionTrace addr
SubscriptionTraceSubscriptionWaiting :: Int -> SubscriptionTrace addr
SubscriptionTraceSubscriptionFailed :: SubscriptionTrace addr
SubscriptionTraceSubscriptionWaitingNewConnection :: DiffTime -> SubscriptionTrace addr
SubscriptionTraceStart :: Int -> SubscriptionTrace addr
SubscriptionTraceRestart :: DiffTime -> Int -> Int -> SubscriptionTrace addr
SubscriptionTraceConnectionExist :: addr -> SubscriptionTrace addr
SubscriptionTraceUnsupportedRemoteAddr :: addr -> SubscriptionTrace addr
SubscriptionTraceMissingLocalAddress :: SubscriptionTrace addr
SubscriptionTraceAllocateSocket :: addr -> SubscriptionTrace addr
SubscriptionTraceCloseSocket :: addr -> SubscriptionTrace addr
data () => DnsTrace
DnsTraceLookupException :: SomeException -> DnsTrace
DnsTraceLookupAError :: DNSError -> DnsTrace
DnsTraceLookupAAAAError :: DNSError -> DnsTrace
DnsTraceLookupIPv6First :: DnsTrace
DnsTraceLookupIPv4First :: DnsTrace
DnsTraceLookupAResult :: [SockAddr] -> DnsTrace
DnsTraceLookupAAAAResult :: [SockAddr] -> DnsTrace
data () => ErrorPolicyTrace
ErrorPolicySuspendPeer :: Maybe (ConnectionOrApplicationExceptionTrace SomeException) -> DiffTime -> DiffTime -> ErrorPolicyTrace
ErrorPolicySuspendConsumer :: Maybe (ConnectionOrApplicationExceptionTrace SomeException) -> DiffTime -> ErrorPolicyTrace
ErrorPolicyLocalNodeError :: ConnectionOrApplicationExceptionTrace SomeException -> ErrorPolicyTrace
ErrorPolicyResumePeer :: ErrorPolicyTrace
ErrorPolicyKeepSuspended :: ErrorPolicyTrace
ErrorPolicyResumeConsumer :: ErrorPolicyTrace
ErrorPolicyResumeProducer :: ErrorPolicyTrace
ErrorPolicyUnhandledApplicationException :: SomeException -> ErrorPolicyTrace
ErrorPolicyUnhandledConnectionException :: SomeException -> ErrorPolicyTrace
ErrorPolicyAcceptException :: IOException -> ErrorPolicyTrace
data () => WithIPList a
WithIPList :: LocalAddresses SockAddr -> [SockAddr] -> a -> WithIPList a
[wilSrc] :: WithIPList a -> LocalAddresses SockAddr
[wilDsts] :: WithIPList a -> [SockAddr]
[wilEvent] :: WithIPList a -> a
data () => WithDomainName a
WithDomainName :: Domain -> a -> WithDomainName a
[wdnDomain] :: WithDomainName a -> Domain
[wdnEvent] :: WithDomainName a -> a
data () => WithAddr addr a
WithAddr :: addr -> a -> WithAddr addr a
[wiaAddr] :: WithAddr addr a -> addr
[wiaEvent] :: WithAddr addr a -> a
type HandshakeTr ntnAddr ntnVersion = WithMuxBearer (ConnectionId ntnAddr) (TraceSendRecv (Handshake ntnVersion Term))
chainSyncMiniProtocolNum :: MiniProtocolNum
blockFetchMiniProtocolNum :: MiniProtocolNum
txSubmissionMiniProtocolNum :: MiniProtocolNum
keepAliveMiniProtocolNum :: MiniProtocolNum
peerSharingMiniProtocolNum :: MiniProtocolNum
instance Ouroboros.Network.Util.ShowProxy.ShowProxy Ouroboros.Network.NodeToNode.RemoteAddress

module Ouroboros.Network.PeerSelection.PeerMetric

-- | Mutable peer metrics state accessible via <a>STM</a>.
data PeerMetrics m p
newtype PeerMetricsConfiguration
PeerMetricsConfiguration :: Int -> PeerMetricsConfiguration

-- | The maximum numbers of slots we will store data for. On some chains
--   sometimes this corresponds to 1h worth of metrics *sighs*.
--   
--   this number MUST correspond to number of headers / blocks which are
--   produced in one hour.
[maxEntriesToTrack] :: PeerMetricsConfiguration -> Int
newPeerMetric :: (MonadLabelledSTM m, NoThunks p, NFData p) => PeerMetricsConfiguration -> m (PeerMetrics m p)
joinedPeerMetricAt :: forall p m. MonadSTM m => Ord p => PeerMetrics m p -> STM m (Map p SlotNo)

-- | Returns a Map which counts the number of times a given peer was the
--   first to present us with a block/header.
upstreamyness :: forall p m. MonadSTM m => Ord p => PeerMetrics m p -> STM m (Map p Int)

-- | Returns a Map which counts the number of bytes downloaded for a given
--   peer.
fetchynessBytes :: forall p m. MonadSTM m => Ord p => PeerMetrics m p -> STM m (Map p Int)

-- | Returns a Map which counts the number of times a given peer was the
--   first we downloaded a block from.
fetchynessBlocks :: forall p m. MonadSTM m => Ord p => PeerMetrics m p -> STM m (Map p Int)

-- | Tracer which updates header metrics (upstreameness) and inserts new
--   peers into <a>peerRegistry</a>.
headerMetricTracer :: forall m p. (MonadSTM m, Ord p) => PeerMetricsConfiguration -> PeerMetrics m p -> Tracer (STM m) (TraceLabelPeer (ConnectionId p) (SlotNo, Time))

-- | Tracer which updates fetched metrics (fetchyness) and inserts new
--   peers into <a>peerRegistry</a>.
fetchedMetricTracer :: forall m p. (MonadSTM m, Ord p) => PeerMetricsConfiguration -> PeerMetrics m p -> Tracer (STM m) (TraceLabelPeer (ConnectionId p) (SizeInBytes, SlotNo, Time))
data () => ReportPeerMetrics (m :: Type -> Type) peerAddr
ReportPeerMetrics :: Tracer (STM m) (TraceLabelPeer peerAddr (SlotNo, Time)) -> Tracer (STM m) (TraceLabelPeer peerAddr (SizeInBytes, SlotNo, Time)) -> ReportPeerMetrics (m :: Type -> Type) peerAddr
[reportHeader] :: ReportPeerMetrics (m :: Type -> Type) peerAddr -> Tracer (STM m) (TraceLabelPeer peerAddr (SlotNo, Time))
[reportFetch] :: ReportPeerMetrics (m :: Type -> Type) peerAddr -> Tracer (STM m) (TraceLabelPeer peerAddr (SizeInBytes, SlotNo, Time))
nullMetric :: MonadSTM m => ReportPeerMetrics m p
reportMetric :: forall m p. (MonadSTM m, Ord p) => PeerMetricsConfiguration -> PeerMetrics m p -> ReportPeerMetrics m (ConnectionId p)

-- | Integer based metric ordered by <a>SlotNo</a> which holds the peer and
--   time.
--   
--   The <tt>p</tt> parameter is truly polymorphic. For
--   <a>upstreamyness</a> and we use peer address, and for
--   <tt>fetchyness</tt> it is a pair of peer id and bytes downloaded.
type SlotMetric p = IntPSQ SlotNo (p, Time)
newPeerMetric' :: (MonadLabelledSTM m, NoThunks p, NFData p) => SlotMetric p -> SlotMetric (p, SizeInBytes) -> PeerMetricsConfiguration -> m (PeerMetrics m p)
instance Control.DeepSeq.NFData Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsConfiguration
instance NoThunks.Class.NoThunks Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsConfiguration
instance GHC.Generics.Generic Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsConfiguration
instance GHC.Show.Show Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsConfiguration
instance Control.DeepSeq.NFData Ouroboros.Network.PeerSelection.PeerMetric.AverageMetrics
instance NoThunks.Class.NoThunks Ouroboros.Network.PeerSelection.PeerMetric.AverageMetrics
instance GHC.Generics.Generic Ouroboros.Network.PeerSelection.PeerMetric.AverageMetrics
instance GHC.Show.Show Ouroboros.Network.PeerSelection.PeerMetric.AverageMetrics
instance Control.DeepSeq.NFData p => Control.DeepSeq.NFData (Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsState p)
instance NoThunks.Class.NoThunks p => NoThunks.Class.NoThunks (Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsState p)
instance GHC.Generics.Generic (Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsState p)
instance GHC.Show.Show p => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerMetric.PeerMetricsState p)

module Ouroboros.Network.Diffusion.Policies

-- | Timeout for <tt>spsDeactivateTimeout</tt>.
--   
--   The maximal timeout on <tt>ChainSync</tt> (in <tt>StMustReply</tt>
--   state) is <tt>269s</tt>.
deactivateTimeout :: DiffTime

-- | Timeout for <tt>spsCloseConnectionTimeout</tt>.
--   
--   This timeout depends on <tt>KeepAlive</tt> and <tt>TipSample</tt>
--   timeouts. <tt>KeepAlive</tt> keeps agency most of the time, but
--   <tt>TipSample</tt> can give away its agency for longer periods of
--   time. Here we allow it to get 6 blocks (assuming a new block every
--   <tt>20s</tt>).
closeConnectionTimeout :: DiffTime

-- | Number of events tracked by <a>PeerMetrics</a>. This corresponds to
--   one hour of blocks on mainnet.
--   
--   TODO: issue #3866
peerMetricsConfiguration :: PeerMetricsConfiguration

-- | Merge two dictionaries where values of the first one are obligatory,
--   while the second one are optional.
optionalMerge :: Ord k => Map k a -> Map k b -> Map k (a, Maybe b)
simplePeerSelectionPolicy :: forall m peerAddr. (MonadSTM m, Ord peerAddr) => StrictTVar m StdGen -> STM m ChurnMode -> PeerMetrics m peerAddr -> RepromoteDelay -> PeerSelectionPolicy peerAddr m

-- | Sort by upstreamness and a random score.
--   
--   Note: this <a>PrunePolicy</a> does not depend on
--   <tt>igsConnections</tt>. We put <tt>igsPrng</tt> in
--   <tt>InboundGovernorState</tt> only to show that we can have a
--   <a>PrunePolicy</a> which depends on the <tt>InboundGovernorState</tt>
--   as a more refined policy would do.
--   
--   <i>complexity:</i> &lt;math&gt;
--   
--   TODO: complexity could be improved.
prunePolicy :: (MonadSTM m, Ord peerAddr) => StrictTVar m InboundGovernorObservableState -> PrunePolicy peerAddr (STM m)


-- | This subsystem manages the discovery and selection of <i>upstream</i>
--   peers.
module Ouroboros.Network.PeerSelection.Governor
data PeerSelectionPolicy peeraddr m
PeerSelectionPolicy :: PickPolicy peeraddr (STM m) -> PickPolicy peeraddr (STM m) -> PickPolicy peeraddr (STM m) -> PickPolicy peeraddr (STM m) -> PickPolicy peeraddr (STM m) -> PickPolicy peeraddr (STM m) -> !DiffTime -> !Int -> !DiffTime -> !DiffTime -> !DiffTime -> !DiffTime -> !DiffTime -> PeerSelectionPolicy peeraddr m
[policyPickKnownPeersForPeerShare] :: PeerSelectionPolicy peeraddr m -> PickPolicy peeraddr (STM m)
[policyPickColdPeersToPromote] :: PeerSelectionPolicy peeraddr m -> PickPolicy peeraddr (STM m)
[policyPickWarmPeersToPromote] :: PeerSelectionPolicy peeraddr m -> PickPolicy peeraddr (STM m)
[policyPickHotPeersToDemote] :: PeerSelectionPolicy peeraddr m -> PickPolicy peeraddr (STM m)
[policyPickWarmPeersToDemote] :: PeerSelectionPolicy peeraddr m -> PickPolicy peeraddr (STM m)
[policyPickColdPeersToForget] :: PeerSelectionPolicy peeraddr m -> PickPolicy peeraddr (STM m)
[policyFindPublicRootTimeout] :: PeerSelectionPolicy peeraddr m -> !DiffTime

-- | Maximum number of peer sharing requests that can be in progress
[policyMaxInProgressPeerShareReqs] :: PeerSelectionPolicy peeraddr m -> !Int

-- | Amount of time a node has to wait before issuing a new peer sharing
--   request
[policyPeerShareRetryTime] :: PeerSelectionPolicy peeraddr m -> !DiffTime

-- | Amount of time a batch of peer sharing requests is allowed to take
[policyPeerShareBatchWaitTime] :: PeerSelectionPolicy peeraddr m -> !DiffTime

-- | Amount of time the overall batches of peer sharing requests are
--   allowed to take
[policyPeerShareOverallTimeout] :: PeerSelectionPolicy peeraddr m -> !DiffTime

-- | Delay until we consider a peer suitable for peer sharing
[policyPeerShareActivationDelay] :: PeerSelectionPolicy peeraddr m -> !DiffTime

-- | Re-promote delay, passed from <a>ExitPolicy</a>.
[policyErrorDelay] :: PeerSelectionPolicy peeraddr m -> !DiffTime

-- | Adjustable targets for the peer selection mechanism.
--   
--   These are used by the peer selection governor as targets. They are
--   used by the peer churn governor loop as knobs to adjust, to influence
--   the peer selection governor.
--   
--   The <i>known</i>, <i>established</i> and <i>active</i> peer targets
--   are targets both from below and from above: the governor will attempt
--   to grow or shrink the sets to hit these targets.
--   
--   Unlike the other targets, the <i>root</i> peer target is "one sided",
--   it is only a target from below. The governor does not try to shrink
--   the root set to hit it, it simply stops looking for more.
--   
--   There is also an implicit target that enough local root peers are
--   selected as active. This comes from the configuration for local roots,
--   and is not an independently adjustable target.
data PeerSelectionTargets
PeerSelectionTargets :: !Int -> !Int -> !Int -> !Int -> !Int -> !Int -> !Int -> PeerSelectionTargets
[targetNumberOfRootPeers] :: PeerSelectionTargets -> !Int

-- | The target number of all known peers. This includes ledger, big ledger
--   peers.
[targetNumberOfKnownPeers] :: PeerSelectionTargets -> !Int

-- | The target number of established peers (does not include big ledger
--   peers).
--   
--   The target includes root peers, local root peers, ledger peers and big
--   ledger peers.
[targetNumberOfEstablishedPeers] :: PeerSelectionTargets -> !Int

-- | The target number of active peers (does not include big ledger peers).
--   
--   The
[targetNumberOfActivePeers] :: PeerSelectionTargets -> !Int

-- | Target number of known big ledger peers.
--   
--   This target is independent of <a>targetNumberOfKnownPeers</a>. The
--   total number of known peers will be sum of the two targets.
[targetNumberOfKnownBigLedgerPeers] :: PeerSelectionTargets -> !Int

-- | Target number of established big ledger peers.
--   
--   This target is independent of <a>targetNumberOfEstablishedPeers</a>.
--   The total number of established peers will be sum of the two targets
--   and local root peers.
[targetNumberOfEstablishedBigLedgerPeers] :: PeerSelectionTargets -> !Int

-- | Target number of active big ledger peers.
--   
--   This target is independent of <a>targetNumberOfActivePeers</a>. The
--   total number of active peers will be sum of the two targets and active
--   local root peers.
[targetNumberOfActiveBigLedgerPeers] :: PeerSelectionTargets -> !Int

-- | Actions performed by the peer selection governor.
--   
--   These being pluggable allows:
--   
--   <ul>
--   <li>choice of known peer root sets</li>
--   <li>running both in simulation and for real</li>
--   </ul>
data PeerSelectionActions peeraddr peerconn m
PeerSelectionActions :: STM m PeerSelectionTargets -> STM m [(HotValency, WarmValency, Map peeraddr PeerAdvertise)] -> STM m (peeraddr, PeerSharing) -> PeerSharing -> (peerconn -> PeerSharing) -> (Int -> m (Map peeraddr (PeerAdvertise, IsLedgerPeer), DiffTime)) -> (Int -> m (Set peeraddr, DiffTime)) -> (PeerSharingAmount -> peeraddr -> m (PeerSharingResult peeraddr)) -> PeerStateActions peeraddr peerconn m -> PeerSelectionActions peeraddr peerconn m
[readPeerSelectionTargets] :: PeerSelectionActions peeraddr peerconn m -> STM m PeerSelectionTargets

-- | Read the current set of locally or privately known root peers.
--   
--   In general this is expected to be updated asynchronously by some other
--   thread. It is intended to cover the use case of peers from local
--   configuration. It could be dynamic due to DNS resolution, or due to
--   dynamic configuration updates.
--   
--   It is structured as a collection of (non-overlapping) groups of peers
--   where we are supposed to select n from each group.
[readLocalRootPeers] :: PeerSelectionActions peeraddr peerconn m -> STM m [(HotValency, WarmValency, Map peeraddr PeerAdvertise)]
[readNewInboundConnection] :: PeerSelectionActions peeraddr peerconn m -> STM m (peeraddr, PeerSharing)

-- | Read the current Peer Sharing willingness value
--   
--   This value comes from the Node's configuration file.
[peerSharing] :: PeerSelectionActions peeraddr peerconn m -> PeerSharing

-- | Get the remote's side PeerSharing value from <tt>peerconn</tt>
--   
--   <tt>peerconn</tt> ideally comes from a call to
--   <a>establishPeerConnection</a>. This will establish a connection and
--   perform handshake. The returned <tt>peerconn</tt> has all the
--   versionData negotiated in the handshake, including the remote peer's
--   <a>PeerSharing</a> willingness information.
[peerConnToPeerSharing] :: PeerSelectionActions peeraddr peerconn m -> peerconn -> PeerSharing

-- | Request a sample of public root peers.
--   
--   It is intended to cover use cases including:
--   
--   <ul>
--   <li>federated relays from a DNS pool</li>
--   <li>stake pool relays published in the blockchain</li>
--   <li>a pre-distributed snapshot of stake pool relays from the
--   blockchain</li>
--   </ul>
[requestPublicRootPeers] :: PeerSelectionActions peeraddr peerconn m -> Int -> m (Map peeraddr (PeerAdvertise, IsLedgerPeer), DiffTime)

-- | Request a sample of big ledger peers.
[requestBigLedgerPeers] :: PeerSelectionActions peeraddr peerconn m -> Int -> m (Set peeraddr, DiffTime)

-- | The action to contact a known peer and request a sample of its known
--   peers.
[requestPeerShare] :: PeerSelectionActions peeraddr peerconn m -> PeerSharingAmount -> peeraddr -> m (PeerSharingResult peeraddr)

-- | Core actions run by the governor to change <a>PeerStatus</a>.
[peerStateActions] :: PeerSelectionActions peeraddr peerconn m -> PeerStateActions peeraddr peerconn m

-- | Callbacks which are performed to change peer state.
data PeerStateActions peeraddr peerconn m
PeerStateActions :: (peerconn -> STM m (PeerStatus, Maybe RepromoteDelay)) -> (IsBigLedgerPeer -> peeraddr -> m peerconn) -> (IsBigLedgerPeer -> peerconn -> m ()) -> (peerconn -> m ()) -> (peerconn -> m ()) -> PeerStateActions peeraddr peerconn m

-- | Monitor peer state. Must be non-blocking.
[monitorPeerConnection] :: PeerStateActions peeraddr peerconn m -> peerconn -> STM m (PeerStatus, Maybe RepromoteDelay)

-- | Establish new connection: cold to warm.
--   
--   <a>IsBigLedgerPeer</a> is passed from the outbound governor to the
--   mini-protocol callbacks.
[establishPeerConnection] :: PeerStateActions peeraddr peerconn m -> IsBigLedgerPeer -> peeraddr -> m peerconn

-- | Activate a connection: warm to hot promotion.
--   
--   <a>IsBigLedgerPeer</a> is passed from the outbound governor to the
--   mini-protocol callbacks.
[activatePeerConnection] :: PeerStateActions peeraddr peerconn m -> IsBigLedgerPeer -> peerconn -> m ()

-- | Deactive a peer: hot to warm demotion.
[deactivatePeerConnection] :: PeerStateActions peeraddr peerconn m -> peerconn -> m ()

-- | Close a connection: warm to cold transition.
[closePeerConnection] :: PeerStateActions peeraddr peerconn m -> peerconn -> m ()
data TracePeerSelection peeraddr
TraceLocalRootPeersChanged :: LocalRootPeers peeraddr -> LocalRootPeers peeraddr -> TracePeerSelection peeraddr

-- | Peer selection targets changed: old targets, new targets.
TraceTargetsChanged :: PeerSelectionTargets -> PeerSelectionTargets -> TracePeerSelection peeraddr
TracePublicRootsRequest :: Int -> Int -> TracePeerSelection peeraddr
TracePublicRootsResults :: Map peeraddr PeerAdvertise -> Int -> DiffTime -> TracePeerSelection peeraddr
TracePublicRootsFailure :: SomeException -> Int -> DiffTime -> TracePeerSelection peeraddr

-- | target known peers, actual known peers, selected peers
TraceForgetColdPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr
TraceBigLedgerPeersRequest :: Int -> Int -> TracePeerSelection peeraddr
TraceBigLedgerPeersResults :: Set peeraddr -> Int -> DiffTime -> TracePeerSelection peeraddr
TraceBigLedgerPeersFailure :: SomeException -> Int -> DiffTime -> TracePeerSelection peeraddr

-- | target known big ledger peers, actual known big ledger peers, selected
--   peers
TraceForgetBigLedgerPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | target known peers, actual known peers, peers available for peer
--   sharing, peers selected for peer sharing
TracePeerShareRequests :: Int -> Int -> Set peeraddr -> Set peeraddr -> TracePeerSelection peeraddr
TracePeerShareResults :: [(peeraddr, Either SomeException (PeerSharingResult peeraddr))] -> TracePeerSelection peeraddr
TracePeerShareResultsFiltered :: [peeraddr] -> TracePeerSelection peeraddr
TraceKnownInboundConnection :: peeraddr -> PeerSharing -> TracePeerSelection peeraddr

-- | target established, actual established, selected peers
TracePromoteColdPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | target local established, actual local established, selected peers
TracePromoteColdLocalPeers :: [(WarmValency, Int)] -> Set peeraddr -> TracePeerSelection peeraddr
TracePromoteColdFailed :: Int -> Int -> peeraddr -> DiffTime -> SomeException -> TracePeerSelection peeraddr

-- | target established, actual established, peer
TracePromoteColdDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | target established big ledger peers, actual established big ledger
--   peers, selected peers
TracePromoteColdBigLedgerPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | target established big ledger peers, actual established big ledger
--   peers, peer, delay until next promotion, reason
TracePromoteColdBigLedgerPeerFailed :: Int -> Int -> peeraddr -> DiffTime -> SomeException -> TracePeerSelection peeraddr

-- | target established big ledger peers, actual established big ledger
--   peers, peer
TracePromoteColdBigLedgerPeerDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | target active, actual active, selected peers
TracePromoteWarmPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | Promote local peers to warm
TracePromoteWarmLocalPeers :: [(HotValency, Int)] -> Set peeraddr -> TracePeerSelection peeraddr
TracePromoteWarmFailed :: Int -> Int -> peeraddr -> SomeException -> TracePeerSelection peeraddr

-- | target active, actual active, peer
TracePromoteWarmDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | aborted promotion of a warm peer; likely it was asynchronously demoted
--   in the meantime.
--   
--   target active, actual active, peer
TracePromoteWarmAborted :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | target active big ledger peers, actual active big ledger peers,
--   selected peers
TracePromoteWarmBigLedgerPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | target active big ledger peers, actual active big ledger peers, peer,
--   reason
TracePromoteWarmBigLedgerPeerFailed :: Int -> Int -> peeraddr -> SomeException -> TracePeerSelection peeraddr

-- | target active big ledger peers, actual active big ledger peers, peer
TracePromoteWarmBigLedgerPeerDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | aborted promotion of a warm big ledger peer; likely it was
--   asynchronously demoted in the meantime.
--   
--   target active, actual active, peer
TracePromoteWarmBigLedgerPeerAborted :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | target established, actual established, selected peers
TraceDemoteWarmPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | target established, actual established, peer, reason
TraceDemoteWarmFailed :: Int -> Int -> peeraddr -> SomeException -> TracePeerSelection peeraddr

-- | target established, actual established, peer
TraceDemoteWarmDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | target established big ledger peers, actual established big ledger
--   peers, selected peers
TraceDemoteWarmBigLedgerPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | target established big ledger peers, actual established big ledger
--   peers, peer, reason
TraceDemoteWarmBigLedgerPeerFailed :: Int -> Int -> peeraddr -> SomeException -> TracePeerSelection peeraddr

-- | target established big ledger peers, actual established big ledger
--   peers, peer
TraceDemoteWarmBigLedgerPeerDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | target active, actual active, selected peers
TraceDemoteHotPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | local per-group (target active, actual active), selected peers
TraceDemoteLocalHotPeers :: [(HotValency, Int)] -> Set peeraddr -> TracePeerSelection peeraddr

-- | target active, actual active, peer, reason
TraceDemoteHotFailed :: Int -> Int -> peeraddr -> SomeException -> TracePeerSelection peeraddr

-- | target active, actual active, peer
TraceDemoteHotDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr

-- | target active big ledger peers, actual active big ledger peers,
--   selected peers
TraceDemoteHotBigLedgerPeers :: Int -> Int -> Set peeraddr -> TracePeerSelection peeraddr

-- | target active big ledger peers, actual active big ledger peers, peer,
--   reason
TraceDemoteHotBigLedgerPeerFailed :: Int -> Int -> peeraddr -> SomeException -> TracePeerSelection peeraddr

-- | target active big ledger peers, actual active big ledger peers, peer
TraceDemoteHotBigLedgerPeerDone :: Int -> Int -> peeraddr -> TracePeerSelection peeraddr
TraceDemoteAsynchronous :: Map peeraddr (PeerStatus, Maybe RepromoteDelay) -> TracePeerSelection peeraddr
TraceDemoteLocalAsynchronous :: Map peeraddr (PeerStatus, Maybe RepromoteDelay) -> TracePeerSelection peeraddr
TraceDemoteBigLedgerPeersAsynchronous :: Map peeraddr (PeerStatus, Maybe RepromoteDelay) -> TracePeerSelection peeraddr
TraceGovernorWakeup :: TracePeerSelection peeraddr
TraceChurnWait :: DiffTime -> TracePeerSelection peeraddr
TraceChurnMode :: ChurnMode -> TracePeerSelection peeraddr
data DebugPeerSelection peeraddr
[TraceGovernorState] :: forall peeraddr peerconn. Show peerconn => Time -> Maybe DiffTime -> PeerSelectionState peeraddr peerconn -> DebugPeerSelection peeraddr

peerSelectionGovernor :: (Alternative (STM m), MonadAsync m, MonadDelay m, MonadLabelledSTM m, MonadMask m, MonadTimer m, Ord peeraddr, Show peerconn) => Tracer m (TracePeerSelection peeraddr) -> Tracer m (DebugPeerSelection peeraddr) -> Tracer m PeerSelectionCounters -> StdGen -> StrictTVar m (PublicPeerSelectionState peeraddr) -> PeerSelectionActions peeraddr peerconn m -> PeerSelectionPolicy peeraddr m -> m Void

-- | Churn governor.
--   
--   At every churn interval decrease active peers for a short while (1s),
--   so that we can pick new ones. Then we churn non-active peers.
--   
--   On startup the churn governor gives a head start to local root peers
--   over root peers.
peerChurnGovernor :: forall m peeraddr. (MonadSTM m, MonadDelay m) => Tracer m (TracePeerSelection peeraddr) -> DiffTime -> DiffTime -> DiffTime -> PeerMetrics m peeraddr -> StrictTVar m ChurnMode -> StdGen -> STM m FetchMode -> PeerSelectionTargets -> StrictTVar m PeerSelectionTargets -> m Void
assertPeerSelectionState :: Ord peeraddr => PeerSelectionState peeraddr peerconn -> a -> a
sanePeerSelectionTargets :: PeerSelectionTargets -> Bool

-- | A view of the status of each established peer, for testing and
--   debugging.
establishedPeersStatus :: Ord peeraddr => PeerSelectionState peeraddr peerconn -> Map peeraddr PeerStatus

-- | The internal state used by the <tt>peerSelectionGovernor</tt>.
--   
--   The local and public root sets are disjoint, and their union is the
--   overall root set.
--   
--   Documentation of individual fields describes some of the invariants
--   these structures should maintain. For the entire picture, see
--   <a>assertPeerSelectionState</a>.
data PeerSelectionState peeraddr peerconn
PeerSelectionState :: !PeerSelectionTargets -> !LocalRootPeers peeraddr -> !Set peeraddr -> !Set peeraddr -> !KnownPeers peeraddr -> !EstablishedPeers peeraddr peerconn -> !Set peeraddr -> !Int -> !Time -> !Bool -> !Int -> !Time -> !Bool -> !Int -> !Set peeraddr -> !Set peeraddr -> !Set peeraddr -> !Set peeraddr -> !Set peeraddr -> !StdGen -> !Cache PeerSelectionCounters -> PeerSelectionState peeraddr peerconn
[targets] :: PeerSelectionState peeraddr peerconn -> !PeerSelectionTargets

-- | The current set of local root peers. This is structured as a bunch of
--   groups, with a target for each group. This gives us a set of n-of-m
--   choices, e.g. "pick 2 from this group and 1 from this group".
--   
--   The targets must of course be achievable, and to keep things simple,
--   the groups must be disjoint.
[localRootPeers] :: PeerSelectionState peeraddr peerconn -> !LocalRootPeers peeraddr
[publicRootPeers] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr

-- | Set of big ledger peers.
[bigLedgerPeers] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr

-- | Known peers.
[knownPeers] :: PeerSelectionState peeraddr peerconn -> !KnownPeers peeraddr

-- | Established peers.
[establishedPeers] :: PeerSelectionState peeraddr peerconn -> !EstablishedPeers peeraddr peerconn

-- | Active peers.
[activePeers] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr

-- | A counter to manage the exponential backoff strategy for when to retry
--   querying for more public root peers. It is negative for retry counts
--   after failure, and positive for retry counts that are successful but
--   make no progress.
[publicRootBackoffs] :: PeerSelectionState peeraddr peerconn -> !Int

-- | The earliest time we would be prepared to request more public root
--   peers. This is used with the <a>publicRootBackoffs</a> to manage the
--   exponential backoff.
[publicRootRetryTime] :: PeerSelectionState peeraddr peerconn -> !Time

-- | Whether a request for more public root peers is in progress.
[inProgressPublicRootsReq] :: PeerSelectionState peeraddr peerconn -> !Bool

-- | A counter to manage the exponential backoff strategy for when to retry
--   querying for more public root peers. It is negative for retry counts
--   after failure, and positive for retry counts that are successful but
--   make no progress.
[bigLedgerPeerBackoffs] :: PeerSelectionState peeraddr peerconn -> !Int

-- | The earliest time we would be prepared to request more big ledger
--   peers. This is used with the <a>bigLedgerPeerBackoffs</a> to manage
--   the exponential backoff.
[bigLedgerPeerRetryTime] :: PeerSelectionState peeraddr peerconn -> !Time

-- | Whether a request for more big ledger peers is in progress.
[inProgressBigLedgerPeersReq] :: PeerSelectionState peeraddr peerconn -> !Bool
[inProgressPeerShareReqs] :: PeerSelectionState peeraddr peerconn -> !Int
[inProgressPromoteCold] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr
[inProgressPromoteWarm] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr
[inProgressDemoteWarm] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr
[inProgressDemoteHot] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr

-- | Peers that had an async demotion and their connections are still being
--   closed
[inProgressDemoteToCold] :: PeerSelectionState peeraddr peerconn -> !Set peeraddr

-- | Rng for fuzzy delay
[fuzzRng] :: PeerSelectionState peeraddr peerconn -> !StdGen

-- | <a>PeerSelectionCounters</a> counters cache. Allows to only trace
--   values when necessary.
[countersCache] :: PeerSelectionState peeraddr peerconn -> !Cache PeerSelectionCounters

-- | Public <a>PeerSelectionState</a> that can be accessed by Peer Sharing
--   mechanisms without any problem.
--   
--   This data type should not expose too much information and keep only
--   essential data needed for computing the peer sharing request result
newtype PublicPeerSelectionState peeraddr
PublicPeerSelectionState :: Set peeraddr -> PublicPeerSelectionState peeraddr
[availableToShare] :: PublicPeerSelectionState peeraddr -> Set peeraddr
data PeerSelectionCounters
PeerSelectionCounters :: !Int -> !Int -> !Int -> !Int -> !Int -> !Int -> ![(Int, Int)] -> PeerSelectionCounters

-- | All cold peers including ledger peers, root peers, big ledger peers
--   and peers discovered through peer sharing.
[coldPeers] :: PeerSelectionCounters -> !Int

-- | All warm peers including ledger peers, root peers, big ledger peers,
--   local root peers and peers discovered through peer sharing.
[warmPeers] :: PeerSelectionCounters -> !Int

-- | All hot peers including ledger peers, root peers, big ledger peers,
--   local root peers and peers discovered through peer sharing.
[hotPeers] :: PeerSelectionCounters -> !Int

-- | Cold big ledger peers.
[coldBigLedgerPeers] :: PeerSelectionCounters -> !Int

-- | Warm big ledger peers.
[warmBigLedgerPeers] :: PeerSelectionCounters -> !Int

-- | Hot big ledger peers.
[hotBigLedgerPeers] :: PeerSelectionCounters -> !Int

-- | Local root peers with one entry per group. First entry is the number
--   of warm peers in that group the second is the number of hot peers in
--   that group.
[localRoots] :: PeerSelectionCounters -> ![(Int, Int)]
nullPeerSelectionTargets :: PeerSelectionTargets
emptyPeerSelectionState :: StdGen -> [(Int, Int)] -> PeerSelectionState peeraddr peerconn
emptyPublicPeerSelectionState :: Ord peeraddr => PublicPeerSelectionState peeraddr
data ChurnMode
ChurnModeBulkSync :: ChurnMode
ChurnModeNormal :: ChurnMode

module Ouroboros.Network.PeerSelection.PeerStateActions

-- | Record of arguments of <tt>peerSelectionActions</tt>.
data PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b
PeerStateActionsArguments :: Tracer m (PeerSelectionActionsTrace peerAddr versionNumber) -> DiffTime -> DiffTime -> MuxConnectionManager muxMode socket (ExpandedInitiatorContext peerAddr m) responderCtx peerAddr versionData versionNumber ByteString m a b -> ExitPolicy a -> PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b
[spsTracer] :: PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b -> Tracer m (PeerSelectionActionsTrace peerAddr versionNumber)

-- | Peer deactivation timeout: timeouts stopping hot protocols.
[spsDeactivateTimeout] :: PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b -> DiffTime

-- | Timeout on closing connection: timeouts stopping established and warm
--   peer protocols.
[spsCloseConnectionTimeout] :: PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b -> DiffTime
[spsConnectionManager] :: PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b -> MuxConnectionManager muxMode socket (ExpandedInitiatorContext peerAddr m) responderCtx peerAddr versionData versionNumber ByteString m a b
[spsExitPolicy] :: PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b -> ExitPolicy a

-- | Each established connection has access to <a>PeerConnectionHandle</a>.
--   It allows to promote / demote or close the connection, by having
--   access to <tt>Mux</tt>, three bundles of miniprotocols: for hot, warm
--   and established peers together with their state <a>StrictTVar</a>s.
data PeerConnectionHandle (muxMode :: MuxMode) responderCtx peerAddr versionData bytes m a b
withPeerStateActions :: forall (muxMode :: MuxMode) socket responderCtx peerAddr versionData versionNumber m a b x. (Alternative (STM m), MonadAsync m, MonadCatch m, MonadLabelledSTM m, MonadMask m, MonadTimer m, MonadThrow (STM m), HasInitiator muxMode ~ True, Typeable versionNumber, Show versionNumber, Ord peerAddr, Typeable peerAddr, Show peerAddr) => PeerStateActionsArguments muxMode socket responderCtx peerAddr versionData versionNumber m a b -> (PeerStateActions peerAddr (PeerConnectionHandle muxMode responderCtx peerAddr versionData ByteString m a b) m -> m x) -> m x
pchPeerSharing :: (versionData -> PeerSharing) -> PeerConnectionHandle muxMode responderCtx peerAddr versionData bytes m a b -> PeerSharing

-- | Parent exception of all peer selection action exceptions.
data PeerSelectionActionException
PeerSelectionActionException :: e -> PeerSelectionActionException
data EstablishConnectionException versionNumber

-- | Handshake client failed
ClientException :: !HandshakeException versionNumber -> EstablishConnectionException versionNumber

-- | Handshake server failed
ServerException :: !HandshakeException versionNumber -> EstablishConnectionException versionNumber
data PeerSelectionTimeoutException peerAddr
DeactivationTimeout :: !ConnectionId peerAddr -> PeerSelectionTimeoutException peerAddr

-- | Throw an exception when <a>monitorPeerConnection</a> blocks.
data MonitorPeerConnectionBlocked
MonitorPeerConnectionBlocked :: MonitorPeerConnectionBlocked

-- | Traces produced by <tt>peerSelectionActions</tt>.
data PeerSelectionActionsTrace peerAddr vNumber
PeerStatusChanged :: PeerStatusChangeType peerAddr -> PeerSelectionActionsTrace peerAddr vNumber
PeerStatusChangeFailure :: PeerStatusChangeType peerAddr -> FailureType vNumber -> PeerSelectionActionsTrace peerAddr vNumber
PeerMonitoringError :: ConnectionId peerAddr -> SomeException -> PeerSelectionActionsTrace peerAddr vNumber
PeerMonitoringResult :: ConnectionId peerAddr -> Maybe (WithSomeProtocolTemperature FirstToFinishResult) -> PeerSelectionActionsTrace peerAddr vNumber

-- | All transitions.
data PeerStatusChangeType peerAddr

-- | During the <a>ColdToWarm</a> transition we have the remote address,
--   and only if establishing connection (establishing bearer &amp;
--   handshake negotiation) is successful we have access to full
--   <a>ConnectionId</a>.
ColdToWarm :: !Maybe peerAddr -> !peerAddr -> PeerStatusChangeType peerAddr
WarmToHot :: !ConnectionId peerAddr -> PeerStatusChangeType peerAddr
HotToWarm :: !ConnectionId peerAddr -> PeerStatusChangeType peerAddr
WarmToCooling :: !ConnectionId peerAddr -> PeerStatusChangeType peerAddr
HotToCooling :: !ConnectionId peerAddr -> PeerStatusChangeType peerAddr
CoolingToCold :: !ConnectionId peerAddr -> PeerStatusChangeType peerAddr

-- | Type of failure with additional exception context; We don't log
--   handshake errors as this will be done by the handshake tracer.
data FailureType versionNumber
HandshakeClientFailure :: !HandshakeException versionNumber -> FailureType versionNumber
HandshakeServerFailure :: !HandshakeException versionNumber -> FailureType versionNumber
HandleFailure :: !SomeException -> FailureType versionNumber
MuxStoppedFailure :: FailureType versionNumber
TimeoutError :: FailureType versionNumber
ActiveCold :: FailureType versionNumber
ApplicationFailure :: ![MiniProtocolException] -> FailureType versionNumber
instance GHC.Show.Show Ouroboros.Network.PeerSelection.PeerStateActions.MiniProtocolException
instance GHC.Show.Show Ouroboros.Network.PeerSelection.PeerStateActions.MiniProtocolExceptions
instance GHC.Show.Show Ouroboros.Network.PeerSelection.PeerStateActions.FirstToFinishResult
instance GHC.Show.Show Ouroboros.Network.PeerSelection.PeerStateActions.MonitorPeerConnectionBlocked
instance GHC.Show.Show versionNumber => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerStateActions.EstablishConnectionException versionNumber)
instance GHC.Show.Show peerAddr => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerStateActions.PeerSelectionTimeoutException peerAddr)
instance GHC.Show.Show peerAddr => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerStateActions.ColdActionException peerAddr)
instance GHC.Show.Show versionNumber => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerStateActions.FailureType versionNumber)
instance GHC.Show.Show peerAddr => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerStateActions.PeerStatusChangeType peerAddr)
instance (GHC.Show.Show peerAddr, GHC.Show.Show vNumber) => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerStateActions.PeerSelectionActionsTrace peerAddr vNumber)
instance (GHC.Show.Show peerAddr, Data.Typeable.Internal.Typeable peerAddr) => GHC.Exception.Type.Exception (Ouroboros.Network.PeerSelection.PeerStateActions.ColdActionException peerAddr)
instance (GHC.Show.Show peerAddr, Data.Typeable.Internal.Typeable peerAddr) => GHC.Exception.Type.Exception (Ouroboros.Network.PeerSelection.PeerStateActions.PeerSelectionTimeoutException peerAddr)
instance (GHC.Show.Show versionNumber, Data.Typeable.Internal.Typeable versionNumber) => GHC.Exception.Type.Exception (Ouroboros.Network.PeerSelection.PeerStateActions.EstablishConnectionException versionNumber)
instance GHC.Exception.Type.Exception Ouroboros.Network.PeerSelection.PeerStateActions.MonitorPeerConnectionBlocked
instance GHC.Show.Show Ouroboros.Network.PeerSelection.PeerStateActions.PeerSelectionActionException
instance GHC.Exception.Type.Exception Ouroboros.Network.PeerSelection.PeerStateActions.PeerSelectionActionException
instance (GHC.Show.Show peerAddr, GHC.Show.Show versionData) => GHC.Show.Show (Ouroboros.Network.PeerSelection.PeerStateActions.PeerConnectionHandle muxMode responderCtx peerAddr versionData bytes m a b)
instance GHC.Base.Semigroup (Ouroboros.Network.PeerSelection.PeerStateActions.LastToFinishResult a)
instance GHC.Base.Monoid (Ouroboros.Network.PeerSelection.PeerStateActions.LastToFinishResult a)
instance GHC.Base.Semigroup Ouroboros.Network.PeerSelection.PeerStateActions.FirstToFinishResult
instance GHC.Exception.Type.Exception Ouroboros.Network.PeerSelection.PeerStateActions.MiniProtocolExceptions


-- | This module is expected to be imported qualified (it will clash with
--   the <a>Ouroboros.Network.Diffusion.NonP2P</a>).
module Ouroboros.Network.Diffusion.P2P

-- | P2P DiffusionTracers Extras
data TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m
TracersExtra :: Tracer m (TraceLocalRootPeers ntnAddr resolverError) -> Tracer m TracePublicRootPeers -> Tracer m TraceLedgerPeers -> Tracer m (TracePeerSelection ntnAddr) -> Tracer m (DebugPeerSelection ntnAddr) -> Tracer m (DebugPeerSelection ntnAddr) -> Tracer m PeerSelectionCounters -> Tracer m (PeerSelectionActionsTrace ntnAddr ntnVersion) -> Tracer m (ConnectionManagerTrace ntnAddr (ConnectionHandlerTrace ntnVersion ntnVersionData)) -> Tracer m (AbstractTransitionTrace ntnAddr) -> Tracer m (ServerTrace ntnAddr) -> Tracer m (InboundGovernorTrace ntnAddr) -> Tracer m (RemoteTransitionTrace ntnAddr) -> Tracer m (ConnectionManagerTrace ntcAddr (ConnectionHandlerTrace ntcVersion ntcVersionData)) -> Tracer m (ServerTrace ntcAddr) -> Tracer m (InboundGovernorTrace ntcAddr) -> TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m
[dtTraceLocalRootPeersTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (TraceLocalRootPeers ntnAddr resolverError)
[dtTracePublicRootPeersTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m TracePublicRootPeers

-- | Ledger Peers tracer
[dtTraceLedgerPeersTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m TraceLedgerPeers
[dtTracePeerSelectionTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (TracePeerSelection ntnAddr)
[dtDebugPeerSelectionInitiatorTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (DebugPeerSelection ntnAddr)
[dtDebugPeerSelectionInitiatorResponderTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (DebugPeerSelection ntnAddr)
[dtTracePeerSelectionCounters] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m PeerSelectionCounters
[dtPeerSelectionActionsTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (PeerSelectionActionsTrace ntnAddr ntnVersion)
[dtConnectionManagerTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (ConnectionManagerTrace ntnAddr (ConnectionHandlerTrace ntnVersion ntnVersionData))
[dtConnectionManagerTransitionTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (AbstractTransitionTrace ntnAddr)
[dtServerTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (ServerTrace ntnAddr)
[dtInboundGovernorTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (InboundGovernorTrace ntnAddr)
[dtInboundGovernorTransitionTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (RemoteTransitionTrace ntnAddr)

-- | Connection manager tracer for local clients
[dtLocalConnectionManagerTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (ConnectionManagerTrace ntcAddr (ConnectionHandlerTrace ntcVersion ntcVersionData))

-- | Server tracer for local clients
[dtLocalServerTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (ServerTrace ntcAddr)

-- | Inbound protocol governor tracer for local clients
[dtLocalInboundGovernorTracer] :: TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Tracer m (InboundGovernorTrace ntcAddr)
nullTracers :: Applicative m => TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m

-- | P2P Arguments Extras
data ArgumentsExtra m
ArgumentsExtra :: PeerSelectionTargets -> STM m [(HotValency, WarmValency, Map RelayAccessPoint PeerAdvertise)] -> STM m (Map RelayAccessPoint PeerAdvertise) -> PeerSharing -> STM m UseLedgerAfter -> DiffTime -> DiffTime -> DiffTime -> DiffTime -> ArgumentsExtra m

-- | selection targets for the peer governor
[daPeerSelectionTargets] :: ArgumentsExtra m -> PeerSelectionTargets
[daReadLocalRootPeers] :: ArgumentsExtra m -> STM m [(HotValency, WarmValency, Map RelayAccessPoint PeerAdvertise)]
[daReadPublicRootPeers] :: ArgumentsExtra m -> STM m (Map RelayAccessPoint PeerAdvertise)

-- | Peer's own PeerSharing value.
--   
--   This value comes from the node's configuration file and is static.
[daOwnPeerSharing] :: ArgumentsExtra m -> PeerSharing
[daReadUseLedgerAfter] :: ArgumentsExtra m -> STM m UseLedgerAfter

-- | Timeout which starts once all responder protocols are idle. If the
--   responders stay idle for duration of the timeout, the connection will
--   be demoted, if it wasn't used by the p2p-governor it will be closed.
--   
--   Applies to <a>Unidirectional</a> as well as <a>Duplex</a>
--   <i>node-to-node</i> connections.
--   
--   See <tt>serverProtocolIdleTimeout</tt>.
[daProtocolIdleTimeout] :: ArgumentsExtra m -> DiffTime

-- | Time for which <i>node-to-node</i> connections are kept in
--   <a>TerminatingState</a>, it should correspond to the OS configured
--   <tt>TCP</tt> <tt>TIME_WAIT</tt> timeout.
--   
--   This timeout will apply to after a connection has been closed, its
--   purpose is to be resilient for delayed packets in the same way
--   <tt>TCP</tt> is using <tt>TIME_WAIT</tt>.
[daTimeWaitTimeout] :: ArgumentsExtra m -> DiffTime

-- | Churn interval between churn events in deadline mode. A small fuzz is
--   added (max 10 minutes) so that not all nodes churn at the same time.
--   
--   By default it is set to 3300 seconds.
[daDeadlineChurnInterval] :: ArgumentsExtra m -> DiffTime

-- | Churn interval between churn events in bulk sync mode. A small fuzz is
--   added (max 1 minute) so that not all nodes churn at the same time.
--   
--   By default it is set to 300 seconds.
[daBulkChurnInterval] :: ArgumentsExtra m -> DiffTime
data () => AcceptedConnectionsLimit
AcceptedConnectionsLimit :: !Word32 -> !Word32 -> !DiffTime -> AcceptedConnectionsLimit
[acceptedConnectionsHardLimit] :: AcceptedConnectionsLimit -> !Word32
[acceptedConnectionsSoftLimit] :: AcceptedConnectionsLimit -> !Word32
[acceptedConnectionsDelay] :: AcceptedConnectionsLimit -> !DiffTime

-- | P2P Applications Extras
--   
--   TODO: we need initiator only mode for Daedalus, there's no reason why
--   it should run a node-to-node server side.
data ApplicationsExtra ntnAddr m a
ApplicationsExtra :: RethrowPolicy -> ReturnPolicy a -> RethrowPolicy -> PeerMetrics m ntnAddr -> STM m FetchMode -> PeerSharingRegistry ntnAddr m -> ApplicationsExtra ntnAddr m a

-- | <i>node-to-node</i> rethrow policy
[daRethrowPolicy] :: ApplicationsExtra ntnAddr m a -> RethrowPolicy

-- | <i>node-to-node</i> return policy
[daReturnPolicy] :: ApplicationsExtra ntnAddr m a -> ReturnPolicy a

-- | <i>node-to-client</i> rethrow policy
[daLocalRethrowPolicy] :: ApplicationsExtra ntnAddr m a -> RethrowPolicy

-- | <a>PeerMetrics</a> used by peer selection policy (see
--   <tt>simplePeerSelectionPolicy</tt>)
[daPeerMetrics] :: ApplicationsExtra ntnAddr m a -> PeerMetrics m ntnAddr

-- | Used by churn-governor
[daBlockFetchMode] :: ApplicationsExtra ntnAddr m a -> STM m FetchMode

-- | Used for peer sharing protocol
[daPeerSharingRegistry] :: ApplicationsExtra ntnAddr m a -> PeerSharingRegistry ntnAddr m

-- | Main entry point for data diffusion service. It allows to:
--   
--   <ul>
--   <li>connect to upstream peers;</li>
--   <li>accept connection from downstream peers, if run in
--   <a>InitiatorAndResponderDiffusionMode</a>.</li>
--   <li>runs a local service which allows to use node-to-client protocol
--   to obtain information from the running system. This is used by
--   'cardano-cli' or a wallet and a like local services.</li>
--   </ul>
run :: Tracers RemoteAddress NodeToNodeVersion LocalAddress NodeToClientVersion IO -> TracersExtra RemoteAddress NodeToNodeVersion NodeToNodeVersionData LocalAddress NodeToClientVersion NodeToClientVersionData IOException IO -> Arguments Socket RemoteAddress LocalSocket LocalAddress -> ArgumentsExtra IO -> Applications RemoteAddress NodeToNodeVersion NodeToNodeVersionData LocalAddress NodeToClientVersion NodeToClientVersionData IO a -> ApplicationsExtra RemoteAddress IO a -> IO Void
data Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m
Interfaces :: Snocket m ntnFd ntnAddr -> MakeBearer m ntnFd -> (ntnFd -> Maybe ntnAddr -> m ()) -> (ntnFd -> ntnAddr -> m ()) -> HandshakeArguments (ConnectionId ntnAddr) ntnVersion ntnVersionData m -> (ntnAddr -> Maybe AddressType) -> (ntnVersion -> ntnVersionData -> DataFlow) -> (ntnVersionData -> PeerSharing) -> (IP -> PortNumber -> ntnAddr) -> Snocket m ntcFd ntcAddr -> MakeBearer m ntcFd -> HandshakeArguments (ConnectionId ntcAddr) ntcVersion ntcVersionData m -> (ntcFd -> m FileDescriptor) -> StdGen -> (forall mode x y. NodeToNodeConnectionManager mode ntnFd ntnAddr ntnVersionData ntnVersion m x y -> m ()) -> (DNSLookupType -> DNSActions resolver resolverError m) -> Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m

-- | node-to-node snocket
[diNtnSnocket] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> Snocket m ntnFd ntnAddr

-- | node-to-node <a>MakeBearer</a> callback
[diNtnBearer] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> MakeBearer m ntnFd

-- | node-to-node socket configuration
[diNtnConfigureSocket] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> ntnFd -> Maybe ntnAddr -> m ()

-- | node-to-node systemd socket configuration
[diNtnConfigureSystemdSocket] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> ntnFd -> ntnAddr -> m ()

-- | node-to-node handshake configuration
[diNtnHandshakeArguments] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> HandshakeArguments (ConnectionId ntnAddr) ntnVersion ntnVersionData m

-- | node-to-node address type
[diNtnAddressType] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> ntnAddr -> Maybe AddressType

-- | node-to-node data flow used by connection manager to classify
--   negotiated connections
[diNtnDataFlow] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> ntnVersion -> ntnVersionData -> DataFlow

-- | remote side peer sharing information used by peer selection governor
--   to decide which peers are available for performing peer sharing
[diNtnPeerSharing] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> ntnVersionData -> PeerSharing

-- | node-to-node peer address
[diNtnToPeerAddr] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> IP -> PortNumber -> ntnAddr

-- | node-to-client snocket
[diNtcSnocket] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> Snocket m ntcFd ntcAddr

-- | node-to-client <a>MakeBearer</a> callback
[diNtcBearer] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> MakeBearer m ntcFd

-- | node-to-client handshake configuration
[diNtcHandshakeArguments] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> HandshakeArguments (ConnectionId ntcAddr) ntcVersion ntcVersionData m

-- | node-to-client file descriptor
[diNtcGetFileDescriptor] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> ntcFd -> m FileDescriptor

-- | diffusion pseudo random generator. It is split between various
--   components that need randomness, e.g. inbound governor, peer
--   selection, policies, etc.
[diRng] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> StdGen

-- | callback which is used to register <tt>SIGUSR1</tt> signal handler.
[diInstallSigUSR1Handler] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> forall mode x y. NodeToNodeConnectionManager mode ntnFd ntnAddr ntnVersionData ntnVersion m x y -> m ()

-- | diffusion dns actions
[diDnsActions] :: Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> DNSLookupType -> DNSActions resolver resolverError m
runM :: forall m ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError a. (Alternative (STM m), MonadAsync m, MonadDelay m, MonadEvaluate m, MonadFix m, MonadFork m, MonadLabelledSTM m, MonadTraceSTM m, MonadMask m, MonadThrow (STM m), MonadTime m, MonadTimer m, MonadMVar m, Typeable ntnAddr, Ord ntnAddr, Show ntnAddr, Typeable ntnVersion, Ord ntnVersion, Show ntnVersion, Show ntnVersionData, Typeable ntcAddr, Ord ntcAddr, Show ntcAddr, Ord ntcVersion, Exception resolverError) => Interfaces ntnFd ntnAddr ntnVersion ntnVersionData ntcFd ntcAddr ntcVersion ntcVersionData resolver resolverError m -> Tracers ntnAddr ntnVersion ntcAddr ntcVersion m -> TracersExtra ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData resolverError m -> Arguments ntnFd ntnAddr ntcFd ntcAddr -> ArgumentsExtra m -> Applications ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData m a -> ApplicationsExtra ntnAddr m a -> m Void
type NodeToNodePeerConnectionHandle (mode :: MuxMode) ntnAddr ntnVersionData m a b = PeerConnectionHandle mode (ResponderContext ntnAddr) ntnAddr ntnVersionData ByteString m a b
type AbstractTransitionTrace peerAddr = TransitionTrace' peerAddr AbstractState
type RemoteTransitionTrace peerAddr = TransitionTrace' peerAddr Maybe RemoteSt


-- | This module is expected to be imported qualified (it will clash with
--   the <a>Ouroboros.Network.Diffusion.P2P</a>).
module Ouroboros.Network.Diffusion.NonP2P

-- | NonP2P DiffusionTracers Extras
data TracersExtra
TracersExtra :: Tracer IO (WithIPList (SubscriptionTrace SockAddr)) -> Tracer IO (WithDomainName (SubscriptionTrace SockAddr)) -> Tracer IO (WithDomainName DnsTrace) -> Tracer IO (WithAddr SockAddr ErrorPolicyTrace) -> Tracer IO (WithAddr LocalAddress ErrorPolicyTrace) -> Tracer IO AcceptConnectionsPolicyTrace -> TracersExtra

-- | IP subscription tracer
[dtIpSubscriptionTracer] :: TracersExtra -> Tracer IO (WithIPList (SubscriptionTrace SockAddr))

-- | DNS subscription tracer
[dtDnsSubscriptionTracer] :: TracersExtra -> Tracer IO (WithDomainName (SubscriptionTrace SockAddr))

-- | DNS resolver tracer
[dtDnsResolverTracer] :: TracersExtra -> Tracer IO (WithDomainName DnsTrace)
[dtErrorPolicyTracer] :: TracersExtra -> Tracer IO (WithAddr SockAddr ErrorPolicyTrace)
[dtLocalErrorPolicyTracer] :: TracersExtra -> Tracer IO (WithAddr LocalAddress ErrorPolicyTrace)

-- | Trace rate limiting of accepted connections
[dtAcceptPolicyTracer] :: TracersExtra -> Tracer IO AcceptConnectionsPolicyTrace
nullTracers :: TracersExtra

-- | NonP2P extra applications
newtype ApplicationsExtra
ApplicationsExtra :: ErrorPolicies -> ApplicationsExtra

-- | Error policies
[daErrorPolicies] :: ApplicationsExtra -> ErrorPolicies

-- | NonP2P extra arguments
data ArgumentsExtra
ArgumentsExtra :: IPSubscriptionTarget -> [DnsSubscriptionTarget] -> ArgumentsExtra

-- | ip subscription addresses
[daIpProducers] :: ArgumentsExtra -> IPSubscriptionTarget

-- | list of domain names to subscribe to
[daDnsProducers] :: ArgumentsExtra -> [DnsSubscriptionTarget]
run :: Tracers RemoteAddress NodeToNodeVersion LocalAddress NodeToClientVersion IO -> TracersExtra -> Arguments Socket RemoteAddress LocalSocket LocalAddress -> ArgumentsExtra -> Applications RemoteAddress NodeToNodeVersion NodeToNodeVersionData LocalAddress NodeToClientVersion NodeToClientVersionData IO a -> ApplicationsExtra -> IO ()

module Ouroboros.Network.Diffusion

-- | Promoted data types.
data P2P
P2P :: P2P
NonP2P :: P2P

-- | The <a>DiffusionTracer</a> logs
--   
--   <ul>
--   <li>diffusion initialisation messages</li>
--   <li>terminal errors thrown by diffusion</li>
--   </ul>
data DiffusionTracer ntnAddr ntcAddr
RunServer :: NonEmpty ntnAddr -> DiffusionTracer ntnAddr ntcAddr
RunLocalServer :: ntcAddr -> DiffusionTracer ntnAddr ntcAddr
UsingSystemdSocket :: ntcAddr -> DiffusionTracer ntnAddr ntcAddr
CreateSystemdSocketForSnocketPath :: ntcAddr -> DiffusionTracer ntnAddr ntcAddr
CreatedLocalSocket :: ntcAddr -> DiffusionTracer ntnAddr ntcAddr
ConfiguringLocalSocket :: ntcAddr -> FileDescriptor -> DiffusionTracer ntnAddr ntcAddr
ListeningLocalSocket :: ntcAddr -> FileDescriptor -> DiffusionTracer ntnAddr ntcAddr
LocalSocketUp :: ntcAddr -> FileDescriptor -> DiffusionTracer ntnAddr ntcAddr
CreatingServerSocket :: ntnAddr -> DiffusionTracer ntnAddr ntcAddr
ConfiguringServerSocket :: ntnAddr -> DiffusionTracer ntnAddr ntcAddr
ListeningServerSocket :: ntnAddr -> DiffusionTracer ntnAddr ntcAddr
ServerSocketUp :: ntnAddr -> DiffusionTracer ntnAddr ntcAddr
UnsupportedLocalSystemdSocket :: ntnAddr -> DiffusionTracer ntnAddr ntcAddr
UnsupportedReadySocketCase :: DiffusionTracer ntnAddr ntcAddr
DiffusionErrored :: SomeException -> DiffusionTracer ntnAddr ntcAddr
SystemdSocketConfiguration :: SystemdSocketTracer -> DiffusionTracer ntnAddr ntcAddr

-- | Common DiffusionTracers interface between P2P and NonP2P
data Tracers ntnAddr ntnVersion ntcAddr ntcVersion m
Tracers :: Tracer m (WithMuxBearer (ConnectionId ntnAddr) MuxTrace) -> Tracer m (HandshakeTr ntnAddr ntnVersion) -> Tracer m (WithMuxBearer (ConnectionId ntcAddr) MuxTrace) -> Tracer m (HandshakeTr ntcAddr ntcVersion) -> Tracer m (DiffusionTracer ntnAddr ntcAddr) -> Tracers ntnAddr ntnVersion ntcAddr ntcVersion m

-- | Mux tracer
[dtMuxTracer] :: Tracers ntnAddr ntnVersion ntcAddr ntcVersion m -> Tracer m (WithMuxBearer (ConnectionId ntnAddr) MuxTrace)

-- | Handshake protocol tracer
[dtHandshakeTracer] :: Tracers ntnAddr ntnVersion ntcAddr ntcVersion m -> Tracer m (HandshakeTr ntnAddr ntnVersion)

-- | Mux tracer for local clients
[dtLocalMuxTracer] :: Tracers ntnAddr ntnVersion ntcAddr ntcVersion m -> Tracer m (WithMuxBearer (ConnectionId ntcAddr) MuxTrace)

-- | Handshake protocol tracer for local clients
[dtLocalHandshakeTracer] :: Tracers ntnAddr ntnVersion ntcAddr ntcVersion m -> Tracer m (HandshakeTr ntcAddr ntcVersion)

-- | Diffusion initialisation tracer
[dtDiffusionTracer] :: Tracers ntnAddr ntnVersion ntcAddr ntcVersion m -> Tracer m (DiffusionTracer ntnAddr ntcAddr)
nullTracers :: Applicative m => Tracers ntnAddr ntnVersion ntcAddr ntcVersion m

-- | Tracers which depend on p2p mode.
data ExtraTracers (p2p :: P2P)
[P2PTracers] :: TracersExtra RemoteAddress NodeToNodeVersion NodeToNodeVersionData LocalAddress NodeToClientVersion NodeToClientVersionData IOException IO -> ExtraTracers 'P2P
[NonP2PTracers] :: TracersExtra -> ExtraTracers 'NonP2P
data Failure
[UnsupportedReadySocket] :: Failure
[UnexpectedIPv4Address] :: forall ntnAddr. (Show ntnAddr, Typeable ntnAddr) => ntnAddr -> Failure
[UnexpectedIPv6Address] :: forall ntnAddr. (Show ntnAddr, Typeable ntnAddr) => ntnAddr -> Failure
[NoSocket] :: Failure
[DiffusionError] :: SomeException -> Failure

-- | Common DiffusionArguments interface between P2P and NonP2P
data Arguments ntnFd ntnAddr ntcFd ntcAddr
Arguments :: Maybe (Either ntnFd ntnAddr) -> Maybe (Either ntnFd ntnAddr) -> Maybe (Either ntcFd ntcAddr) -> AcceptedConnectionsLimit -> DiffusionMode -> Arguments ntnFd ntnAddr ntcFd ntcAddr

-- | an <tt>IPv4</tt> socket ready to accept connections or an
--   <tt>IPv4</tt> addresses
[daIPv4Address] :: Arguments ntnFd ntnAddr ntcFd ntcAddr -> Maybe (Either ntnFd ntnAddr)

-- | an <tt>IPv6</tt> socket ready to accept connections or an
--   <tt>IPv6</tt> addresses
[daIPv6Address] :: Arguments ntnFd ntnAddr ntcFd ntcAddr -> Maybe (Either ntnFd ntnAddr)

-- | an <tt>AF_UNIX</tt> socket ready to accept connections or an
--   <tt>AF_UNIX</tt> socket path
[daLocalAddress] :: Arguments ntnFd ntnAddr ntcFd ntcAddr -> Maybe (Either ntcFd ntcAddr)

-- | parameters for limiting number of accepted connections
[daAcceptedConnectionsLimit] :: Arguments ntnFd ntnAddr ntcFd ntcAddr -> AcceptedConnectionsLimit

-- | run in initiator only mode
[daMode] :: Arguments ntnFd ntnAddr ntcFd ntcAddr -> DiffusionMode

-- | Diffusion arguments which depend on p2p mode.
data ExtraArguments (p2p :: P2P) m
[P2PArguments] :: ArgumentsExtra m -> ExtraArguments 'P2P m
[NonP2PArguments] :: ArgumentsExtra -> ExtraArguments 'NonP2P m

-- | Versioned mini-protocol bundles run on a negotiated connection.
data Applications ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData m a
Applications :: Versions ntnVersion ntnVersionData (OuroborosBundleWithExpandedCtx InitiatorMode ntnAddr ByteString m a Void) -> ((PeerSharingAmount -> m [ntnAddr]) -> Versions ntnVersion ntnVersionData (OuroborosBundleWithExpandedCtx InitiatorResponderMode ntnAddr ByteString m a ())) -> Versions ntcVersion ntcVersionData (OuroborosApplicationWithMinimalCtx ResponderMode ntcAddr ByteString m Void ()) -> LedgerPeersConsensusInterface m -> Applications ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData m a

-- | NodeToNode initiator applications for initiator only mode.
--   
--   TODO: we should accept one or the other, but not both:
--   <a>daApplicationInitiatorMode</a>,
--   <a>daApplicationInitiatorResponderMode</a>.
--   
--   Even in non-p2p mode we use p2p apps.
[daApplicationInitiatorMode] :: Applications ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData m a -> Versions ntnVersion ntnVersionData (OuroborosBundleWithExpandedCtx InitiatorMode ntnAddr ByteString m a Void)

-- | NodeToNode initiator &amp; responder applications for bidirectional
--   mode.
[daApplicationInitiatorResponderMode] :: Applications ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData m a -> (PeerSharingAmount -> m [ntnAddr]) -> Versions ntnVersion ntnVersionData (OuroborosBundleWithExpandedCtx InitiatorResponderMode ntnAddr ByteString m a ())

-- | NodeToClient responder application (server role)
--   
--   Because p2p mode does not infect local connections we we use non-p2p
--   apps.
[daLocalResponderApplication] :: Applications ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData m a -> Versions ntcVersion ntcVersionData (OuroborosApplicationWithMinimalCtx ResponderMode ntcAddr ByteString m Void ())

-- | Interface used to get peers from the current ledger.
--   
--   TODO: it should be in <tt>InterfaceExtra</tt>
[daLedgerPeersCtx] :: Applications ntnAddr ntnVersion ntnVersionData ntcAddr ntcVersion ntcVersionData m a -> LedgerPeersConsensusInterface m

-- | Application data which depend on p2p mode.
data ExtraApplications (p2p :: P2P) ntnAddr m a
[P2PApplications] :: ApplicationsExtra ntnAddr m a -> ExtraApplications 'P2P ntnAddr m a
[NonP2PApplications] :: ApplicationsExtra -> ExtraApplications 'NonP2P ntnAddr m a

-- | Run data diffusion in either <a>P2P</a> or <a>NonP2P</a> mode.
run :: forall (p2p :: P2P) a. Tracers RemoteAddress NodeToNodeVersion LocalAddress NodeToClientVersion IO -> ExtraTracers p2p -> Arguments Socket RemoteAddress LocalSocket LocalAddress -> ExtraArguments p2p IO -> Applications RemoteAddress NodeToNodeVersion NodeToNodeVersionData LocalAddress NodeToClientVersion NodeToClientVersionData IO a -> ExtraApplications p2p RemoteAddress IO a -> IO ()
type AbstractTransitionTrace peerAddr = TransitionTrace' peerAddr AbstractState
